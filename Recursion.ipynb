{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning\n",
      "creating string\n",
      "checking for palindrome:  22\n",
      "manaplanafcanalpanam\n",
      "result:  True\n",
      "checking for palindrome:  20\n",
      "anaplanafcanalpana\n",
      "result:  True\n",
      "checking for palindrome:  18\n",
      "naplanafcanalpan\n",
      "result:  True\n",
      "checking for palindrome:  16\n",
      "aplanafcanalpa\n",
      "result:  True\n",
      "checking for palindrome:  14\n",
      "planafcanalp\n",
      "result:  True\n",
      "checking for palindrome:  12\n",
      "lanafcanal\n",
      "result:  True\n",
      "checking for palindrome:  10\n",
      "anafcana\n",
      "result:  True\n",
      "checking for palindrome:  8\n",
      "nafcan\n",
      "result:  True\n",
      "checking for palindrome:  6\n",
      "afca\n",
      "result:  True\n",
      "checking for palindrome:  4\n",
      "fc\n",
      "result:  True\n",
      "checking for palindrome:  2\n",
      "\n",
      "result:  False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def isPalindrome(s):\n",
    "    print('beginning')\n",
    "    \n",
    "    def toChars(s):\n",
    "        print('creating string')\n",
    "        s = s.lower()\n",
    "        ans = ''\n",
    "        for c in s:\n",
    "            if c.islower(): # 'abcdefghijklmnopqrstuvwxyz':\n",
    "                ans = ans + c\n",
    "        return ans\n",
    "    \n",
    "    def isPal(s):\n",
    "        print('checking for palindrome: ', len(s))\n",
    "        if len(s) <=1:\n",
    "            return True\n",
    "        else:\n",
    "            print(s[1:-1])\n",
    "            print(\"result: \", s[0] == s[-1])\n",
    "            return s[0] == s[-1] and isPal(s[1:-1])\n",
    "        \n",
    "    return isPal(toChars(s))\n",
    "\n",
    "print(isPalindrome('A man, a plan, a fcanal, Panama'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFunction to remove predicate phrases from noun_chunks\\n\\ninput: the chunk to be checked, list of predicate phrases\\nreturns: the chnunk with predicate phrases removed.\\n\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates = [['such', 'as'], ['know', 'as'], ['such', 'NOUN', 'as'], ['include'], ['especially'], ['other'], ['which', 'may', 'include'], ['which', 'be', 'similar', 'to'], ['example', 'of', 'this', 'be'], [',type'], ['mainly'], ['mostly'], ['notably'], ['particularly'], ['principally'], ['in', 'particular'], ['except'], ['other', 'than'], ['eg'], ['ie'], ['for', 'example'], ['example', 'of', 'be'], ['like'], ['whether'], ['compare', 'to'], ['among', '-PRON-'], ['for', 'instance'], ['and-or', 'any', 'other'], ['some', 'other'], ['be', 'a'], ['like', 'other'], ['one', 'of', 'the'], ['one', 'of', 'these'], ['one', 'of', 'those'], ['be', 'example', 'of'], ['which', 'be', 'call'], ['which', 'be', 'name'], ['a', 'kind', 'of'], ['form', 'of'], ['which', 'look', 'like'], ['which', 'sound', 'like'], ['type'], ['compare', 'with'], ['as'], ['sort', 'of']]\n",
    "print(len(predicates))\n",
    "\n",
    "def check(predicates):\n",
    "    if len(predicates) == 0:\n",
    "        return len(predicates), \"finshed\"\n",
    "    \n",
    "    print(len(predicates), predicates[0])\n",
    "    \n",
    "    return check(predicates[1:])\n",
    "\n",
    "#check(predicates)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "from cndlib import entities\n",
    "\n",
    "def isPredicateMatch(chunk, predicates):\n",
    "\n",
    "    def match(empty, count, chunk, predicates):#\n",
    "        # check for empty list, whether the\n",
    "        while not empty and chunk[count].lemma_ != predicates[0][-count] and chunk[count].lemma_ == predicates[0][count]: #and count != len(predicates[0])\n",
    "            count += 1\n",
    "        \n",
    "        return empty, count\n",
    "    \n",
    "    def isMatch(chunk, predicates):\n",
    "        \n",
    "        empty, counter = match(predicates == [], 0, chunk, predicates)\n",
    "        if empty or counter == len(predicates[0]):\n",
    "            \n",
    "            new_chunk = chunk[counter:]\n",
    "            if not empty and len(new_chunk) == 0:\n",
    "                print('-----')\n",
    "                print(new_chunk.sent)\n",
    "                print(chunk, 'becomes', new_chunk)\n",
    "                print(len(chunk), '|', len(new_chunk))\n",
    "                print(len(chunk), '|', len(predicates[0]), '|', predicates[0])\n",
    "                #counter = 0\n",
    "            \n",
    "            return chunk[counter:]\n",
    "        else:\n",
    "            return isMatch(chunk, predicates[1:])\n",
    "        \n",
    "    return isMatch(chunk, predicates)\n",
    "\n",
    "dirpath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\speeches\"\n",
    "\n",
    "orators = entities.Dataset(dirpath)\n",
    "\n",
    "doc = nlp(orators[\"king\"][0].text)\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "\n",
    "        attrs = {\"tag\": chunk.root.tag, \"dep\": chunk.root.dep}\n",
    "        #retokenizer.merge(chunk, attrs = attrs)\n",
    "        new_chunk =  isPredicateMatch(chunk, predicates)\n",
    "        \n",
    "#         if len(new_chunk) == 0:\n",
    "#             print('#####')\n",
    "#             print(new_chunk.sent)\n",
    "#             print(chunk, 'becomes', new_chunk)\n",
    "\n",
    "        retokenizer.merge(new_chunk, attrs = attrs)\n",
    "    print('complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  |  3\n",
      "6  |  10\n",
      "5  |  16\n",
      "4  |  21\n",
      "3  |  25\n",
      "2  |  28\n",
      "1  |  30\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "def Total(x, y):\n",
    "\n",
    "    def foo(x, y):\n",
    "        return 2*x+y\n",
    "\n",
    "    def bar(x, y):\n",
    "        if x == 0:\n",
    "            return 'finished'\n",
    "        else:\n",
    "            print(x, ' | ', y)\n",
    "            return bar((x-1), y+x)\n",
    "    \n",
    "    return bar(foo(x, y), y)\n",
    "\n",
    "print(Total(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1: 4\n",
      " step 1: 3\n",
      " step 1: 2\n",
      "move from P1 to P3\n",
      " step 2: 2\n",
      "move from P1 to P2\n",
      " step 3: 2\n",
      "move from P3 to P2\n",
      " step 2: 3\n",
      "move from P1 to P3\n",
      " step 3: 3\n",
      " step 1: 2\n",
      "move from P2 to P1\n",
      " step 2: 2\n",
      "move from P2 to P3\n",
      " step 3: 2\n",
      "move from P1 to P3\n",
      " step 2: 4\n",
      "move from P1 to P2\n",
      " step 3: 4\n",
      " step 1: 3\n",
      " step 1: 2\n",
      "move from P3 to P2\n",
      " step 2: 2\n",
      "move from P3 to P1\n",
      " step 3: 2\n",
      "move from P2 to P1\n",
      " step 2: 3\n",
      "move from P3 to P2\n",
      " step 3: 3\n",
      " step 1: 2\n",
      "move from P1 to P3\n",
      " step 2: 2\n",
      "move from P1 to P2\n",
      " step 3: 2\n",
      "move from P3 to P2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def printMove(n, fr, to):\n",
    "    print('move from ' + str(fr) + ' to ' + str(to))\n",
    "    \n",
    "def Towers(n, fr, to, spare):\n",
    "    if n == 1:\n",
    "        printMove(n, fr, to)\n",
    "    else:\n",
    "        print(f\" step 1: {n}\")\n",
    "        Towers(n-1, fr, spare, to)\n",
    "        print(f\" step 2: {n}\")\n",
    "        Towers(1, fr, to, spare)\n",
    "        print(f\" step 3: {n}\")\n",
    "        Towers(n-1, spare, to, fr)\n",
    "        \n",
    "print(Towers(4, 'P1', 'P2', 'P3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('entity_dict len', len(entity_dict))\n",
    "max_key = entity_dict.pop(max(entity_dict.items(), key=lambda score: score[1]['score'])[0])\n",
    "print('entity_dict len', len(entity_dict))\n",
    "\n",
    "print(max_key)\n",
    "print(max_key['entity'])\n",
    "print(max_key['group'])\n",
    "print(max_key['score'])\n",
    "\n",
    "print('ent_set len: ', len(ent_set))\n",
    "ent_set.remove(max_key['entity'])\n",
    "print('ent_set len: ', len(ent_set))\n",
    "\n",
    "print('outgroup seeds len: ', len(seed_dict['OUT']))\n",
    "seed_dict[max_key['group']].append(max_key['entity'])\n",
    "print('outgroup seeds len: ', len(seed_dict['OUT']))\n",
    "\n",
    "\n",
    "print (\"\\nItems in sorted order\") \n",
    "for key, value in sorted(entity_dict.items(), \n",
    "                         key = lambda score: score[1]['score'],\n",
    "                        reverse = True): \n",
    "    print(key, \" | \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for ent in new_doc.ents: # entity extraction completed by Spacy pipeline \n",
    "    if ent.label_ in {'ORG', 'GPE', 'NORP'} and ent.text in model: #creates a set of entity types and checks if in model: \n",
    "        ent_set.add(ent.text.replace(' ', '_')) # noun phrases in model are created using _\n",
    "       \n",
    "\n",
    "seed_dict = {'IN' : ['friend', 'Friend'],\n",
    "             'OUT' : ['enemy', 'Enemy']}\n",
    "\n",
    "ent_dict = {}                    \n",
    "for i, entity in enumerate(ent_set):    \n",
    "    ent_pres[i] = {'entity': entity, \n",
    "            'ingp_scores' : [{'word': ingp, 'score' : model.similarity(entity, ingp)} for ingp in seed_dict['IN']],\n",
    "            'outgp_scores' : [{'word': outgp, 'score' : model.similarity(entity, outgp)} for outgp in seed_dict['OUT']]\n",
    "           }\n",
    "        \n",
    "for d in ent_dict.values():\n",
    "    print(d['entity'])\n",
    "    print(pd.DataFrame.from_dict(d['ingp_scores']).T)\n",
    "    print('--')\n",
    "    print(pd.DataFrame(d['outgp_scores']).T)\n",
    "    print('-----')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
