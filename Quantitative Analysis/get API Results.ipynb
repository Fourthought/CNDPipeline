{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Google and Watson API Results and store on file\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'Named Entity Matcher', 'merge_entities', 'Concept Matcher']\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import importlib\n",
    "import pipeline\n",
    "importlib.reload(pipeline)\n",
    "\n",
    "cnd = pipeline.CND()\n",
    "\n",
    "print([name for name in cnd.nlp.pipe_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Text Count</th>\n",
       "      <th>Word Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ref</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hitler</th>\n",
       "      <th>0</th>\n",
       "      <td>Adolf Hitler</td>\n",
       "      <td>1</td>\n",
       "      <td>706,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bush</th>\n",
       "      <th>1</th>\n",
       "      <td>George Bush</td>\n",
       "      <td>14</td>\n",
       "      <td>143,936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king</th>\n",
       "      <th>2</th>\n",
       "      <td>Martin Luther King</td>\n",
       "      <td>5</td>\n",
       "      <td>122,815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laden</th>\n",
       "      <th>3</th>\n",
       "      <td>Osama bin Laden</td>\n",
       "      <td>6</td>\n",
       "      <td>93,646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Totals</th>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>26</td>\n",
       "      <td>1,066,497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name Text Count Word Count\n",
       "Ref                                               \n",
       "hitler 0        Adolf Hitler          1    706,100\n",
       "bush   1         George Bush         14    143,936\n",
       "king   2  Martin Luther King          5    122,815\n",
       "laden  3     Osama bin Laden          6     93,646\n",
       "Totals 4                             26  1,066,497"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import importlib\n",
    "from IPython.display import clear_output\n",
    "import cndobjects\n",
    "importlib.reload(cndobjects)\n",
    "\n",
    "\n",
    "dirpath = r'C:\\\\Users\\\\Steve\\\\OneDrive - University of Southampton\\\\CNDPipeline\\\\dataset'\n",
    "\n",
    "orators = cndobjects.Dataset(cnd, dirpath)\n",
    "clear_output(wait=True)\n",
    "\n",
    "display(orators.summarise())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Google API Results\n",
    "\n",
    "Get the data from the online API and store on file to save from repeated calls.\n",
    "\n",
    "Authentication\n",
    "- https://cloud.google.com/docs/authentication/getting-started\n",
    "\n",
    "Dashboard\n",
    "- https://console.cloud.google.com/home/dashboard?project=modern-heading-262419\n",
    "\n",
    "Documents\n",
    "- https://cloud.google.com/natural-language/docs/reference/rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"\"\n",
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get overall document analytics for the Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "document_results = dict()\n",
    "\n",
    "#instantiate dict() for each orator()\n",
    "orators_dict = {}\n",
    "    \n",
    "# iterate through each orator() object\n",
    "for ref, orator in orators.orators_dict.items():\n",
    "    \n",
    "    # instantiate orator dict()\n",
    "    orators_dict[ref] = list()\n",
    "   \n",
    "    \n",
    "    # iterate over each Text() of the orator() object\n",
    "    for text in tqdm(orator.texts, total = len(orator.texts), desc = ref):\n",
    "    # instantiate document dict()\n",
    "        document = dict()\n",
    "        document[\"title\"] = text.title\n",
    "        document[\"analytics\"] = list()\n",
    "\n",
    "        # Mein Kampf is too large for the Google API, the document sentiment is calculated using an average of sentence sentiments\n",
    "        if ref == \"hitler\":\n",
    "            document[\"analytics\"].append(\n",
    "                {\"sentiment\" : 0),\n",
    "                 \"entities\" : '',\n",
    "                 \"classifications\" : '',\n",
    "                 \"syntax\" : ''\n",
    "                })\n",
    "            \n",
    "            orators_dict[ref].append(document)\n",
    "            break\n",
    "        \n",
    "        \n",
    "        doc_obj = types.Document(\n",
    "            content=str(text.doc.text),\n",
    "            type=enums.Document.Type.PLAIN_TEXT)\n",
    "        encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "        document[\"analytics\"] = {\"sentiment\" : client.analyze_sentiment(document=doc_obj).document_sentiment,\n",
    "                                 \"entities\" : client.analyze_entity_sentiment(doc_obj, encoding_type=encoding_type),\n",
    "                                 \"classifications\" : client.classify_text(doc_obj),\n",
    "                                 \"syntax\" : client.analyze_syntax(doc_obj, encoding_type=encoding_type)\n",
    "                                }\n",
    "    \n",
    "        # append the document object to the orator dict()\n",
    "        orators_dict[ref].append(document)\n",
    "        \n",
    "    # append the orator dict() to the overall dataset\n",
    "    document_results.update(orators_dict)\n",
    "    \n",
    "google_document_results = document_results\n",
    "# time = 3min 51s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Google document object to file\n",
    "\n",
    "the NLP objects for Google can't be stored as a json object, therefore they are serialised using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# import os\n",
    "# import pickle\n",
    "# filepath = os.getcwd()\n",
    "# pickle_filename = \"google_document_analytics\"\n",
    "# with open(os.path.join(filepath, pickle_filename), 'wb') as file:\n",
    "#     pickle.dump(google_document_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentence level sentiment analytics for Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "# instantiate dataset dictionary\n",
    "sentence_sentiment = dict()\n",
    "\n",
    "#instantiate dict() for each orator()\n",
    "orators_dict = {}\n",
    "    \n",
    "for ref, orator in orators.orators_dict.items():\n",
    "    \n",
    "    # instantiate orator dict()\n",
    "    orators_dict[ref] = list()\n",
    "    \n",
    "    # iterator through orator() texts\n",
    "    for text in orator.texts:\n",
    "        \n",
    "        # instantiate document dict()\n",
    "        document = dict()\n",
    "        document[\"title\"] = text.title\n",
    "        document[\"sentiments\"] = list()\n",
    "        \n",
    "        # iterator through each sentence of the text and append sentiment score to the sentiments list\n",
    "        for sentence in tqdm(text.doc.sents, total = len(list(text.doc.sents)), desc = document[\"title\"]):\n",
    "            \n",
    "            sent_obj = types.Document(\n",
    "                content=str(sentence).strip(),\n",
    "                type=enums.Document.Type.PLAIN_TEXT)\n",
    "            document[\"sentiments\"].append(client.analyze_sentiment(document=sent_obj).document_sentiment)\n",
    "            \n",
    "        # append the document object to the orator dict()\n",
    "        orators_dict[ref].append(document)\n",
    "    \n",
    "    # append the orator dict() to the overall dataset\n",
    "    sentence_sentiment.update(orators_dict)\n",
    "    \n",
    "google_sentence_sentiment = sentence_sentiment\n",
    "# time = 27min 52s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get average sentence sentiment score for Google API\n",
    "\n",
    "Since Mein Kampf is too large for Google's  open source API, the score is derived from an average of sentence sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference (2 decimal places) =  0.06\n",
      "difference (2 decimal places) =  0.0\n",
      "difference (2 decimal places) =  0.04\n",
      "difference (2 decimal places) =  0.07\n",
      "difference (2 decimal places) =  0.01\n",
      "difference (2 decimal places) =  0.08\n",
      "difference (2 decimal places) =  0.02\n",
      "difference (2 decimal places) =  0.02\n",
      "difference (2 decimal places) =  0.01\n",
      "difference (2 decimal places) =  0.06\n",
      "difference (2 decimal places) =  0.01\n",
      "difference (2 decimal places) =  0.0\n",
      "difference (2 decimal places) =  0.03\n",
      "difference (2 decimal places) =  0.01\n",
      "difference (2 decimal places) =  0.03\n",
      "difference (2 decimal places) =  0.02\n",
      "difference (2 decimal places) =  0.0\n",
      "difference (2 decimal places) =  0.02\n",
      "difference (2 decimal places) =  0.03\n",
      "difference (2 decimal places) =  0.06\n",
      "difference (2 decimal places) =  0.02\n",
      "difference (2 decimal places) =  0.01\n",
      "difference (2 decimal places) =  0.03\n",
      "difference (2 decimal places) =  0.1\n",
      "difference (2 decimal places) =  0.0\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# code for getting average sentence sentiment score\n",
    "sentence_sentiment_mean = mean([sentiment.score for sentiment in google_sentence_sentiment[\"hitler\"][0][\"sentiments\"]])\n",
    "\n",
    "# check for differences between document and sentence level score\n",
    "for ref, document in google_document_analytics.items():\n",
    "    for text in range(len(document)):\n",
    "        if ref == \"hitler\":\n",
    "            continue\n",
    "        doc_score = google_document_analytics[ref][text][\"analytics\"][\"sentiment\"].score \n",
    "        sent_score = mean([sentiment.score for sentiment in google_sentence_sentiment[ref][text][\"sentiments\"]])\n",
    "        n = 2\n",
    "        difference = round(abs(doc_score - sent_score), n)\n",
    "        print(f\"difference ({n} decimal places) =  {difference}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Google document object to file\n",
    "\n",
    "the NLP objects for Google can't be stored as a json object, therefore they are serialised using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# filepath = os.getcwd()\n",
    "# pickle_filename = \"google_sentence_sentiment\"\n",
    "# with open(os.path.join(filepath, pickle_filename), 'wb') as file:\n",
    "#     pickle.dump(google_sentence_sentiment, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc analytics size: 4\n",
      "sentence sentiment size: 4\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "google_document_analytics_filename = \"google_document_analytics\"\n",
    "google_sentence_sentiment_filename = \"google_sentence_sentiment\"\n",
    "\n",
    "filepath = os.getcwd()\n",
    "with open(os.path.join(filepath, google_document_analytics_filename), 'rb') as file:\n",
    "    google_document_analytics = pickle.load(file)\n",
    "    \n",
    "with open(os.path.join(filepath, google_sentence_sentiment_filename), 'rb') as file:\n",
    "    google_sentence_sentiment = pickle.load(file)\n",
    "    \n",
    "print(\"doc analytics size:\", len(google_document_analytics))\n",
    "print(\"sentence sentiment size:\", len(google_sentence_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Watson API Results\n",
    "\n",
    "Get the data from the online API and store on file to save from repeated calls.\n",
    "\n",
    "API Documentation\n",
    "- https://cloud.ibm.com/apidocs/natural-language-understanding\n",
    "\n",
    "Source Code\n",
    "- http://watson-developer-cloud.github.io/python-sdk/v1.0.2/_modules/watson_developer_cloud/natural_language_understanding_v1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Watson API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, ConceptsOptions, EmotionOptions, EntitiesOptions, KeywordsOptions, CategoriesOptions, SentimentOptions\n",
    "\n",
    "apikey = ''\n",
    "url = ''\n",
    "\n",
    "authenticator = IAMAuthenticator(apikey)\n",
    "service = NaturalLanguageUnderstandingV1(version='2019-07-12', authenticator=authenticator)\n",
    "service.set_service_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get overall document analytics for the Watson API\n",
    "\n",
    "Generates output at the for all of Watson's natural langugage processing features.\n",
    "\n",
    "Output object is in jsonlines format and stored to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# instantiate dataset dictionary\n",
    "document_results = dict()\n",
    "\n",
    "#instantiate dict() for each orator()\n",
    "orators_dict = {}\n",
    "    \n",
    "# iterate through each orator() object\n",
    "for ref, orator in orators.orators_dict.items():\n",
    "    \n",
    "    # instantiate orator dict()\n",
    "    orators_dict[ref] = list()\n",
    "   \n",
    "    \n",
    "    # iterate over each Text() of the orator() object\n",
    "    for text in tqdm(orator.texts, total = len(orator.texts), desc = ref):\n",
    "    # instantiate document dict()\n",
    "        document = dict()\n",
    "        document[\"title\"] = text.title\n",
    "        document[\"ref\"] : text.ref\n",
    "        document[\"targets\"] = targets = list(set([str(ent) for ent in text.doc.ents if ent.label_ in [\"GPE\", \"NORP\", \"ORG\", \"PERSON\"]])),\n",
    "        \n",
    "        document[\"analytics\"] = service.analyze(\n",
    "                        text=str(text.doc.text),\n",
    "                        features=Features(\n",
    "                            concepts=ConceptsOptions(limit=50),\n",
    "                            emotion=EmotionOptions(targets = targets),\n",
    "                            entities=EntitiesOptions(emotion=True, sentiment=True),\n",
    "                            keywords=KeywordsOptions(emotion=True, sentiment=True),\n",
    "                            categories=CategoriesOptions(),\n",
    "                            sentiment=SentimentOptions()\n",
    "                        )).get_result()\n",
    "        \n",
    "        # append the document object to the orator dict()\n",
    "        orators_dict[ref].append(document)\n",
    "        \n",
    "    # append the orator dict() to the overall dataset\n",
    "    document_results.update(orators_dict)\n",
    "    \n",
    "watson_document_results = document_results\n",
    "# time = 1m3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Watson document object to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# import json\n",
    "# filepath = os.getcwd()\n",
    "# json_filename = \"watson_document_analytics.json\"\n",
    "# with open(os.path.join(filepath, json_filename), 'w') as file:\n",
    "#     file.write(json.dumps(watson_document_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentence level sentiment analytics for Watson API\n",
    "\n",
    "The output object uses the same structure as the orators dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# instantiate dataset dictionary\n",
    "watson_sentiment = dict()\n",
    "\n",
    "#instantiate dict() for each orator()\n",
    "orators_dict = {}\n",
    "    \n",
    "for ref, orator in orators.orators_dict.items():\n",
    "    \n",
    "    # instantiate orator dict()\n",
    "    orators_dict[ref] = list()\n",
    "    \n",
    "    # iterator through orator() texts\n",
    "    for text in orator.texts:\n",
    "        \n",
    "        # instantiate document dict()\n",
    "        document = dict()\n",
    "        document[\"title\"] = text.title\n",
    "        document[\"sentiments\"] = list()\n",
    "        \n",
    "        # iterator through each sentence of the text and append sentiment score to the sentiments list\n",
    "        for sentence in tqdm(text.doc.sents, total = len(list(text.doc.sents))):\n",
    "            document[\"sentiments\"].append(service.analyze(\n",
    "                text=str(sentence.text),\n",
    "                features=Features(sentiment=SentimentOptions()),\n",
    "                language = \"en\").get_result())\n",
    "            \n",
    "        # aappend the document object to the orator dict()\n",
    "        orators_dict[ref].append(document)\n",
    "    \n",
    "    # append the orator dict() to the overall dataset\n",
    "    watson_sentiment.update(orators_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Watson sentence sentiment document to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# filepath = os.getcwd()\n",
    "# json_filename = \"watson_sentence_sentiment.json\"\n",
    "# with open(os.path.join(filepath, json_filename), 'w') as file:\n",
    "#     file.write(json.dumps(watson_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for loading Watson data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc analytics size: 4\n",
      "sentence sentiment size: 4\n",
      "Wall time: 87.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "watson_document_analytics_filename = \"watson_document_analytics.json\"\n",
    "watson_sentence_sentiment_filename = \"watson_sentence_sentiment.json\"\n",
    "\n",
    "filepath = os.getcwd()\n",
    "with open(os.path.join(filepath, watson_document_analytics_filename), 'r') as file:\n",
    "    watson_document_analytics = json.load(file)\n",
    "    \n",
    "with open(os.path.join(filepath, watson_sentence_sentiment_filename), 'r') as file:\n",
    "    watson_sentence_sentiment = json.load(file)\n",
    "    \n",
    "print(\"doc analytics size:\", len(watson_document_analytics))\n",
    "print(\"sentence sentiment size:\", len(watson_sentence_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Relevant Information to Make a Usuable Object for this Task.\n",
    "\n",
    "Output at the document level is in the following format\n",
    "\n",
    "`\n",
    "\n",
    "\n",
    "    google_document_analytics[\"bush\"][4][\"analytics\"] = #(remember to sort out the metadata clash)\n",
    "    {\"sentiment\": {\"magnitude\": 87.0, \"score\": -0.10000000149011612},\n",
    "        \"entities\": {\"entities\": [{\"name\": \"al Qaeda\", \"type\": \"ORGANIZATION\", \"metadata\": {\"key\": \"mid\", \"value\": \"/m/0v74\"},\n",
    "                                   \"metadata\": {\"key\": \"wikipedia_url\", \"value\": \"https://en.wikipedia.org/wiki/Al-Qaeda\"},\n",
    "                                   \"salience\": 0.009386186487972736,\n",
    "                                   \"mentions\": [{\"text\": {\"content\": \"al Qaeda\", \"begin_offset\": 4680}, \"type\": \"PROPER\", \"sentiment\": {\"magnitude\": 0.699999988079071, \"score\": 0.699999988079071}}]}]},\n",
    "        \"classifications\": {\"categories\": {\"name\": \"/Sensitive Subjects\",\n",
    "                                         \"confidence\": 0.7900000214576721}},\n",
    "        \"syntax\": {\n",
    "        \"sentences\": {\"text\": {\"content\": \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\", \"begin_offset\": 3309}},\n",
    "        \"tokens\": {\"text\": {\"content\": \"terrorist\", \"begin_offset\": 3388},\n",
    "          \"part_of_speech\": {\"tag\": \"ADJ\"},\n",
    "          \"dependency_edge\": {\"head_token_index\": 679, \"label\": \"AMOD\"},\n",
    "          \"lemma\": \"terrorist\"}}}\n",
    "\n",
    "`\n",
    "\n",
    "`\n",
    "\n",
    "    google_sentence_sentiment[\"bush\"][4][\"sentiments\"][32].magnitude = 0.20000000298023224\n",
    "\n",
    "    google_sentence_sentiment[\"bush\"][4][\"sentiments\"][32].magnitude.score = -0.20000000298023224\n",
    "\n",
    "`\n",
    "\n",
    "Outputs for Watson are the following format:\n",
    "\n",
    "`\n",
    "    \n",
    "    watson_document_analytics[\"bush\"][4][\"analytics\"] = \n",
    "    {\"usage\": {\"text_units\": 2, \"text_characters\": \"17321\", \"features\": 5},\n",
    "     \"sentiment\": {\"document\": {\"score\": -0.331922, \"label\": \"negative\"}},\n",
    "     \"language\": \"en\",\n",
    "     \"keywords\": [{\"text\": \"United States of America\",\n",
    "                   \"sentiment\": {\"score\": -0.535138,\n",
    "                                 \"mixed\": \"1\",\n",
    "                                 \"label\": \"negative\"},\n",
    "                   \"relevance\": 0.527585,\n",
    "                   \"emotion\": {\"sadness\": 0.28403,\n",
    "                               \"joy\": 0.355131,\n",
    "                               \"fear\": 0.328408,\n",
    "                               \"disgust\": 0.099274,\n",
    "                               \"anger\": 0.113816},\n",
    "                   \"count\": 3}],\n",
    "     \"entities\": [{\"type\": \"Organization\", \n",
    "                   \"text\": \"al Qaeda\",\n",
    "                   \"sentiment\": {\"score\": -0.738021, \n",
    "                                 \"mixed\": \"1\", \n",
    "                                 \"label\": \"negative\"},\n",
    "                   \"relevance\": 0.624347,\n",
    "                   \"emotion\": {\"sadness\": 0.174989,\n",
    "                               \"joy\": 0.139256,\n",
    "                               \"fear\": 0.699752,\n",
    "                               \"disgust\": 0.154138,\n",
    "                               \"anger\": 0.129947},\n",
    "                   \"disambiguation\": {\"subtype\": [\"MembershipOrganization\"],\n",
    "                                      \"name\": \"Al-Qaeda\", \"dbpedia_resource\": \"http://dbpedia.org/resource/Al-Qaeda\"},\n",
    "                   \"count\": 6,\n",
    "                   \"confidence\": 1}],\n",
    "     \"concepts\": {\"text\": \"Taliban\", \"relevance\": 0.695602, \"dbpedia_resource\": \"http://dbpedia.org/resource/Taliban\"},\n",
    "     \"categories\": [{\"score\": 0.954255, \"label\": \"/society/unrest and war\"}]\n",
    "    }\n",
    "\n",
    "`\n",
    "                                                            \n",
    "`\n",
    "    \n",
    "    watson_sentence_sentiment[\"bush\"][4][\"sentiments\"][32] = {'usage': \n",
    "                                                             {'text_units': 1, 'text_characters': 134, 'features': 1}, \n",
    "                                                              'sentiment': {'document': {'score': 0.920489, 'label': 'positive'}}, \n",
    "                                                              'language': 'en'}\n",
    "                                                        \n",
    "`\n",
    "                                                    \n",
    "The relevant information is as follows:\n",
    "\n",
    "`\n",
    "    \n",
    "    watson_sentiment[\"bush\"][4][\"sentiments\"][32][\"sentiment\"][\"document\"] = 'scores': {'watson': {'score': 0, 'label': 'neutral'}}\n",
    "\n",
    "`\n",
    "\n",
    "the output document object for each orator will be a list() of dict() in the following format:\n",
    "\n",
    "`\n",
    "    \n",
    "    document = {\"title\" : \"\",\n",
    "        \"sentiment_scores\" : {\"textblob\" : score, \"watson\" : score, \"google\" : score},\n",
    "        \"most_pos_sents\" : {\"textblob\" : [(index)], \"watson\" : [(index)], \"google\" : [(index)]}, # list of sentences with a score of +1\n",
    "        \"most_neg_sents\" : {\"textblob\" : [(index)], \"watson\" : [(index)], \"google\" : [(index)]}, # list of sentence indicies with a score of -1\n",
    "        \"pos_sents\" : {\"textblob\" : [(index, score)], \"watson\" : [(index, score)], \"google\" : [(index, score)]}, # sentence indicies with highest score other than +1\n",
    "        \"neg_sents\" : {\"textblob\" : [(index, score)], \"watson\" : [(index, score)], \"google\" : [(index, score)]}, # sentence indicies with lowest score other than +1\n",
    "        \"sentences\" : []}\n",
    "\n",
    "`\n",
    "\n",
    "The output sentence object for each sentence will a list of dict() objects:\n",
    "\n",
    "`\n",
    "    \n",
    "    sent_obj = {\"text\" : \"\", \"scores\" : { \n",
    "        \"watson\" : 0,\n",
    "        \"google\" : 0,\n",
    "        \"textblob\" : 0\n",
    "        }}\n",
    "\n",
    "`\n",
    "\n",
    "Each sentence object is accessed as follows:\n",
    "\n",
    "`\n",
    "    \n",
    "    <API>_sentiment_analysis[\"bush\"][4][\"sentences\"][32] = \n",
    "    {'text': 'The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.',\n",
    "    'scores': {'watson': {'score': 0, 'label': 'neutral'}}}\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mein Kampf: 100%|██████████| 4527/4527 [00:01<00:00, 2723.18it/s]\n",
      "911 Address to the Nation: 100%|██████████| 37/37 [00:00<00:00, 2318.58it/s]\n",
      "Remarks at the National Day of Prayer & Remembrance Service: 100%|██████████| 57/57 [00:00<00:00, 2041.22it/s]\n",
      "First Radio Address following 911: 100%|██████████| 30/30 [00:00<00:00, 2313.67it/s]\n",
      "Address at Islamic Center of Washington, D.C.: 100%|██████████| 39/39 [00:00<00:00, 2172.15it/s]\n",
      "Address to Joint Session of Congress Following 911 Attacks: 100%|██████████| 186/186 [00:00<00:00, 3727.79it/s]\n",
      "Operation Enduring Freedom in Afghanistan Address to the Nation: 100%|██████████| 57/57 [00:00<00:00, 2381.44it/s]\n",
      "911 Pentagon Remembrance Address: 100%|██████████| 93/93 [00:00<00:00, 2451.73it/s]\n",
      "Prime Time News Conference on War on Terror: 100%|██████████| 37/37 [00:00<00:00, 1612.94it/s]\n",
      "Prime Time News Conference Q&A: 100%|██████████| 410/410 [00:00<00:00, 3453.75it/s]\n",
      "Address on Signing the USA Patriot Act of 2001: 100%|██████████| 64/64 [00:00<00:00, 2069.92it/s]\n",
      "First Address to the United Nations General Assembly: 100%|██████████| 177/177 [00:00<00:00, 3549.67it/s]\n",
      "Address to Citadel Cadets: 100%|██████████| 179/179 [00:00<00:00, 2848.97it/s]\n",
      "The World Will Always Remember 911: 100%|██████████| 34/34 [00:00<00:00, 3774.55it/s]\n",
      "First (Official) Presidential State of the Union Address: 100%|██████████| 221/221 [00:00<00:00, 3355.94it/s]\n",
      "Ive Been to the Mountaintop: 100%|██████████| 300/300 [00:00<00:00, 2179.48it/s]\n",
      "I Have a Dream: 100%|██████████| 93/93 [00:00<00:00, 1695.20it/s]\n",
      "Our God is Marching On: 100%|██████████| 315/315 [00:00<00:00, 2305.35it/s]\n",
      "Beyond Vietnam: 100%|██████████| 312/312 [00:00<00:00, 2463.27it/s]\n",
      "The Other America: 100%|██████████| 291/291 [00:00<00:00, 1747.19it/s]\n",
      "Declaration of Jihad Against the Americans Occupying the Land of the Two Holiest Sites: 100%|██████████| 326/326 [00:00<00:00, 2723.92it/s]\n",
      "Osama Bin Laden Letter Calling For Global Islamic State: 100%|██████████| 29/29 [00:00<00:00, 2414.49it/s]\n",
      "Al-Jazirah Carries Bin Ladin's Address on US Strikes: 100%|██████████| 45/45 [00:00<00:00, 2148.43it/s]\n",
      "Bin Laden's Statement The Sword Fell: 100%|██████████| 33/33 [00:00<00:00, 1741.93it/s]\n",
      "OBL Letter to America: 100%|██████████| 215/215 [00:00<00:00, 2994.15it/s]\n",
      "Al Jazeera Speech: 100%|██████████| 92/92 [00:00<00:00, 2795.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "    \n",
    "# instantiate dataset dictionary\n",
    "sentiment_analysis = dict()\n",
    "\n",
    "#instantiate dict() for each orator()\n",
    "orators_dict = {}\n",
    "\n",
    "#access each orator\n",
    "for ref, orator in orators.orators_dict.items():\n",
    "    \n",
    "    # instantiate orator dict()\n",
    "    orators_dict[ref] = list()\n",
    "    \n",
    "    # iterator through orator() texts as a index\n",
    "    for text in range(len(orator.texts)):\n",
    "        \n",
    "        # instantiate document dict()\n",
    "        document = dict()\n",
    "        document[\"ref\"] = ref\n",
    "        document[\"datestamp\"] = orators[ref][text].datestamp\n",
    "        document[\"title\"] = orators[ref][text].title\n",
    "        document[\"word count\"] = len(orators[ref][text].doc.text)\n",
    "        scores_obj = dict()\n",
    "        scores_obj[\"textblob\"] = list()\n",
    "        scores_obj[\"watson\"] = list()\n",
    "        scores_obj[\"google\"] = list()\n",
    "        document[\"most_pos_sents\"] = scores_obj # list of sentences with a score of +1\n",
    "        document[\"most_neg_sents\"] = scores_obj # list of sentence indicies with a score of -1\n",
    "        \n",
    "        scores_obj = dict()\n",
    "        scores_obj[\"textblob\"] = None\n",
    "        scores_obj[\"watson\"] = None\n",
    "        scores_obj[\"google\"] = None\n",
    "        document[\"pos_sents\"] = scores_obj # sentence indicies with highest score other than +1\n",
    "        document[\"neg_sents\"] = scores_obj # sentence indicies with lowest score other than +1\n",
    "        \n",
    "        # add document level sentiment results\n",
    "        document[\"sentiment_scores\"] = dict()\n",
    "        \n",
    "        #add textblob scores\n",
    "        document[\"sentiment_scores\"][\"textblob\"] = TextBlob(str(orators[ref][text].doc.text)).sentiment[0]\n",
    "        \n",
    "        # add watson scores\n",
    "        document[\"sentiment_scores\"][\"watson\"] = watson_document_analytics[ref][text][\"analytics\"][\"sentiment\"][\"document\"][\"score\"]\n",
    "        \n",
    "        # add google scores\n",
    "        if ref == \"hitler\":\n",
    "             document[\"sentiment_scores\"][\"google\"] = mean([sentiment.score for sentiment in google_sentence_sentiment[\"hitler\"][0][\"sentiments\"]])\n",
    "        else:\n",
    "            document[\"sentiment_scores\"][\"google\"] = google_document_analytics[ref][text][\"analytics\"][\"sentiment\"].score\n",
    "        \n",
    "        # instantiate list of sentences for the sentence objects\n",
    "        document[\"sentences\"] = list() \n",
    "        \n",
    "        # create a list of sentence texts\n",
    "        sentence_text = list(orators[ref][text].doc.sents)\n",
    "        \n",
    "        textblob_max = 0\n",
    "        textblob_min = 0\n",
    "        watson_max = 0\n",
    "        watson_min = 0\n",
    "        google_max = 0\n",
    "        google_min = 0\n",
    "        api = \"\"\n",
    "\n",
    "        # iterate through the sentence_texts using an index\n",
    "        for sentence in tqdm(range(len(sentence_text)), total = len(sentence_text), desc = document[\"title\"]):\n",
    "            \n",
    "            # initiate sent object\n",
    "            sent = dict()\n",
    "            \n",
    "            # get the sentence text\n",
    "            sent[\"text\"] = str(sentence_text[sentence]).strip()\n",
    "            \n",
    "            # initate scores object\n",
    "            sent[\"scores\"] = dict()\n",
    "            \n",
    "            # add textblob scores\n",
    "            api = \"textblob\"\n",
    "            sent[\"scores\"][api] = TextBlob(sent[\"text\"]).sentiment[0]\n",
    "            \n",
    "            # initaite sentence scores object for max-min analysis\n",
    "            sent_score = {\"text\" : sent[\"text\"], \"score\" : sent[\"scores\"][api]}\n",
    "            \n",
    "            if sent[\"scores\"][api] == 1:\n",
    "#                 print(f'most_pos_sents ({sent[\"scores\"][api]}) => {sent[\"text\"]}')\n",
    "                document[\"most_pos_sents\"][api].append(sent[\"text\"])\n",
    "            \n",
    "            if sent[\"scores\"][api] == -1:\n",
    "#                 print(f'most_neg_sents ({sent[\"scores\"][api]}) => {sent[\"text\"]}')\n",
    "                document[\"most_neg_sents\"][api].append(sent[\"text\"])\n",
    "            \n",
    "            if sent[\"scores\"][api] > 0 and sent[\"scores\"][api] < 1 and sent[\"scores\"][api] > textblob_max:\n",
    "                textblob_max = sent[\"scores\"][api]\n",
    "#                 print(f'pos_sents ({sent[\"scores\"][api]}) => {sent[\"text\"]}')\n",
    "                document[\"pos_sents\"][api] = sent_score\n",
    "            \n",
    "            if sent[\"scores\"][api] < 0 and sent[\"scores\"][api] > -1 and sent[\"scores\"][api] < textblob_min:\n",
    "                textblob_min = sent[\"scores\"][api]\n",
    "#                 print(f'neg_sents ({sent[\"scores\"][api]}) => {sent[\"text\"]}')\n",
    "                document[\"neg_sents\"][api] = sent_score\n",
    "            \n",
    "            # add watson scores\n",
    "            api = \"watson\"\n",
    "            sent[\"scores\"][api] = watson_sentence_sentiment[ref][text][\"sentiments\"][sentence][\"sentiment\"][\"document\"][\"score\"]\n",
    "            \n",
    "            if sent[\"scores\"][api] == 1:\n",
    "                document[\"most_pos_sents\"][api].append(sent[\"text\"])\n",
    "            \n",
    "            if sent[\"scores\"][api] == -1:\n",
    "                document[\"most_neg_sents\"][api].append(sent[\"text\"])\n",
    "            \n",
    "            if sent[\"scores\"][api] > 0 and sent[\"scores\"][api] < 1 and sent[\"scores\"][api] > watson_max:\n",
    "                watson_max = sent[\"scores\"][api]\n",
    "                document[\"pos_sents\"][api] = sent_score\n",
    "            \n",
    "            if sent[\"scores\"][api] < 0 and sent[\"scores\"][api] > -1 and sent[\"scores\"][api] < watson_min:\n",
    "                watson_min = sent[\"scores\"][api]\n",
    "                document[\"neg_sents\"][api] = sent_score\n",
    "            \n",
    "            # add google scores\n",
    "            api = \"google\"\n",
    "            sent[\"scores\"][api] = google_sentence_sentiment[ref][text][\"sentiments\"][sentence].score\n",
    "            \n",
    "            if sent[\"scores\"][api] == 1:\n",
    "                document[\"most_pos_sents\"][api].append(sent[\"text\"])\n",
    "            \n",
    "            if sent[\"scores\"][api] == -1:\n",
    "                document[\"most_neg_sents\"][api].append(sent[\"text\"])\n",
    "            \n",
    "            if sent[\"scores\"][api] < 1 and sent[\"scores\"][api] > google_max:\n",
    "                google_max = sent[\"scores\"][api]\n",
    "                document[\"pos_sents\"][api] = sent_score\n",
    "            \n",
    "            if sent[\"scores\"][api] > -1 and sent[\"scores\"][api] < google_min:\n",
    "                google_min = sent[\"scores\"][api]\n",
    "                document[\"neg_sents\"][api] = sent_score\n",
    "            \n",
    "            # append the sent object to the list of document objects\n",
    "            document[\"sentences\"].append(sent)\n",
    "        \n",
    "        # append the document object to the orator dict()\n",
    "        orators_dict[ref].append(document)\n",
    "    \n",
    "    # append the orator dict() to the overall dataset\n",
    "    sentiment_analysis.update(orators_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the sentence sentiments object to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "filepath = os.getcwd()\n",
    "pickle_filename = \"sentiment_analysis\"\n",
    "with open(os.path.join(filepath, pickle_filename), 'wb') as file:\n",
    "    pickle.dump(sentiment_analysis, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "filepath = os.getcwd()\n",
    "pickle_filename = \"sentiment_analysis\"\n",
    "with open(os.path.join(filepath, pickle_filename), 'rb') as file:\n",
    "    sentiment_analysis = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
