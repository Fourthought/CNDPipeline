{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/explosion/spaCy/blob/9ce059dd067ecc3f097d04023e3cfa0d70d35bb8/spacy/tokens/doc.pyx\n",
    "\n",
    "https://github.com/explosion/spaCy/blob/f49e2810e6ea5c8b848df5b0f393c27ee31bb7f4/spacy/tokens/span.pyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def custom_chunks(doclike):\n",
    "    \"\"\"\n",
    "    custom extension for noun_chunks incorporating custom attributes\n",
    "    \n",
    "    adapted from noun_chunk extension in spaCy library\n",
    "    https://github.com/explosion/spaCy/blob/master/spacy/lang/en/syntax_iterators.py\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        \"nsubj\",\n",
    "        \"dobj\",\n",
    "        \"nsubjpass\",\n",
    "        \"pcomp\",\n",
    "        \"pobj\",\n",
    "        \"dative\",\n",
    "        \"appos\",\n",
    "        \"attr\",\n",
    "        \"ROOT\",\n",
    "    ]\n",
    "    doc = doclike.doc  # Ensure works on both Doc and Span.\n",
    "\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add(\"conj\")\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    prev_end = -1\n",
    "\n",
    "    predicates = [\n",
    "            'able', 'available', 'brief', 'certain',\n",
    "            'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "            'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "            'its', 'last', 'latter', 'least', 'less', 'likely', 'little',\n",
    "            'many', 'ml', 'more', 'most', 'much', 'my', 'necessary',\n",
    "            'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',\n",
    "            'particular', 'past', 'possible', 'present', 'proud', 'recent',\n",
    "            'same', 'several', 'significant', 'similar', 'some', 'such', 'sup', 'sure'\n",
    "        ]\n",
    "\n",
    "    for i, word in enumerate(doclike):\n",
    "        \n",
    "        if word.pos_ not in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "            continue\n",
    "        \n",
    "        # Prevent nested chunks from being produced\n",
    "        if word.left_edge.i <= prev_end:\n",
    "            continue\n",
    "        \n",
    "        if word.dep in np_deps:\n",
    "            prev_end = word.i\n",
    "            # print(word.left_edge.i, '=>', prev_end)\n",
    "            # print(\"first yield: \", doc[word.left_edge.i: word.i + 1])\n",
    "            # yield word.left_edge.i, word.i + 1, np_label\n",
    "\n",
    "            span = Span(doc, word.left_edge.i, word.i + 1)\n",
    "\n",
    "            counter = 0\n",
    "            for tok in span:\n",
    "                if tok.lemma_ in predicates:\n",
    "                    counter += 1\n",
    "            \n",
    "            #remove empty spans, eg the noun_chunk 'others' becomes a zero length span\n",
    "            if len(span[counter:]) == 0:\n",
    "                counter = 0\n",
    "\n",
    "            yield span[counter:]\n",
    "    \n",
    "        elif word.dep == conj:\n",
    "            head = word.head\n",
    "            while head.dep == conj and head.head.i < head.i:\n",
    "                head = head.head\n",
    "            # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "            if head.dep in np_deps:\n",
    "                prev_end = word.i\n",
    "                # print(\"second yield: \", doc[word.left_edge.i: word.i + 1])\n",
    "                # yield word.left_edge.i, word.i + 1, np_label\n",
    "                span = Span(doc, word.left_edge.i, word.i + 1)\n",
    "\n",
    "                counter = 0\n",
    "                for tok in span:\n",
    "                    if tok.lemma_ in predicates:\n",
    "                        counter += 1\n",
    "                \n",
    "                #remove empty spans, eg the noun_chunk 'others' becomes a zero length span\n",
    "                if len(span[counter:]) == 0:\n",
    "                    counter = 0\n",
    "                        \n",
    "                yield span[counter:]\n",
    "\n",
    "def merge_named_concepts(doc):\n",
    "\n",
    "    \"\"\"Merge named concepts into a single token.\n",
    "    doc (Doc): The Doc object.\n",
    "    RETURNS (Doc): The Doc object with merged named concepts.\n",
    "\n",
    "    Adapts the spacy merge noun chunks function\n",
    "    code: https://github.com/explosion/spaCy/blob/master/spacy/pipeline/functions.py\n",
    "    \"\"\"\n",
    "    if not doc.is_parsed:\n",
    "        return doc\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in doc._.named_concepts:\n",
    "            attrs = {\n",
    "                    \"tag\": span.root.tag, \n",
    "                    \"dep\": span.root.dep\n",
    "                    }\n",
    "\n",
    "            retokenizer.merge(span, attrs=attrs)\n",
    "    # \"_\" : {\"CONCEPT\" : span._.CONCEPT,\n",
    "    #                         \"IDEOLOGY\" : span._.IDEOLOGY,\n",
    "    #                         \"ATTRIBUTE\" : span._.ATTRIBUTE},\n",
    "    \n",
    "    return doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
