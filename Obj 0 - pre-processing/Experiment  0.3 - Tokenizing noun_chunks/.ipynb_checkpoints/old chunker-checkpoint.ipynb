{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "import pipeline\n",
    "import cndutils as ut\n",
    "import visuals as viz\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\"\n",
    "test_jsonl = \"test_chunks.jsonl\"\n",
    "cust_jsonl = \"cust_chunks.jsonl\"\n",
    "index_str = \"index.json\"\n",
    "test_filepath = os.path.join(path, test_jsonl)\n",
    "cust_filepath = os.path.join(path, cust_jsonl)\n",
    "index_filepath = os.path.join(path, index_str)\n",
    "\n",
    "with jsonlines.open(test_filepath) as f:\n",
    "    test_chunks = list(f.iter())\n",
    "    \n",
    "try:  \n",
    "    with jsonlines.open(cust_filepath) as f:\n",
    "        cust_chunk_list = list(f.iter())\n",
    "    if len(test_chunks) == 0:\n",
    "        cust_chunk_list = list()    \n",
    "\n",
    "except:\n",
    "    cust_chunk_list = list()\n",
    "\n",
    "try:\n",
    "    with open(index_filepath, \"r\") as index_json:\n",
    "        index = json.load(index_json)\n",
    "        \n",
    "except:\n",
    "    index = 0\n",
    "    \n",
    "lookup = pipeline.ConceptMatcher(cnd.nlp)\n",
    "    \n",
    "while index < len(test_chunks):\n",
    "    \n",
    "    quit = False\n",
    "    \n",
    "    line = test_chunks[index]\n",
    "            \n",
    "    with open(index_filepath, \"wb\") as f:\n",
    "        f.write(json.dumps(index).encode(\"utf-8\"))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "  \n",
    "    #parse document\n",
    "    doc = cnd(line[str(index)])\n",
    "    \n",
    "    # display dependency parse\n",
    "    displacy.render(doc, style = \"dep\")\n",
    "              \n",
    "    display(viz.sent_frame(doc, compact = False))\n",
    "    \n",
    "    # collate, create and display a dict of merged noun_chunks\n",
    "    chunk_dict = dict()    \n",
    "\n",
    "    # get the noun root accounting for pronoun lemma\n",
    "    root = \"\"\n",
    "    for n, chunk in enumerate(doc.noun_chunks):\n",
    "        idx = chunk.root.i\n",
    "        root =  chunk.root.lemma_.lower() \n",
    "        if root == \"-pron-\":\n",
    "            root = chunk.text.lower()\n",
    "    \n",
    "        chunk_dict[n] = {\"text\" : str(chunk).lower(), \"root\" : root, \"idx\" : idx, \"concept\" : str(chunk._.CONCEPT), \"attribute\" : str(chunk._.ATTRIBUTE), \"ideology\" : str(chunk._.IDEOLOGY)}\n",
    "    \n",
    "    display(pd.DataFrame(chunk_dict))\n",
    "    \n",
    "    # iterate through noun chunks to get desired text and notes for each and\n",
    "    # create a dict object of the new chunks data.\n",
    "    new_chunk_dict = dict()\n",
    "    for key, value in chunk_dict.items():\n",
    "        \n",
    "        chunk_str = value[\"text\"]\n",
    "        root = value[\"root\"]\n",
    "        idx = value[\"idx\"]\n",
    "        concept = value[\"concept\"]\n",
    "        attribute = value[\"attribute\"]\n",
    "        ideology = value[\"ideology\"]\n",
    "        notes = \"\"\n",
    "\n",
    "        print(f'{key} => {chunk_str}')\n",
    "        \n",
    "        ## get chunk string\n",
    "        temp_str = \"\"\n",
    "        det = False\n",
    "        if cnd(chunk_str)[0].pos_ == \"DET\":\n",
    "            det = True\n",
    "            temp_str = \" \".join(chunk_str.split()[1:])\n",
    "        else:\n",
    "            temp_str = chunk_str\n",
    "        chunk_str = input(f\"new chunk text [{temp_str}]: \").lower()\n",
    "        \n",
    "        if len(chunk_str) == 0:\n",
    "            chunk_str = temp_str\n",
    "            \n",
    "        # does the user want to quit?\n",
    "        elif chunk_str == \"q\":\n",
    "            print(\"QUITTING\")\n",
    "            quit = True\n",
    "            break\n",
    "            \n",
    "        # get notes\n",
    "        temp_notes = \"\"\n",
    "        if det:\n",
    "            temp_notes = \"remove det. concepts from root.\"\n",
    "        notes = input(f\"notes [{temp_notes}]: \")\n",
    "        if len(notes) == 0:\n",
    "            notes = temp_notes\n",
    "\n",
    "        # get root\n",
    "        root = input(f\"root [{root}]: \").lower()\n",
    "        if len(root) == 0:\n",
    "            root = value[\"root\"]\n",
    "\n",
    "        # get idx\n",
    "        if chunk_str == \"nil\":\n",
    "            idx = \"nil\"\n",
    "        idx = input(f\"idx [{idx}]: \").lower()\n",
    "        if chunk_str == \"nil\" and len(idx) == 0:\n",
    "            idx = \"nil\"\n",
    "        elif len(idx) == 0:\n",
    "            idx = value[\"idx\"]\n",
    "\n",
    "        # concept lookup\n",
    "        concept_lookup = lookup.get_concept(root)\n",
    "        concept = input(f\"concept [{concept_lookup}]:\").upper()\n",
    "        if len(concept) == 0:\n",
    "            concept = concept_lookup\n",
    "\n",
    "        # attribute lookup\n",
    "        attribute_lookup = lookup.get_attribute(concept.lower())\n",
    "        attribute = input(f\"attribute [{attribute_lookup}]: \").lower()\n",
    "        if len(attribute) == 0:\n",
    "            attribute = attribute_lookup\n",
    "\n",
    "        # ideology lookup\n",
    "        ideology_lookup = lookup.get_ideology(concept.lower())\n",
    "        ideology = input(f\"ideology [{ideology_lookup}]: \").lower()\n",
    "        if len(ideology) == 0:\n",
    "            ideology = ideology_lookup\n",
    "        \n",
    "        new_chunk_dict[key] = {\"text\" : chunk_str, \"root\" : root, \"idx\" : idx, \"concept\" : concept, \"attribute\" : attribute, \"ideology\" : ideology, \"notes\" : notes}\n",
    "        \n",
    "    # if quit has been selected then quit\n",
    "    if quit == True:\n",
    "        break\n",
    "        \n",
    "    # append the original and new chunk dicts to jsonl object in json readable format   \n",
    "    line.update({\"orig_chunks\" : ut.doubleQuoteDict(chunk_dict)})\n",
    "    line.update({\"new_chunks\" : ut.doubleQuoteDict(new_chunk_dict)})\n",
    "    cust_chunk_list.append(line)\n",
    "    \n",
    "    #write jsonl object to disk\n",
    "    with jsonlines.open(os.path.join(path, cust_filepath), 'w') as writer:\n",
    "        writer.write_all(cust_chunk_list)\n",
    "        \n",
    "    index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
