{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Noun Chunking\n",
    "-----\n",
    "\n",
    "There is a problem whereby spaCy's inbuilt noun_chunks is too course grained for the chunking required for detecting the ingroups and outgroups.\n",
    "\n",
    "For the purposes of the methodology, a more fine grained noun chunking algorithm is required.\n",
    "\n",
    "There are several examples in the test ingroup and outgroup sentences named entities are chunked with other nouns when they would preferable be kept separate.\n",
    "\n",
    "There are also several examples where a noun chunk contains more than one noun of a custom attribute, therefore, the chunk needs to be resolved to a single instance\n",
    "\n",
    "This notebook adapt spaCy's noun_chunk source code and adapt for the specific purpose of this pipeline.\n",
    "\n",
    "Source code at these links:\n",
    "\n",
    "    Noun Chunker Code\n",
    "    \n",
    "    https://github.com/explosion/spaCy/blob/master/spacy/lang/en/syntax_iterators.py\n",
    "\n",
    "    Class extensions\n",
    "\n",
    "    https://github.com/explosion/spaCy/blob/9ce059dd067ecc3f097d04023e3cfa0d70d35bb8/spacy/tokens/doc.pyx\n",
    "\n",
    "    https://github.com/explosion/spaCy/blob/f49e2810e6ea5c8b848df5b0f393c27ee31bb7f4/spacy/tokens/span.pyx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The existing spacy code for noun chunks is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunks(doclike):\n",
    "    \"\"\"\n",
    "    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n",
    "    \n",
    "    source code: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/syntax_iterators.py\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        \"nsubj\",\n",
    "        \"dobj\",\n",
    "        \"nsubjpass\",\n",
    "        \"pcomp\",\n",
    "        \"pobj\",\n",
    "        \"dative\",\n",
    "        \"appos\",\n",
    "        \"attr\",\n",
    "        \"ROOT\",\n",
    "    ]\n",
    "    doc = doclike.doc  # Ensure works on both Doc and Span.\n",
    "\n",
    "    if not doc.is_parsed:\n",
    "        raise ValueError(Errors.E029)\n",
    "\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add(\"conj\")\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    \n",
    "    prev_end = -1\n",
    "    \n",
    "    for i, word in enumerate(doclike):\n",
    "        \n",
    "        if word.pos not in (NOUN, PROPN, PRON):\n",
    "            continue\n",
    "        # Prevent nested chunks from being produced\n",
    "        if word.left_edge.i <= prev_end:\n",
    "            continue\n",
    "        if word.dep in np_deps:\n",
    "            prev_end = word.i\n",
    "            yield word.left_edge.i, word.i + 1, np_label\n",
    "        elif word.dep == conj:\n",
    "            head = word.head\n",
    "            while head.dep == conj and head.head.i < head.i:\n",
    "                head = head.head\n",
    "            # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "            if head.dep in np_deps:\n",
    "                prev_end = word.i\n",
    "                yield word.left_edge.i, word.i + 1, np_label\n",
    "\n",
    "def merge_named_concepts(doc):\n",
    "\n",
    "    \"\"\"Merge named concepts into a single token.\n",
    "    doc (Doc): The Doc object.\n",
    "    RETURNS (Doc): The Doc object with merged named concepts.\n",
    "\n",
    "    Adapts the spacy merge noun chunks function\n",
    "    code: https://github.com/explosion/spaCy/blob/master/spacy/pipeline/functions.py\n",
    "    \"\"\"\n",
    "    if not doc.is_parsed:\n",
    "        return doc\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in doc._.named_concepts:\n",
    "            attrs = {\n",
    "                    \"tag\": span.root.tag, \n",
    "                    \"dep\": span.root.dep\n",
    "                    }\n",
    "\n",
    "            retokenizer.merge(span, attrs=attrs)\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crux of this code is in this section, the purpose of this note book is to determine how this section should be modified to create more fine-grained noun chunks with the correct named concepts:\n",
    "\n",
    "`\n",
    "if word.pos not in (NOUN, PROPN, PRON):\n",
    "            continue\n",
    "        # Prevent nested chunks from being produced\n",
    "        if word.left_edge.i <= prev_end:\n",
    "            continue\n",
    "        if word.dep in np_deps:\n",
    "            prev_end = word.i\n",
    "            yield word.left_edge.i, word.i + 1, np_label\n",
    "        elif word.dep == conj:\n",
    "            head = word.head\n",
    "            while head.dep == conj and head.head.i < head.i:\n",
    "                head = head.head\n",
    "            # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "            if head.dep in np_deps:\n",
    "                prev_end = word.i\n",
    "                yield word.left_edge.i, word.i + 1, np_label\n",
    "`\n",
    "\n",
    "`np_deps` is a list of dependency labels denoting a noun token\n",
    "\n",
    "`prev_end` is a index to ensure subsequent chunks do not overlap with existing chunks\n",
    "\n",
    "`word.left_edge.i` creates a chunk from the root token and all other tokens in its leftwards facing dependency tree.\n",
    "\n",
    "Where `word.left_edge.i` is too course grained, the custom chunk will expand the number of tests to become a more fine grained chunker. For example: \n",
    "\n",
    "There are rightward facing noun chunks also of interest, for example: \n",
    "- \"weapons of mass destruction\": with weapon as the root, the chunk is rightward facing.\n",
    "\n",
    "There are noun chunks containin multiple tokens of interest that need to be resolved to a single annotation, for example:\n",
    "- \"the occupying American enemy\": needs to be resolved to a merged noun chunk annotated as an outgroup\n",
    "- \"the alliance of Jews, Christians, and their agents\": with alliance as the root, this is a rightwards facing group noun chunk\n",
    "\n",
    "Additional functionality for custom attributes will have to be added and there is the need to remove predicate terms for the hearst pattern detection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 66 records from C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\\bush_ingroup_sents.jsonl\n",
      "Loaded 37 records from C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\\bush_outgroup_sents.jsonl\n",
      "Loaded 41 records from C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\\laden_ingroup_sents.jsonl\n",
      "Loaded 113 records from C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\\laden_outgroup_sents.jsonl\n",
      "['bush_ingroup_sents.jsonl', 'bush_outgroup_sents.jsonl', 'laden_ingroup_sents.jsonl', 'laden_outgroup_sents.jsonl']\n"
     ]
    }
   ],
   "source": [
    "## create a dict object of all the ingroup/outgroup sentences\n",
    "import os\n",
    "import cndutils as ut\n",
    "path = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\"\n",
    "\n",
    "sent_dict = dict()\n",
    "jsonl_files = [f for f in os.listdir(path) if os.path.splitext(f)[1] == \".jsonl\" and \"group\" in f]\n",
    "for file in jsonl_files:\n",
    "    data_list = ut.load_jsonl(os.path.join(path, file))\n",
    "    for entry in data_list:\n",
    "        for value in entry.values():\n",
    "            sent_dict[len(sent_dict)] = value\n",
    "            \n",
    "print(jsonl_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'Named Entity Matcher', 'merge_entities', 'Concept Matcher']\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "import pipeline\n",
    "cnd = pipeline.CND()\n",
    "\n",
    "# merge_nps = cnd.nlp.create_pipe(\"merge_noun_chunks\")\n",
    "# cnd.nlp.add_pipe(merge_nps)\n",
    "\n",
    "print([pipe for pipe in cnd.nlp.pipe_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through Test Data for Sentences of Interest\n",
    "\n",
    "These sentences will be used to tune the existing spaCy noun chunker for the purposes of this methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import cndutils\n",
    "importlib.reload(cndutils)\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\"\n",
    "ss = cndutils.sent_select(path = path, file = \"test_sents\")\n",
    "output = ss(cnd.nlp, sent_dict)\n",
    "\n",
    "        \n",
    "    \n",
    "# remove dets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the noun chunks of each sentence of interest to ascertain the required changes\n",
    "\n",
    "iterate over each sentence and review each noun chunk to determine the desired noun chunk, and develop notes to determine what modeifications to the noun chunk doc extension is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trial, we, world, fellow Americans]\n",
      "Americans\n",
      "fellow => AFFILIATE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "def custom_chunks(doclike):\n",
    "    \"\"\"\n",
    "    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n",
    "    \n",
    "    source code: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/syntax_iterators.py\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        \"nsubj\",\n",
    "        \"dobj\",\n",
    "        \"nsubjpass\",\n",
    "        \"pcomp\",\n",
    "        \"pobj\",\n",
    "        \"dative\",\n",
    "        \"appos\",\n",
    "        \"attr\",\n",
    "        \"ROOT\",\n",
    "    ]\n",
    "    \n",
    "    notes = \"\"\n",
    "    \n",
    "    doc = doclike.doc  # Ensure works on both Doc and Span.\n",
    "\n",
    "    if not doc.is_parsed:\n",
    "        raise ValueError(Errors.E029)\n",
    "\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add(\"conj\")\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    \n",
    "    prev_end = -1\n",
    "    \n",
    "    for i, word in enumerate(doclike):\n",
    "        \n",
    "        if word.pos_ not in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "            continue\n",
    "       \n",
    "        # Prevent nested chunks from being produced\n",
    "        if word.left_edge.i <= prev_end:\n",
    "            continue\n",
    "        \n",
    "        if word.dep in np_deps:\n",
    "            prev_end = word.i\n",
    "            \n",
    "            # test whether noun is left or right facing            \n",
    "            # for named entities followed by a noun, merge attrs in merge custom chunks\n",
    "            # append compounds            \n",
    "            # remove det\n",
    "            \n",
    "            include  = [\"compound\"]\n",
    "            exclude = [\"det\", \"poss\"]\n",
    "            left_edge_index = word.left_edge.i\n",
    "            \n",
    "            n = 0\n",
    "            while left_edge_index < i + n: # and doc[word.i + n].dep_ in exclude:\n",
    "                if doc[word.i + n - 1].dep_ in exclude:\n",
    "                    break\n",
    "                else:\n",
    "                    n -= 1\n",
    "\n",
    "            yield doc[word.i + n : word.i + 1]\n",
    "            \n",
    "        elif word.dep == conj:\n",
    "            head = word.head\n",
    "            while head.dep == conj and head.head.i < head.i:\n",
    "                head = head.head\n",
    "            \n",
    "            # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "            if head.dep in np_deps:\n",
    "                prev_end = word.i\n",
    "                yield doc[word.left_edge.i : word.i + 1]\n",
    "\n",
    "Doc.set_extension(\"custom_chunks\", getter = custom_chunks, force = True)\n",
    "\n",
    "text = \"In this trial, we have been reminded and the world has seen that our fellow Americans are generous and kind, resourceful and brave.\"\n",
    "doc = cnd(text)\n",
    "\n",
    "print(list(doc._.custom_chunks))\n",
    "# the shithouse prime minister, a lying prick, I, he]\n",
    "print(doc[16])\n",
    "for t in doc[16].lefts:\n",
    "    if t.dep_ in [\"amod\"]:\n",
    "        print(t, '=>', t._.CONCEPT)\n",
    "        \n",
    "lookup = pipeline.ConceptMatcher(cnd.nlp)\n",
    "lookup.get_concept(\"they\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': \"They have attacked America because we are freedom's home and defender, and the commitment of our Fathers is now the calling of our time.\", 'orig_chunks': {0: {'text': 'they', 'root': 'they', 'i': '0', 'concept': '', 'attribute': '', 'ideology': ''}, 1: {'text': 'america', 'root': 'america', 'i': '3', 'concept': '', 'attribute': '', 'ideology': ''}, 2: {'text': 'we', 'root': 'we', 'i': '5', 'concept': '', 'attribute': '', 'ideology': ''}, 3: {'text': \"freedom's home\", 'root': 'home', 'i': '9', 'concept': '', 'attribute': '', 'ideology': ''}, 4: {'text': 'defender', 'root': 'defender', 'i': '11', 'concept': 'ARMEDGROUP', 'attribute': 'identity', 'ideology': 'military'}, 5: {'text': 'the commitment', 'root': 'commitment', 'i': '15', 'concept': '', 'attribute': '', 'ideology': ''}, 6: {'text': 'our fathers', 'root': 'father', 'i': '18', 'concept': '', 'attribute': '', 'ideology': ''}, 7: {'text': 'the calling', 'root': 'calling', 'i': '22', 'concept': '', 'attribute': '', 'ideology': ''}, 8: {'text': 'our time', 'root': 'time', 'i': '25', 'concept': '', 'attribute': '', 'ideology': ''}}, 'new_chunks': {}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"553fb877115442ec9caa03d2f4c53c69-0\" class=\"displacy\" width=\"4425\" height=\"662.0\" direction=\"ltr\" style=\"max-width: none; height: 662.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">They</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">have</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">attacked</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">America</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">because</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">we</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">freedom</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">home</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">defender,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">commitment</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">our</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">Fathers</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">now</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">calling</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">our</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">time.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-0\" stroke-width=\"2px\" d=\"M70,527.0 C70,352.0 380.0,352.0 380.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,529.0 L62,517.0 78,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-1\" stroke-width=\"2px\" d=\"M245,527.0 C245,439.5 375.0,439.5 375.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,529.0 L237,517.0 253,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-2\" stroke-width=\"2px\" d=\"M420,527.0 C420,439.5 550.0,439.5 550.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550.0,529.0 L558.0,517.0 542.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-3\" stroke-width=\"2px\" d=\"M770,527.0 C770,352.0 1080.0,352.0 1080.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,529.0 L762,517.0 778,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-4\" stroke-width=\"2px\" d=\"M945,527.0 C945,439.5 1075.0,439.5 1075.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,529.0 L937,517.0 953,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-5\" stroke-width=\"2px\" d=\"M420,527.0 C420,177.0 1090.0,177.0 1090.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,529.0 L1098.0,517.0 1082.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-6\" stroke-width=\"2px\" d=\"M1295,527.0 C1295,352.0 1605.0,352.0 1605.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,529.0 L1287,517.0 1303,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-7\" stroke-width=\"2px\" d=\"M1295,527.0 C1295,439.5 1425.0,439.5 1425.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1425.0,529.0 L1433.0,517.0 1417.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-8\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,264.5 1610.0,264.5 1610.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610.0,529.0 L1618.0,517.0 1602.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-9\" stroke-width=\"2px\" d=\"M1645,527.0 C1645,439.5 1775.0,439.5 1775.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1775.0,529.0 L1783.0,517.0 1767.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-10\" stroke-width=\"2px\" d=\"M1645,527.0 C1645,352.0 1955.0,352.0 1955.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1955.0,529.0 L1963.0,517.0 1947.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-11\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,89.5 2145.0,89.5 2145.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2145.0,529.0 L2153.0,517.0 2137.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-12\" stroke-width=\"2px\" d=\"M2345,527.0 C2345,439.5 2475.0,439.5 2475.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,529.0 L2337,517.0 2353,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-13\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,177.0 3190.0,177.0 3190.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,529.0 L2512,517.0 2528,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-14\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,439.5 2650.0,439.5 2650.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2650.0,529.0 L2658.0,517.0 2642.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-15\" stroke-width=\"2px\" d=\"M2870,527.0 C2870,439.5 3000.0,439.5 3000.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,529.0 L2862,517.0 2878,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-16\" stroke-width=\"2px\" d=\"M2695,527.0 C2695,352.0 3005.0,352.0 3005.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3005.0,529.0 L3013.0,517.0 2997.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-17\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,2.0 3200.0,2.0 3200.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3200.0,529.0 L3208.0,517.0 3192.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-18\" stroke-width=\"2px\" d=\"M3220,527.0 C3220,439.5 3350.0,439.5 3350.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3350.0,529.0 L3358.0,517.0 3342.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-19\" stroke-width=\"2px\" d=\"M3570,527.0 C3570,439.5 3700.0,439.5 3700.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3570,529.0 L3562,517.0 3578,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-20\" stroke-width=\"2px\" d=\"M3220,527.0 C3220,264.5 3710.0,264.5 3710.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3710.0,529.0 L3718.0,517.0 3702.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-21\" stroke-width=\"2px\" d=\"M3745,527.0 C3745,439.5 3875.0,439.5 3875.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3875.0,529.0 L3883.0,517.0 3867.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-22\" stroke-width=\"2px\" d=\"M4095,527.0 C4095,439.5 4225.0,439.5 4225.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4095,529.0 L4087,517.0 4103,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-553fb877115442ec9caa03d2f4c53c69-0-23\" stroke-width=\"2px\" d=\"M3920,527.0 C3920,352.0 4230.0,352.0 4230.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-553fb877115442ec9caa03d2f4c53c69-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4230.0,529.0 L4238.0,517.0 4222.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>They</td>\n",
       "      <td>have</td>\n",
       "      <td>attacked</td>\n",
       "      <td>America</td>\n",
       "      <td>because</td>\n",
       "      <td>we</td>\n",
       "      <td>are</td>\n",
       "      <td>freedom</td>\n",
       "      <td>'s</td>\n",
       "      <td>home</td>\n",
       "      <td>and</td>\n",
       "      <td>defender</td>\n",
       "      <td>,</td>\n",
       "      <td>and</td>\n",
       "      <td>the</td>\n",
       "      <td>commitment</td>\n",
       "      <td>of</td>\n",
       "      <td>our</td>\n",
       "      <td>Fathers</td>\n",
       "      <td>is</td>\n",
       "      <td>now</td>\n",
       "      <td>the</td>\n",
       "      <td>calling</td>\n",
       "      <td>of</td>\n",
       "      <td>our</td>\n",
       "      <td>time</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemma</th>\n",
       "      <td>-PRON-</td>\n",
       "      <td>have</td>\n",
       "      <td>attack</td>\n",
       "      <td>America</td>\n",
       "      <td>because</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>be</td>\n",
       "      <td>freedom</td>\n",
       "      <td>'s</td>\n",
       "      <td>home</td>\n",
       "      <td>and</td>\n",
       "      <td>defender</td>\n",
       "      <td>,</td>\n",
       "      <td>and</td>\n",
       "      <td>the</td>\n",
       "      <td>commitment</td>\n",
       "      <td>of</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>father</td>\n",
       "      <td>be</td>\n",
       "      <td>now</td>\n",
       "      <td>the</td>\n",
       "      <td>calling</td>\n",
       "      <td>of</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>time</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ent_type</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>GPE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>PRON</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VERB</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>PRON</td>\n",
       "      <td>AUX</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>PART</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>AUX</td>\n",
       "      <td>ADV</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <td>PRP</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VBN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>IN</td>\n",
       "      <td>PRP</td>\n",
       "      <td>VBP</td>\n",
       "      <td>NN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NN</td>\n",
       "      <td>CC</td>\n",
       "      <td>NN</td>\n",
       "      <td>,</td>\n",
       "      <td>CC</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>IN</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>NNS</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>RB</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>IN</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>NN</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep</th>\n",
       "      <td>nsubj</td>\n",
       "      <td>aux</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dobj</td>\n",
       "      <td>mark</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>advcl</td>\n",
       "      <td>poss</td>\n",
       "      <td>case</td>\n",
       "      <td>attr</td>\n",
       "      <td>cc</td>\n",
       "      <td>conj</td>\n",
       "      <td>punct</td>\n",
       "      <td>cc</td>\n",
       "      <td>det</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>prep</td>\n",
       "      <td>poss</td>\n",
       "      <td>pobj</td>\n",
       "      <td>conj</td>\n",
       "      <td>advmod</td>\n",
       "      <td>det</td>\n",
       "      <td>attr</td>\n",
       "      <td>prep</td>\n",
       "      <td>poss</td>\n",
       "      <td>pobj</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MILACTION</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>BENEVOLANCE</td>\n",
       "      <td></td>\n",
       "      <td>PLACE</td>\n",
       "      <td></td>\n",
       "      <td>ARMEDGROUP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FAMILY</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attribute</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>trade</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>trade</td>\n",
       "      <td></td>\n",
       "      <td>entity</td>\n",
       "      <td></td>\n",
       "      <td>identity</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ingroup</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ideology</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>military</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>social</td>\n",
       "      <td></td>\n",
       "      <td>social</td>\n",
       "      <td></td>\n",
       "      <td>military</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>social</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0     1          2        3        4       5      6   \\\n",
       "text         They  have   attacked  America  because      we    are   \n",
       "lemma      -PRON-  have     attack  America  because  -PRON-     be   \n",
       "ent_type                                GPE                           \n",
       "pos          PRON   AUX       VERB    PROPN    SCONJ    PRON    AUX   \n",
       "tag           PRP   VBP        VBN      NNP       IN     PRP    VBP   \n",
       "dep         nsubj   aux       ROOT     dobj     mark   nsubj  advcl   \n",
       "concept                  MILACTION                                    \n",
       "attribute                    trade                                    \n",
       "ideology                  military                                    \n",
       "\n",
       "                    7     8       9      10          11     12     13   14  \\\n",
       "text           freedom    's    home    and    defender      ,    and  the   \n",
       "lemma          freedom    's    home    and    defender      ,    and  the   \n",
       "ent_type                                                                     \n",
       "pos               NOUN  PART    NOUN  CCONJ        NOUN  PUNCT  CCONJ  DET   \n",
       "tag                 NN   POS      NN     CC          NN      ,     CC   DT   \n",
       "dep               poss  case    attr     cc        conj  punct     cc  det   \n",
       "concept    BENEVOLANCE         PLACE         ARMEDGROUP                      \n",
       "attribute        trade        entity           identity                      \n",
       "ideology        social        social           military                      \n",
       "\n",
       "                   15    16      17       18    19      20   21       22  \\\n",
       "text       commitment    of     our  Fathers    is     now  the  calling   \n",
       "lemma      commitment    of  -PRON-   father    be     now  the  calling   \n",
       "ent_type                                                                   \n",
       "pos              NOUN   ADP     DET     NOUN   AUX     ADV  DET     NOUN   \n",
       "tag                NN    IN    PRP$      NNS   VBZ      RB   DT       NN   \n",
       "dep             nsubj  prep    poss     pobj  conj  advmod  det     attr   \n",
       "concept                               FAMILY                               \n",
       "attribute                            ingroup                               \n",
       "ideology                              social                               \n",
       "\n",
       "             23      24    25     26  \n",
       "text         of     our  time      .  \n",
       "lemma        of  -PRON-  time      .  \n",
       "ent_type                              \n",
       "pos         ADP     DET  NOUN  PUNCT  \n",
       "tag          IN    PRP$    NN      .  \n",
       "dep        prep    poss  pobj  punct  \n",
       "concept                               \n",
       "attribute                             \n",
       "ideology                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They have attacked America because we are freedom's home and defender, and the commitment of our Fathers is now the calling of our time.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>they</td>\n",
       "      <td>america</td>\n",
       "      <td>we</td>\n",
       "      <td>freedom's home</td>\n",
       "      <td>defender</td>\n",
       "      <td>the commitment</td>\n",
       "      <td>our fathers</td>\n",
       "      <td>the calling</td>\n",
       "      <td>our time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>root</th>\n",
       "      <td>they</td>\n",
       "      <td>america</td>\n",
       "      <td>we</td>\n",
       "      <td>home</td>\n",
       "      <td>defender</td>\n",
       "      <td>commitment</td>\n",
       "      <td>father</td>\n",
       "      <td>calling</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ARMEDGROUP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attribute</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>identity</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ideology</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>military</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1   2               3           4               5  \\\n",
       "text       they  america  we  freedom's home    defender  the commitment   \n",
       "root       they  america  we            home    defender      commitment   \n",
       "i             0        3   5               9          11              15   \n",
       "concept                                       ARMEDGROUP                   \n",
       "attribute                                       identity                   \n",
       "ideology                                        military                   \n",
       "\n",
       "                     6            7         8  \n",
       "text       our fathers  the calling  our time  \n",
       "root            father      calling      time  \n",
       "i                   18           22        25  \n",
       "concept                                        \n",
       "attribute                                      \n",
       "ideology                                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "new chunk text [we] (q):  \n",
      "notes [] \n",
      "new root text [we]:  \n",
      "idx [5]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lefts:  []\n",
      "we\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "concept [SELF]: \n",
      "attribute [ingroup]:  \n",
      "ideology [social]:  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>root</th>\n",
       "      <th>idx</th>\n",
       "      <th>concept</th>\n",
       "      <th>attribute</th>\n",
       "      <th>ideology</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>5</td>\n",
       "      <td>SELF</td>\n",
       "      <td>ingroup</td>\n",
       "      <td>social</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text root  idx concept attribute ideology notes\n",
       "0   we   we    5    SELF   ingroup   social      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "satisfied q\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Stop right there!",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Stop right there!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from itertools import zip_longest\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "import pipeline\n",
    "import cndutils as ut\n",
    "import visuals as viz\n",
    "\n",
    "\n",
    "#################################\n",
    "# chunk attrs\n",
    "#################################\n",
    "\n",
    "def chunk_attrs(token):\n",
    "    \n",
    "    attrs = [\"text\", \"root\", \"i\", \"concept\", \"attribute\", \"ideology\"]\n",
    "    \n",
    "    root = \"\"\n",
    "    root = token.root.lemma_.lower() \n",
    "    if root == \"-pron-\":\n",
    "        root = token.text.lower()\n",
    "\n",
    "    return dict(zip(attrs, [str(token).lower(), root.lower(), str(token.root.i), \\\n",
    "                               str(token._.CONCEPT), str(token._.ATTRIBUTE), str(token._.IDEOLOGY)]))\n",
    "\n",
    "################################\n",
    "# get notes\n",
    "################################\n",
    "   \n",
    "def get_notes(index):\n",
    "   \n",
    "    notes = ''\n",
    "    include  = [\"compound\"]\n",
    "    exclude = [\"det\", \"poss\"]\n",
    "    left_edge_index = doc[index].left_edge.i\n",
    "\n",
    "    n = 0\n",
    "    while left_edge_index <= index + n: # and doc[word.i + n].dep_ in exclude:\n",
    "        if doc[index + n].dep_ in exclude:\n",
    "            notes = f\"exclude {doc[index + n].dep_}. \"\n",
    "        index -= 1\n",
    "\n",
    "    return notes\n",
    "\n",
    "path = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\"\n",
    "test_jsonl = \"test_chunks.jsonl\"\n",
    "cust_jsonl = \"cust_chunks.jsonl\"\n",
    "index_str = \"index.json\"\n",
    "test_filepath = os.path.join(path, test_jsonl)\n",
    "cust_filepath = os.path.join(path, cust_jsonl)\n",
    "index_filepath = os.path.join(path, index_str)\n",
    "\n",
    "with jsonlines.open(test_filepath) as f:\n",
    "    test_chunks = list(f.iter())\n",
    "    \n",
    "try:  \n",
    "    with jsonlines.open(cust_filepath) as f:\n",
    "        cust_chunk_list = list(f.iter())\n",
    "    if len(test_chunks) == 0:\n",
    "        cust_chunk_list = list()    \n",
    "\n",
    "except:\n",
    "    cust_chunk_list = list()\n",
    "\n",
    "try:\n",
    "    with open(index_filepath, \"r\") as index_json:\n",
    "        index = json.load(index_json)\n",
    "        \n",
    "except:\n",
    "    index = 0\n",
    "    \n",
    "lookup = pipeline.ConceptMatcher(cnd.nlp)\n",
    "    \n",
    "#################################\n",
    "# main body\n",
    "#################################\n",
    "    \n",
    "while index < len(test_chunks):\n",
    "    \n",
    "    quit = False\n",
    "    \n",
    "    line = test_chunks[index]\n",
    "            \n",
    "    with open(index_filepath, \"wb\") as f:\n",
    "        f.write(json.dumps(index).encode(\"utf-8\"))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "  \n",
    "    #parse document\n",
    "    doc = cnd(line[str(index)])\n",
    "    \n",
    "    # add the original and indexed noun_chunks to the line\n",
    "        \n",
    "    line[\"orig_chunks\"] = {k : chunk_attrs(v) for k, v in enumerate(doc.noun_chunks)}\n",
    "    line[\"new_chunks\"] = {}\n",
    "    \n",
    "    new_chunk_dict = dict()\n",
    "    stop = False\n",
    "\n",
    "    for key, orig, new in zip_longest(range(len(list(doc.noun_chunks))), doc.noun_chunks, doc._.custom_chunks):\n",
    "\n",
    "        satisfied = False\n",
    "        while satisfied == False:\n",
    "\n",
    "            clear_output(wait = True)\n",
    "            \n",
    "            displacy.render(doc, style = \"dep\")\n",
    "\n",
    "            display(viz.sent_frame(doc, compact = False))\n",
    "\n",
    "            attrs = [\"text\", \"root\", \"i\", \"concept\", \"attribute\", \"ideology\"]\n",
    "            \n",
    "            print(doc.text)\n",
    "\n",
    "            display(pd.DataFrame(line[\"orig_chunks\"]))\n",
    "\n",
    "            new_text = \"\"\n",
    "            new_notes = \"\"\n",
    "            new_root = \"\"\n",
    "            new_i = \"\"\n",
    "            new_concept = \"\"\n",
    "            new_attribute = \"\"\n",
    "            new_ideology = \"\"\n",
    "\n",
    "            #get new string\n",
    "            cust_text = new.text\n",
    "            new_text = input(f\"new chunk text [{cust_text}] (q): \").lower()\n",
    "            if new_text == \"q\":\n",
    "                raise SystemExit(\"Stop right there!\")\n",
    "            if len(new_text) == 0:\n",
    "                new_text = cust_text                \n",
    "\n",
    "            #get new root\n",
    "            cust_root = new.root.lemma_.lower()\n",
    "            if cust_root.lower() == \"-pron-\":\n",
    "                cust_root = new.text\n",
    "            new_root = input(f\"new root text [{cust_root}]: \").lower()\n",
    "            if len(new_root) == 0:\n",
    "                new_root = cust_root\n",
    "\n",
    "            #get new i\n",
    "            cust_i = new.root.i\n",
    "            new_i = input(f\"idx [{cust_i}]: \").lower()\n",
    "            if len(new_i) == 0:\n",
    "                new_i = cust_i\n",
    "\n",
    "           # get new concept\n",
    "            concept_lookup = lookup.get_concept(new_root)\n",
    "            \n",
    "###########################\n",
    "#do this first, get the attributes of an immediate modifier token\n",
    "#             ## get the custom attributes of any amod token\n",
    "#             print(\"lefts: \", list(doc[new_i].lefts))\n",
    "#             for t in doc[new_i].lefts:\n",
    "#                 if t.dep_ in [\"amod\"]:\n",
    "#                     concept_lookup = t._.CONCEPT\n",
    "#             print(new_root)\n",
    "            \n",
    "            new_concept = input(f\"concept [{concept_lookup}]:\").upper()\n",
    "            if len(new_concept) == 0:\n",
    "                new_concept = concept_lookup\n",
    "\n",
    "            # get new attribute\n",
    "            attribute_lookup = lookup.get_attribute(new_concept.lower())\n",
    "            new_attribute = input(f\"attribute [{attribute_lookup}]: \").lower()\n",
    "            if len(new_attribute) == 0:\n",
    "                new_attribute = attribute_lookup\n",
    "\n",
    "            # get new ideology\n",
    "            ideology_lookup = lookup.get_ideology(new_concept.lower())\n",
    "            new_ideology = input(f\"ideology [{ideology_lookup}]: \").lower()\n",
    "            if len(new_ideology) == 0:\n",
    "                new_ideology = ideology_lookup\n",
    "                \n",
    "            new_notes = get_notes(new.root.i)\n",
    "            note = input(f\"notes [{new_notes}]\")\n",
    "            new_notes += note\n",
    "            if len(note) > 0 and note[-1] != \".\":\n",
    "                new_notes += \".\"\n",
    "\n",
    "            new_chunk_dict[key] = {\"text\" : new_text, \"root\" : new_root, \"idx\" : new_i, \\\n",
    "                                   \"concept\" : new_concept, \"attribute\" : new_attribute, \"ideology\" : new_ideology, \"notes\" : new_notes}\n",
    "\n",
    "            display(pd.DataFrame([new_chunk_dict[key]]))\n",
    "\n",
    "            check = \"\"\n",
    "            while check not in [\"y\", \"n\", \"q\"]:\n",
    "                check = input(\"satisfied\").lower()\n",
    "                if check == \"y\":\n",
    "                    satisfied = True\n",
    "                if check == \"q\":\n",
    "                    raise SystemExit(\"Stop right there!\")\n",
    "        \n",
    "        # append the original and new chunk dicts to jsonl object in json readable format   \n",
    "\n",
    "    line.update({\"new_chunks\" : ut.doubleQuoteDict(new_chunk_dict)})\n",
    "    cust_chunk_list.append(line)\n",
    "    \n",
    "    #write jsonl object to disk\n",
    "    with jsonlines.open(os.path.join(path, cust_filepath), 'w') as writer:\n",
    "        writer.write_all(cust_chunk_list)\n",
    "        \n",
    "    index += 1\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
