{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Noun Chunking\n",
    "-----\n",
    "\n",
    "There is a problem whereby spaCy's inbuilt noun_chunks is too course grained for the chunking required for detecting the ingroups and outgroups.\n",
    "\n",
    "For the purposes of the methodology, a more fine grained noun chunking algorithm is required.\n",
    "\n",
    "There are several examples in the test ingroup and outgroup sentences named entities are chunked with other nouns when they would preferable be kept separate.\n",
    "\n",
    "There are also several examples where a noun chunk contains more than one noun of a custom attribute, therefore, the chunk needs to be resolved to a single instance\n",
    "\n",
    "This notebook adapt spaCy's noun_chunk source code and adapt for the specific purpose of this pipeline.\n",
    "\n",
    "Source code at these links:\n",
    "\n",
    "    Noun Chunker Code\n",
    "    \n",
    "    https://github.com/explosion/spaCy/blob/master/spacy/lang/en/syntax_iterators.py\n",
    "\n",
    "    Class extensions\n",
    "\n",
    "    https://github.com/explosion/spaCy/blob/9ce059dd067ecc3f097d04023e3cfa0d70d35bb8/spacy/tokens/doc.pyx\n",
    "\n",
    "    https://github.com/explosion/spaCy/blob/f49e2810e6ea5c8b848df5b0f393c27ee31bb7f4/spacy/tokens/span.pyx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data\n",
    "\n",
    "the following sentences will be used for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49 records from C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\\binladen_ingroup_sents.jsonl\n",
      "Loaded 101 records from C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\\binladen_outgroup_sents.jsonl\n",
      "Loaded 66 records from C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\\bush_ingroup_sents.jsonl\n",
      "Loaded 37 records from C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\\bush_outgroup_sents.jsonl\n"
     ]
    }
   ],
   "source": [
    "## create a dict object of all the ingroup/outgroup sentences\n",
    "import os\n",
    "import cndutils as ut\n",
    "path = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\"\n",
    "\n",
    "sent_dict = dict()\n",
    "jsonl_files = [f for f in os.listdir(path) if os.path.splitext(f)[1] == \".jsonl\" and \"group\" in os.path.splitext(f)[0]]\n",
    "for file in jsonl_files:\n",
    "    data_list = ut.load_jsonl(os.path.join(path, file))\n",
    "    for entry in data_list:\n",
    "        for value in entry.values():\n",
    "            sent_dict[len(sent_dict)] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "import pipeline\n",
    "cnd = pipeline.CND()\n",
    "\n",
    "merge_nps = cnd.nlp.create_pipe(\"merge_noun_chunks\")\n",
    "cnd.nlp.add_pipe(merge_nps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through Test Data for sentences of Interest\n",
    "\n",
    "These sentences will be used to tune the existing spaCy noun chunker for the purposes of this methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 / 253\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>When</td>\n",
       "      <td>those</td>\n",
       "      <td>have</td>\n",
       "      <td>stood</td>\n",
       "      <td>in</td>\n",
       "      <td>defense</td>\n",
       "      <td>of</td>\n",
       "      <td>their weak children</td>\n",
       "      <td>,</td>\n",
       "      <td>their</td>\n",
       "      <td>brothers</td>\n",
       "      <td>and</td>\n",
       "      <td>sisters</td>\n",
       "      <td>in</td>\n",
       "      <td>Palestine</td>\n",
       "      <td>and</td>\n",
       "      <td>other Muslim nations</td>\n",
       "      <td>,</td>\n",
       "      <td>the whole world</td>\n",
       "      <td>went</td>\n",
       "      <td>into</td>\n",
       "      <td>an uproar</td>\n",
       "      <td>,</td>\n",
       "      <td>the infidels</td>\n",
       "      <td>followed</td>\n",
       "      <td>by</td>\n",
       "      <td>the hypocrites</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ent_type</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>GPE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>LOC</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NORP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MILENTITY</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>FAMILY</td>\n",
       "      <td></td>\n",
       "      <td>FAMILY</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attribute</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>entity</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ingroup</td>\n",
       "      <td></td>\n",
       "      <td>ingroup</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ideology</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>military</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>social</td>\n",
       "      <td></td>\n",
       "      <td>social</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1     2      3   4          5   6                    7   \\\n",
       "text       When  those  have  stood  in    defense  of  their weak children   \n",
       "ent_type                                                                      \n",
       "concept                                  MILENTITY                            \n",
       "attribute                                   entity                            \n",
       "ideology                                  military                            \n",
       "\n",
       "          8      9         10   11       12  13         14   15  \\\n",
       "text       ,  their  brothers  and  sisters  in  Palestine  and   \n",
       "ent_type                                               GPE        \n",
       "concept                FAMILY        FAMILY                       \n",
       "attribute             ingroup       ingroup                       \n",
       "ideology               social        social                       \n",
       "\n",
       "                             16 17               18    19    20         21 22  \\\n",
       "text       other Muslim nations  ,  the whole world  went  into  an uproar  ,   \n",
       "ent_type                                        LOC                             \n",
       "concept                                                                         \n",
       "attribute                                                                       \n",
       "ideology                                                                        \n",
       "\n",
       "                     23        24  25              26 27  \n",
       "text       the infidels  followed  by  the hypocrites  .  \n",
       "ent_type           NORP                                   \n",
       "concept                                                   \n",
       "attribute                                                 \n",
       "ideology                                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import cndutils\n",
    "importlib.reload(cndutils)\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\"\n",
    "ss = cndutils.sent_select(path = path, file = \"test_sents\")\n",
    "output = ss(cnd.nlp, sent_dict)\n",
    "print(output)\n",
    "        \n",
    "    \n",
    "# remove dets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code is adapted from the exising noun chunker in spaCy.\n",
    "\n",
    "The crux of this code is in this section:\n",
    "\n",
    "`\n",
    "if word.pos not in (NOUN, PROPN, PRON):\n",
    "            continue\n",
    "        # Prevent nested chunks from being produced\n",
    "        if word.left_edge.i <= prev_end:\n",
    "            continue\n",
    "        if word.dep in np_deps:\n",
    "            prev_end = word.i\n",
    "            yield word.left_edge.i, word.i + 1, np_label\n",
    "        elif word.dep == conj:\n",
    "            head = word.head\n",
    "            while head.dep == conj and head.head.i < head.i:\n",
    "                head = head.head\n",
    "            # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "            if head.dep in np_deps:\n",
    "                prev_end = word.i\n",
    "                yield word.left_edge.i, word.i + 1, np_label\n",
    "`\n",
    "\n",
    "`np_deps` is a list of dependency labels denoting a noun token\n",
    "\n",
    "`prev_end` is a index to ensure subsequent chunks do not overlap with existing chunks\n",
    "\n",
    "`word.left_edge.i` creates a chunk from the root token and all other tokens in its leftwards facing dependency tree.\n",
    "\n",
    "Where `word.left_edge.i` is too course grained, the custom chunk will expand the number of tests to become a more fine grained chunker.\n",
    "\n",
    "Additionally, functionality for custom attributes will have to be added.\n",
    "\n",
    "For the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunks(doclike):\n",
    "    \"\"\"\n",
    "    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n",
    "    \n",
    "    source code: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/syntax_iterators.py\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        \"nsubj\",\n",
    "        \"dobj\",\n",
    "        \"nsubjpass\",\n",
    "        \"pcomp\",\n",
    "        \"pobj\",\n",
    "        \"dative\",\n",
    "        \"appos\",\n",
    "        \"attr\",\n",
    "        \"ROOT\",\n",
    "    ]\n",
    "    doc = doclike.doc  # Ensure works on both Doc and Span.\n",
    "\n",
    "    if not doc.is_parsed:\n",
    "        raise ValueError(Errors.E029)\n",
    "\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add(\"conj\")\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    \n",
    "    prev_end = -1\n",
    "    \n",
    "    for i, word in enumerate(doclike):\n",
    "        \n",
    "        if word.pos not in (NOUN, PROPN, PRON):\n",
    "            continue\n",
    "        # Prevent nested chunks from being produced\n",
    "        if word.left_edge.i <= prev_end:\n",
    "            continue\n",
    "        if word.dep in np_deps:\n",
    "            prev_end = word.i\n",
    "            yield word.left_edge.i, word.i + 1, np_label\n",
    "        elif word.dep == conj:\n",
    "            head = word.head\n",
    "            while head.dep == conj and head.head.i < head.i:\n",
    "                head = head.head\n",
    "            # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "            if head.dep in np_deps:\n",
    "                prev_end = word.i\n",
    "                yield word.left_edge.i, word.i + 1, np_label\n",
    "\n",
    "def merge_named_concepts(doc):\n",
    "\n",
    "    \"\"\"Merge named concepts into a single token.\n",
    "    doc (Doc): The Doc object.\n",
    "    RETURNS (Doc): The Doc object with merged named concepts.\n",
    "\n",
    "    Adapts the spacy merge noun chunks function\n",
    "    code: https://github.com/explosion/spaCy/blob/master/spacy/pipeline/functions.py\n",
    "    \"\"\"\n",
    "    if not doc.is_parsed:\n",
    "        return doc\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in doc._.named_concepts:\n",
    "            attrs = {\n",
    "                    \"tag\": span.root.tag, \n",
    "                    \"dep\": span.root.dep\n",
    "                    }\n",
    "\n",
    "            retokenizer.merge(span, attrs=attrs)\n",
    "    \n",
    "    return doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
