{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Hearst Patterns\n",
    "---\n",
    "\n",
    "In this experiment we test the utility of Hearst Patterns for detecting the ingroup and outgroup of a text.\n",
    "\n",
    "For this experiment spaCy matcher is used with code adapted from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/hearstPatterns.py\n",
    "\n",
    "Hypernym relations are semantic relationships between two concepts: C1 is a hypernym of C2 means that C1 categorizes C2 (e.g. “instrument” is a hypernym of “Piano”). For this research, the phrase, \"America has enemies, such as Al Qaeda and the Taliban\" would return the following '[('Al Qaeda', 'enemy'), ('the Taliban', 'enemy')]'. In this example, the categorising term 'enemy' is a hypernym of both 'Al Qaeda' and the 'Taliban'; conversely 'al Qaeda' and 'the Tabliban' are hyponyms of 'enemy'. Using this technique, hypernym terms could be classified as ingroup or outgroup and named entities identified as hyponym terms could be identified as either group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the spaCy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# class cna_pipe(object):\n",
    "\n",
    "#     import spacy\n",
    "\n",
    "#     def __init__(self):\n",
    "        \n",
    "#         self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        \n",
    "#         for component in self.nlp.pipe_names:\n",
    "#             if component not in ['tagger', \"parser\", \"ner\"]:\n",
    "#                 self.nlp.remove_pipe(component)\n",
    "        \n",
    "#         merge_ents = self.nlp.create_pipe(\"merge_entities\")\n",
    "#         self.nlp.add_pipe(merge_ents)\n",
    "        \n",
    "#     def __call__(self, text):\n",
    "        \n",
    "#         doc = self.nlp(text)\n",
    "        \n",
    "#         return doc\n",
    "    \n",
    "# nlp = cna_pipe()\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "for component in nlp.pipe_names:\n",
    "    if component not in ['tagger', \"parser\", \"ner\"]:\n",
    "        self.nlp.remove_pipe(component)\n",
    "\n",
    "merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(merge_ents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Bush contains 15 speeches\n",
      "Martin Luther King contains 5 speeches\n",
      "Osama bin Laden contains 7 speeches\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "class Orator:\n",
    "    \n",
    "    \"\"\" \n",
    "    This is the Orator object which refers to the person giving the speech\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ref = '', name = ''):\n",
    "        \n",
    "        self.ref = ref\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.filenames = []\n",
    "        \n",
    "        self.texts = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "path  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CulturalViolence\\KnowledgeBases\\Speeches\"\n",
    "\n",
    "# access the speeches directory\n",
    "def initiate_dataset(filepath):\n",
    "    \n",
    "    \"\"\" \n",
    "    this function initiates the dataset by collating speech texts from the directory associated\n",
    "    with each orator\n",
    "    \n",
    "    function returns a dict object with each entry refering to an orator\n",
    "    \n",
    "    format:\n",
    "    \n",
    "    {\"surname\" : Orator Object}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    orators_dict = dict()\n",
    "    \n",
    "    for dirpath , dirnames, _ in os.walk(filepath): \n",
    "    \n",
    "        # iterate through the folders in the speeches directory, which relates to each orator\n",
    "        for orator_dir in dirnames: \n",
    "            # iniate orator object and add to orators dict()\n",
    "            surname = orator_dir.split()[-1].lower()\n",
    "            orators_dict[surname] = Orator(name  = orator_dir, ref = surname)\n",
    "\n",
    "\n",
    "            # get the filenames in each orators folder\n",
    "            for _, _, filenames in os.walk(os.path.join(dirpath, orator_dir)): \n",
    "\n",
    "                # iterate through the files\n",
    "                for file in filenames: \n",
    "\n",
    "                    with open(os.path.join(dirpath, orator_dir, file), 'r') as text:\n",
    "\n",
    "                        if os.path.splitext(file)[1] == \".txt\" and (file[:8]).isnumeric(): #check whether file meets speech filename format requirement\n",
    "                            orators_dict[surname].filenames.append(file)\n",
    "                            orators_dict[surname].texts.append(text.read())\n",
    "                            \n",
    "    return orators_dict\n",
    "\n",
    "\n",
    "\n",
    "for obj in initiate_dataset(path).values():\n",
    "    print(obj.name, 'contains', len(obj), \"speeches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########  0\n",
      "We are hunting for terrorist groups, particularly the Taliban and al Qaeda\n",
      "particularly => [(terrorist groups, the Taliban), (terrorist groups, al Qaeda)]\n",
      "\n",
      "-----\n",
      "###########  1\n",
      "We are hunting for the IRA, ISIS, al Qaeda and some other terrorist groups, especially the Taliban, Web Scientists and particularly Southampton University\n",
      "some_other => [(terrorist groups, al Qaeda), (terrorist groups, the IRA), (terrorist groups, ISIS)]\n",
      "\n",
      "We are hunting for the IRA, ISIS, al Qaeda and some other terrorist groups, especially the Taliban, Web Scientists and particularly Southampton University\n",
      "especially => [(terrorist groups, the Taliban), (terrorist groups, Web Scientists), (terrorist groups, Southampton University)]\n",
      "\n",
      "-----\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Hearst patterns take the form of (NP <predicate> (NP (and | or)?)+)\n",
    "\n",
    "class hearst_patterns(object):\n",
    "    \n",
    "    \"\"\" Hearst Patterns is a class object used to detects hypernym relations to hyponyms in a text\n",
    "    \n",
    "    input: raw text\n",
    "    returns: list of dict object with each entry all the hypernym-hyponym pairs of a text\n",
    "    entry format: [\"predicate\" : [(hyponym, hypernym), (hyponym, hypernym), ..]]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import spacy    \n",
    "    \n",
    "    def __init__(self, extended=False):\n",
    "        \n",
    "        from spacy.matcher import Matcher\n",
    "        \n",
    "        # make the patterns easier to read\n",
    "        hypernym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}} \n",
    "        hyponym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
    "        punct = {\"IS_PUNCT\": True, \"OP\": \"?\"}\n",
    "        \n",
    "        self.patterns = [\n",
    "            \n",
    "            # Included in each entry is the original regex pattern now adapted as spaCy patterns\n",
    "            # Many of these patterns are in the same format. Nnext iteration of code will include an\n",
    "            # automatic pattern generator for patterns of the same format.\n",
    "            # these patterns need cleaning up and testing.\n",
    "            \n",
    "            # format for the dict entry of each pattern\n",
    "            # {\n",
    "            #  \"label\" : predicate, \n",
    "            #  \"pattern\" : spaCy pattern, \n",
    "            #  \"posn\" : first/last depending on whether the hypernym appears before its hyponym\n",
    "            #  }\n",
    "            \n",
    "            {\"label\" : \"such_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "                 hypernym, punct, {\"LEMMA\": \"such\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"know_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?know as (NP_\\\\w+ ?(, )?(and |or )?)+)', # added for this experiment\n",
    "#                 'first'\n",
    "                 hypernym, punct, {\"LEMMA\": \"know\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"such_NOUN_as\", \"pattern\" : [\n",
    "#                 '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "                 {\"LEMMA\": \"such\"}, hypernym, punct, {\"LEMMA\": \"as\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"include\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "                 hypernym, punct, {\"LEMMA\" : \"include\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"especially\", \"pattern\" : [ ## problem - especially is merged as a modifier in to a noun phrase\n",
    "#                 '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "                 hypernym, punct, {\"LEMMA\" : \"especially\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "            \n",
    "            {\"label\" : \"and-or_other\", \"pattern\" : [ ## problem - other is merged as a modifier in to a noun phrase\n",
    "#                 '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "#                 'last'\n",
    "                 hyponym, punct, {\"DEP\": \"cc\"}, {\"LEMMA\" : \"other\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            ]\n",
    "    \n",
    "        if extended:\n",
    "            self.patterns.extend([\n",
    "                \n",
    "                {\"label\" : \"which_may_include\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "#                     '?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"may\"}, {\"LEMMA\" : \"include\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"which_be_similar_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"similar\"}, {\"LEMMA\" : \"to\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"example_of_this_be\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"this\"}, {\"LEMMA\" : \"be\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \",type\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"type\"}, punct, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"mainly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"mainly\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"mostly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"mostly\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"notably\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"notably\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"particularly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"particularly\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"principally\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)', - fuses in a noun phrase\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"principally\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"in_particular\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"in\"}, {\"LEMMA\" : \"particular\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"except\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"except\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"other_than\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"other\"}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"eg\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"e.g.\", \"eg\"]}}, hyponym \n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "#                 {\"label\" : \"eg-ie\", \"pattern\" : [ \n",
    "# #                     '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+' - need to understand this pattern better\n",
    "# #                     '(\\\\. )?\\\\))',\n",
    "# #                     'first'\n",
    "#                     hypernym, punct, {\"LEMMA\" : {IN : [\"e.g.\", \"i.e.\", \"eg\", \"ie\"]}}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "#                 ]},\n",
    "\n",
    "                {\"label\" : \"ie\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"i.e.\", \"ie\"]}}, hyponym \n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"for_example\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?for example (, )?'\n",
    "#                     '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"example\"}, punct, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"example_of_be\", \"pattern\" : [\n",
    "#                     'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym, punct, {\"LEMMA\" : \"be\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"like\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"like\"}, hyponym,\n",
    "                ], \"posn\" : \"first\"},\n",
    "\n",
    "                # repeat of such_as pattern in primary patterns???\n",
    "#                     'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                \n",
    "                    {\"label\" : \"whether\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"whether\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"compare_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"compare\"}, {\"LEMMA\" : \"to\"}, hyponym \n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"among_-PRON-\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"among\"}, {\"LEMMA\" : \"-PRON-\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"for_instance\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "#                     'for instance)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, hyponym, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"instance\"}\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"and-or_any_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"DEP\": \"cc\"}, {\"LEMMA\" : \"any\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"some_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"some\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"be_a\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"a\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "\n",
    "                {\"label\" : \"like_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"like\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "\n",
    "                 {\"label\" : \"one_of_the\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"the\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"one_of_these\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"these\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"one_of_those\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"those\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"be_example_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)', added optional \"an\" to spaCy pattern for singular vs. plural\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"an\", \"OP\" : \"?\"}, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "               \n",
    "                {\"label\" : \"which_be_call\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"call\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "#               \n",
    "                {\"label\" : \"which_be_name\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"name\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                    \n",
    "                {\"label\" : \"a_kind_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"a\", \"OP\" : \"?\"}, {\"LEMMA\" : \"kind\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)', - combined with above\n",
    "#                     'last'\n",
    "               \n",
    "                {\"label\" : \"form_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"a\", \"OP\" : \"?\"}, {\"LEMMA\" : \"form\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"which_look_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"look\"}, {\"LEMMA\" : \"like\"}, hyponym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"which_sound_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"sound\"}, {\"LEMMA\" : \"like\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"type\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"type\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                                \n",
    "                {\"label\" : \"compare_with\", \"pattern\" : [\n",
    "#                     '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    {\"LEMMA\" : \"compare\"}, hyponym, punct, {\"LEMMA\" : \"with\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"as\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"as\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"sort_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"sort\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "            ]),        \n",
    "        \n",
    "        ## initiate matcher\n",
    "        self.matcher = Matcher(nlp.vocab, validate = True)\n",
    "        \n",
    "        self.predicate_set = set()\n",
    "        self.predicates = []\n",
    "        self.first = []\n",
    "        self.last = []\n",
    "\n",
    "        for pattern in self.patterns:\n",
    "            self.matcher.add(pattern[\"label\"], None, pattern[\"pattern\"])\n",
    "            \n",
    "            # gather list of predicate terms for the noun_chunk deconfliction\n",
    "            self.predicate_set.update(pattern[\"label\"].split('_'))\n",
    "            self.predicates.append(pattern[\"label\"].split('_'))\n",
    "            \n",
    "            # gather list of predicates where the hypernym appears first\n",
    "            if pattern[\"posn\"] == \"first\":\n",
    "                self.first.append(pattern[\"label\"])\n",
    "                \n",
    "            # gather list of predicates where the hypernym appears last\n",
    "            if pattern[\"posn\"] == \"last\":\n",
    "                self.last.append(pattern[\"label\"])\n",
    "   \n",
    "    def find_hyponyms(self, text):\n",
    "        \n",
    "        \"\"\"\n",
    "        this is the main function of the object\n",
    "        \n",
    "        follows logic of:\n",
    "        1. checks whether text has been parsed\n",
    "        2. pre-processing for noun_chunks\n",
    "        3. generate matches\n",
    "        4. create list of dict obje\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(self.predicates)\n",
    "        \n",
    "        from spacy.tokens import Span\n",
    "            \n",
    "        pairs = [] # set up dictionary containing pairs\n",
    "        \n",
    "        if type(text) is spacy.tokens.doc.Doc:\n",
    "            doc = text\n",
    "        else:\n",
    "            doc = nlp(text) # initiate doc \n",
    "        \n",
    "        ## Pre-processing\n",
    "        # there are some predicate terms, such as \"particularly\", \"especially\" and \"some other\" which are\n",
    "        # merged with the noun phrase. Such terms are part of the pattern and become part of the\n",
    "        # merged noun-chunk, consequently, they are not detected in by the matcher.\n",
    "        # This pre-processing, therefore, walks through the noun_chunks of a doc object to remove those\n",
    "        # predicate terms from eah noun_chunk and merges the result.\n",
    "                \n",
    "        #try:\n",
    "        with doc.retokenize() as retokenizer:\n",
    "\n",
    "        #iterate through the noun_chunks\n",
    "            for chunk in doc.noun_chunks:\n",
    "\n",
    "                attrs = {\"tag\": chunk.root.tag, \"dep\": chunk.root.dep}\n",
    "                count = 0\n",
    "\n",
    "                #iterate through all predicate terms.\n",
    "                for predicate in self.predicates:\n",
    "\n",
    "                    # iterate through the noun_chunk. If its first, second etc token match those of a\n",
    "                    #predicate word or phrase, then add to count.\n",
    "                    while count < len(predicate) and doc[chunk.start + count].lower_ == predicate[count]:\n",
    "                        count += 1\n",
    "\n",
    "                # Create a new noun_chunk based excluding the number of tokens detected as part of\n",
    "                # a predicate phrase.\n",
    "                #print(\"result: \", chunk, \" becomes \", doc[chunk.start + count : chunk.end])\n",
    "                retokenizer.merge(doc[chunk.start + count : chunk.end], attrs = attrs)\n",
    "                #print(chunk.sent)\n",
    "#         except:\n",
    "#             print(\"failed at with\")\n",
    "                \n",
    "        # Find matches in doc\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        # If none are found then return None\n",
    "        if not matches:\n",
    "            return pairs\n",
    "\n",
    "        for match_id, start, end in matches:\n",
    "            predicate = nlp.vocab.strings[match_id]\n",
    "            \n",
    "            if predicate in self.last: # if the predicate is in the list where the hypernym is last\n",
    "                hypernym = doc[end - 1]\n",
    "                hyponym = doc[start]\n",
    "            else:\n",
    "                hypernym = doc[start] # if the predicate is in the list where the hypernym is first\n",
    "                hyponym = doc[end - 1]\n",
    "\n",
    "#             if predicate in list(pairs.keys()): #check for double entries\n",
    "#                 continue\n",
    "#             else:\n",
    "                # crate dictionary object with the format:\n",
    "                # pairs[predicate term based on pattern name] + [(hypernym, hyponym)] + [hyponym conjuncts (tokens linked by and | or)]\n",
    "            \n",
    "            pairs.append(dict({\"predicate\" : predicate, \n",
    "                               \"pairs\" : [(hypernym, hyponym)] + [(hypernym, token) for token in hyponym.conjuncts if token != hypernym],\n",
    "                               \"sent\" : (hyponym.sent.text).strip()}))\n",
    "\n",
    "        return pairs\n",
    "    \n",
    "h = hearst_patterns(extended = True)\n",
    "\n",
    "docs = [\n",
    "    \"We are hunting for terrorist groups, particularly the Taliban and al Qaeda\",\n",
    "    \"We are hunting for the IRA, ISIS, al Qaeda and some other terrorist groups, especially the Taliban, Web Scientists and particularly Southampton University\"\n",
    "]\n",
    "\n",
    "def show_hyps(lst):\n",
    "    \n",
    "   \n",
    "    for i, text in enumerate(lst):\n",
    "        print('########### ', i)\n",
    "        hypernyms = h.find_hyponyms(text)\n",
    "\n",
    "        if hypernyms:\n",
    "            for hypernym in hypernyms:\n",
    "                print(hypernym[\"sent\"])\n",
    "                print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "                print()\n",
    "                \n",
    "        \n",
    "    \n",
    "        print('-----')\n",
    "\n",
    "show_hyps(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########  0\n",
      "Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\n",
      "such_as => [(symptoms, red eye), (symptoms, ocular pain), (symptoms, visual acuity), (symptoms, photophobia)]\n",
      "\n",
      "-----\n",
      "###########  1\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "include => [(close friends, Canada), (close friends, Australia), (close friends, Germany), (close friends, France)]\n",
      "\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "as => [(the operation, forces)]\n",
      "\n",
      "-----\n",
      "###########  2\n",
      "The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\n",
      "know_as => [(loosely affiliated terrorist organizations, al Qaeda)]\n",
      "\n",
      "-----\n",
      "###########  3\n",
      "Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\n",
      "like => [(Terrorist groups, al Qaeda)]\n",
      "\n",
      "-----\n",
      "###########  4\n",
      "This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\n",
      "include => [(terrorists, e-mails), (terrorists, the Internet), (terrorists, cell phones)]\n",
      "\n",
      "-----\n",
      "###########  5\n",
      "-----\n",
      "###########  6\n",
      "We are looking out for the Taliban, al Qaeda and other terrorist groups\n",
      "and-or_other => [(terrorist groups, al Qaeda), (terrorist groups, the Taliban)]\n",
      "\n",
      "-----\n",
      "###########  7\n",
      "We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\n",
      "and-or_other => [(terrorist groups, al Qaeda)]\n",
      "\n",
      "We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\n",
      "especially => [(terrorist groups, the Taliban), (terrorist groups, the muppets)]\n",
      "\n",
      "-----\n",
      "Wall time: 283 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a list of docs\n",
    "docs = [\n",
    "    \"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\",\n",
    "    \"Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\",\n",
    "    \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\",\n",
    "    \"Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\",\n",
    "    \"This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\",\n",
    "    \"From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\",\n",
    "    \"We are looking out for the Taliban, al Qaeda and other terrorist groups\",\n",
    "    \"We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\"\n",
    "]\n",
    "show_hyps(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########  0\n",
      "Our unity is a kinship of grief and a steadfast resolve to prevail against our enemies.\n",
      "be_a => [(kinship, Our unity)]\n",
      "\n",
      "America is a nation full of good fortune, with so much to be grateful for, but we are not spared from suffering.\n",
      "be_a => [(nation, America)]\n",
      "\n",
      "-----\n",
      "###########  1\n",
      "-----\n",
      "###########  2\n",
      "Like the good folks standing with me, the American people were appalled and outraged at last Tuesday's attacks.\n",
      "like => [(hand, the good folks)]\n",
      "\n",
      "-----\n",
      "###########  3\n",
      "We have seen it in the courage of passengers, who rushed terrorists to save others on the ground -- passengers like an exceptional man named Todd Beamer.\n",
      "like => [(passengers, an exceptional man)]\n",
      "\n",
      "The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\n",
      "know_as => [(loosely affiliated terrorist organizations, al Qaeda)]\n",
      "\n",
      "The terrorists' directive commands them to kill Christians and Jews, to kill all Americans, and make no distinctions among military and civilians, including women and children.\n",
      "include => [(civilians, women), (civilians, children)]\n",
      "\n",
      "This group and its leader -- a person named Usama bin Laden -- are linked to many other organizations in different countries, including the Egyptian Islamic Jihad and the Islamic Movement of Uzbekistan.\n",
      "include => [(different countries, the Egyptian Islamic Jihad), (different countries, the Islamic Movement of Uzbekistan)]\n",
      "\n",
      "They are recruited from their own nations and neighborhoods and brought to camps in places like Afghanistan, where they are trained in the tactics of terror.\n",
      "like => [(places, Afghanistan)]\n",
      "\n",
      "Release all foreign nationals, including American citizens, you have unjustly imprisoned.\n",
      "include => [(all foreign nationals, American citizens)]\n",
      "\n",
      "Our enemy is a radical network of terrorists, and every government that supports them.\n",
      "be_a => [(radical network, Our enemy)]\n",
      "\n",
      "They want to overthrow existing governments in many Muslim countries, such as Egypt, Saudi Arabia, and Jordan.\n",
      "such_as => [(many Muslim countries, Egypt), (many Muslim countries, Saudi Arabia), (many Muslim countries, Jordan)]\n",
      "\n",
      "By sacrificing human life to serve their radical visions -- by abandoning every value except the will to power -- they follow in the path of fascism, Nazism, and totalitarianism.\n",
      "except => [(every value, the will)]\n",
      "\n",
      "-----\n",
      "###########  4\n",
      "-----\n",
      "###########  5\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "include => [(close friends, Canada), (close friends, Australia), (close friends, Germany), (close friends, France)]\n",
      "\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "as => [(the operation, forces)]\n",
      "\n",
      "More than two weeks ago, I gave Taliban leaders a series of clear and specific demands: Close terrorist training camps; hand over leaders of the Al Qaeda network; and return all foreign nationals, including American citizens, unjustly detained in your country.\n",
      "include => [(all foreign nationals, American citizens)]\n",
      "\n",
      "Initially the terrorists may burrow deeper into caves and other entrenched hiding places.\n",
      "and-or_other => [(entrenched hiding places, caves)]\n",
      "\n",
      "The United States of America is a friend to the Afghan people, and we are the friends of almost a billion worldwide who practice the Islamic faith.\n",
      "be_a => [(friend, The United States of America)]\n",
      "\n",
      "This military action is a part of our campaign against terrorism, another front in a war that has already been joined through diplomacy, intelligence, the freezing of financial assets, and the arrests of known terrorists by law-enforcement agents in 38 countries.\n",
      "be_a => [(part, This military action)]\n",
      "\n",
      "-----\n",
      "###########  6\n",
      "It is enough to know that evil, like goodness, exists.\n",
      "like => [(evil, goodness)]\n",
      "\n",
      "I saw it at Fort Stewart, Georgia, when I first reviewed our troops as Commander-in-Chief, and looked into the faces of proud and determined soldiers.\n",
      "as => [(Commander, our troops)]\n",
      "\n",
      "-----\n",
      "###########  7\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-fc1878896aa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow_hyps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bush\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mshow_hyps\u001b[1;34m(lst)\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mfind_hyponyms\u001b[1;34m(self, text)\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize.Retokenizer.__exit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize._merge\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.vector.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "show_hyps(orators[\"bush\"].texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
