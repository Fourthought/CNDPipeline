{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Hearst Patterns\n",
    "---\n",
    "\n",
    "In this experiment we test the utility of Hearst Patterns for detecting the ingroup and outgroup of a text.\n",
    "\n",
    "For this experiment spaCy matcher is used with code adapted from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/hearstPatterns.py\n",
    "\n",
    "Hypernym relations are semantic relationships between two concepts: C1 is a hypernym of C2 means that C1 categorizes C2 (e.g. “instrument” is a hypernym of “Piano”). For this research, the phrase, \"America has enemies, such as Al Qaeda and the Taliban\" would return the following '[('Al Qaeda', 'enemy'), ('the Taliban', 'enemy')]'. In this example, the categorising term 'enemy' is a hypernym of both 'Al Qaeda' and the 'Taliban'; conversely 'al Qaeda' and 'the Tabliban' are hyponyms of 'enemy'. Using this technique, hypernym terms could be classified as ingroup or outgroup and named entities identified as hyponym terms could be identified as either group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the spaCy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "for component in nlp.pipe_names:\n",
    "    if component not in ['tagger', \"parser\", \"ner\"]:\n",
    "        self.nlp.remove_pipe(component)\n",
    "\n",
    "merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(merge_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\n",
      "C:\\Users\\Steve\\anaconda3\\python37.zip\n",
      "C:\\Users\\Steve\\anaconda3\\DLLs\n",
      "C:\\Users\\Steve\\anaconda3\\lib\n",
      "C:\\Users\\Steve\\anaconda3\n",
      "\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\win32\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\win32\\lib\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\Pythonwin\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\Steve\\.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "#sys.path.insert(0, r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\")\n",
    "\n",
    "for p in sys.path:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset of Political Speeches from George Bush, Osama bin Laden and Martin Luther King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object bush called George Bush has 15 speeches\n",
      "object king called Martin Luther King has 5 speeches\n",
      "object laden called Osama bin Laden has 7 speeches\n"
     ]
    }
   ],
   "source": [
    "from cndlib import entities\n",
    "\n",
    "dirpath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CulturalViolence\\KnowledgeBases\\Speeches\"\n",
    "\n",
    "orators = entities.Dataset(dirpath)\n",
    "\n",
    "for orator in orators:\n",
    "    print(f'object {orator.ref} called {orator.name} has {len(orator)} speeches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hearst Pattern Detection Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Hearst patterns take the form of (NP <predicate> (NP (and | or)?)+)\n",
    "\n",
    "class hearst_patterns(object):\n",
    "    \n",
    "    \"\"\" Hearst Patterns is a class object used to detects hypernym relations to hyponyms in a text\n",
    "    \n",
    "    input: raw text\n",
    "    returns: list of dict object with each entry all the hypernym-hyponym pairs of a text\n",
    "    entry format: [\"predicate\" : [(hyponym, hypernym), (hyponym, hypernym), ..]]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import spacy    \n",
    "    \n",
    "    def __init__(self, extended=False):\n",
    "        \n",
    "        from spacy.matcher import Matcher\n",
    "        \n",
    "        # make the patterns easier to read\n",
    "        hypernym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}} \n",
    "        hyponym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
    "        punct = {\"IS_PUNCT\": True, \"OP\": \"?\"}\n",
    "        \n",
    "        self.patterns = [\n",
    "            \n",
    "            # Included in each entry is the original regex pattern now adapted as spaCy patterns\n",
    "            # Many of these patterns follow the same structure. Next iteration of code will include an\n",
    "            # automatic pattern generator for patterns of the same structure.\n",
    "            \n",
    "            # These patterns need cleaning up and testing.\n",
    "            \n",
    "            # Format for the dict entry of each patternx\n",
    "            # {\n",
    "            #  \"label\" : predicate, \n",
    "            #  \"pattern\" : spaCy pattern, \n",
    "            #  \"posn\" : first/last depending on whether the hypernym appears before its hyponym\n",
    "            #  }\n",
    "            \n",
    "            {\"label\" : \"such_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "                 hypernym, punct, {\"LEMMA\": \"such\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"know_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?know as (NP_\\\\w+ ?(, )?(and |or )?)+)', # added for this experiment\n",
    "#                 'first'\n",
    "                 hypernym, punct, {\"LEMMA\": \"know\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"such_NOUN_as\", \"pattern\" : [\n",
    "#                 '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "                 {\"LEMMA\": \"such\"}, hypernym, punct, {\"LEMMA\": \"as\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"include\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "                 hypernym, punct, {\"LEMMA\" : \"include\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"especially\", \"pattern\" : [ ## problem - especially is merged as a modifier in to a noun phrase\n",
    "#                 '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "                 hypernym, punct, {\"LEMMA\" : \"especially\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "            \n",
    "            {\"label\" : \"and-or_other\", \"pattern\" : [ ## problem - other is merged as a modifier in to a noun phrase\n",
    "#                 '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "#                 'last'\n",
    "                 hyponym, punct, {\"DEP\": \"cc\"}, {\"LEMMA\" : \"other\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            ]\n",
    "    \n",
    "        if extended:\n",
    "            self.patterns.extend([\n",
    "                \n",
    "                {\"label\" : \"which_may_include\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "#                     '?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"may\"}, {\"LEMMA\" : \"include\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"which_be_similar_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"similar\"}, {\"LEMMA\" : \"to\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"example_of_this_be\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"this\"}, {\"LEMMA\" : \"be\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \",type\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"type\"}, punct, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"mainly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"mainly\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"mostly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"mostly\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"notably\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"notably\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"particularly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"particularly\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"principally\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)', - fuses in a noun phrase\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"principally\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"in_particular\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"in\"}, {\"LEMMA\" : \"particular\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"except\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"except\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"other_than\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"other\"}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"eg\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"e.g.\", \"eg\"]}}, hyponym \n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "#                 {\"label\" : \"eg-ie\", \"pattern\" : [ \n",
    "# #                     '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+' - need to understand this pattern better\n",
    "# #                     '(\\\\. )?\\\\))',\n",
    "# #                     'first'\n",
    "#                     hypernym, punct, {\"LEMMA\" : {IN : [\"e.g.\", \"i.e.\", \"eg\", \"ie\"]}}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "#                 ]},\n",
    "\n",
    "                {\"label\" : \"ie\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"i.e.\", \"ie\"]}}, hyponym \n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"for_example\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?for example (, )?'\n",
    "#                     '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"example\"}, punct, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"example_of_be\", \"pattern\" : [\n",
    "#                     'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym, punct, {\"LEMMA\" : \"be\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"like\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"like\"}, hyponym,\n",
    "                ], \"posn\" : \"first\"},\n",
    "\n",
    "                # repeat of such_as pattern in primary patterns???\n",
    "#                     'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                \n",
    "                    {\"label\" : \"whether\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"whether\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"compare_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"compare\"}, {\"LEMMA\" : \"to\"}, hyponym \n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"among_-PRON-\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, {\"LEMMA\" : \"among\"}, {\"LEMMA\" : \"-PRON-\"}, hyponym\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"for_instance\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "#                     'for instance)',\n",
    "#                     'first'\n",
    "                    hypernym, punct, hyponym, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"instance\"}\n",
    "                ], \"posn\" : \"first\"},\n",
    "                \n",
    "                {\"label\" : \"and-or_any_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"DEP\": \"cc\"}, {\"LEMMA\" : \"any\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"some_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"some\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"be_a\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"a\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "\n",
    "                {\"label\" : \"like_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"like\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "\n",
    "                 {\"label\" : \"one_of_the\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"the\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"one_of_these\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"these\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"one_of_those\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"those\"}, hypernym,\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"be_example_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)', added optional \"an\" to spaCy pattern for singular vs. plural\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"an\", \"OP\" : \"?\"}, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "               \n",
    "                {\"label\" : \"which_be_call\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"call\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "#               \n",
    "                {\"label\" : \"which_be_name\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"name\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                    \n",
    "                {\"label\" : \"a_kind_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"a\", \"OP\" : \"?\"}, {\"LEMMA\" : \"kind\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)', - combined with above\n",
    "#                     'last'\n",
    "               \n",
    "                {\"label\" : \"form_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"a\", \"OP\" : \"?\"}, {\"LEMMA\" : \"form\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"which_look_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"look\"}, {\"LEMMA\" : \"like\"}, hyponym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"which_sound_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"sound\"}, {\"LEMMA\" : \"like\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"type\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"type\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                                \n",
    "                {\"label\" : \"compare_with\", \"pattern\" : [\n",
    "#                     '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    {\"LEMMA\" : \"compare\"}, hyponym, punct, {\"LEMMA\" : \"with\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"as\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"as\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "                {\"label\" : \"sort_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                    hyponym, punct, {\"LEMMA\" : \"sort\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "                ], \"posn\" : \"last\"},\n",
    "                \n",
    "            ]),        \n",
    "        \n",
    "        ## initiate matcher\n",
    "        self.matcher = Matcher(nlp.vocab, validate = True)\n",
    "        \n",
    "        self.predicate_set = set()\n",
    "        self.predicates = []\n",
    "        self.first = []\n",
    "        self.last = []\n",
    "\n",
    "        for pattern in self.patterns:\n",
    "            self.matcher.add(pattern[\"label\"], None, pattern[\"pattern\"])\n",
    "            \n",
    "            # gather list of predicate terms for the noun_chunk deconfliction\n",
    "            self.predicate_set.update(pattern[\"label\"].split('_'))\n",
    "            self.predicates.append(pattern[\"label\"].split('_'))\n",
    "            \n",
    "            # gather list of predicates where the hypernym appears first\n",
    "            if pattern[\"posn\"] == \"first\":\n",
    "                self.first.append(pattern[\"label\"])\n",
    "                \n",
    "            # gather list of predicates where the hypernym appears last\n",
    "            if pattern[\"posn\"] == \"last\":\n",
    "                self.last.append(pattern[\"label\"])\n",
    "   \n",
    "    def find_hyponyms(self, text):\n",
    "        \n",
    "        \"\"\"\n",
    "        this is the main function of the object\n",
    "        \n",
    "        follows logic of:\n",
    "        1. checks whether text has been parsed\n",
    "        2. pre-processing for noun_chunks\n",
    "        3. generate matches\n",
    "        4. create list of dict object containing match results\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(self.predicates)\n",
    "        \n",
    "        from spacy.tokens import Span\n",
    "        from spacy.util import filter_spans\n",
    "            \n",
    "        pairs = [] # set up dictionary containing pairs\n",
    "        \n",
    "        if not doc.is_parsed:\n",
    "            doc = nlp(text) # initiate doc\n",
    "        else:\n",
    "            doc = text\n",
    " \n",
    "        \n",
    "        ## Pre-processing\n",
    "        # there are some predicate terms, such as \"particularly\", \"especially\" and \"some other\" which are\n",
    "        # merged with the noun phrase. Such terms are part of the pattern and become part of the\n",
    "        # merged noun-chunk, consequently, they are not detected in by the matcher.\n",
    "        # This pre-processing, therefore, walks through the noun_chunks of a doc object to remove those\n",
    "        # predicate terms from eah noun_chunk and merges the result.\n",
    "\n",
    "        #iterate through the noun_chunks\n",
    "        chunks = []\n",
    "        \n",
    "        with doc.retokenize() as retokenizer:\n",
    "            \n",
    "                           \n",
    "            for chunk in doc.noun_chunks:\n",
    "                print(doc[chunk.start : chunk.end])\n",
    "                \n",
    "\n",
    "                attrs = {\"tag\": chunk.root.tag, \"dep\": chunk.root.dep}\n",
    "\n",
    "                #iterate through all predicate terms.\n",
    "                for predicate in self.predicates:\n",
    "                    count = 0\n",
    "\n",
    "                    # iterate through the noun_chunk. If its first, second etc token match those of a\n",
    "                    # predicate word or phrase, then add to count.\n",
    "                    # iterator probably could be improved with recursion\n",
    "\n",
    "                    while count < len(predicate) and doc[chunk.start + count].lemma_ == predicate[count]:\n",
    "                        count += 1\n",
    "                        print(\"count: \", count, '|', \"predicate len: \", len(predicate))\n",
    "\n",
    "                    # Create a new noun_chunk based excluding the number of tokens detected as part of\n",
    "                    # a predicate phrase.\n",
    "                    # for example \"some other terrorist organisation\" become \"terrorist organisation\"\n",
    "\n",
    "                    if count > 0:\n",
    "                        print(\"count = \", count)\n",
    "                        print(f'start: doc[{chunk.start}] => plus count: doc[{chunk.start + count}] => end: doc[{chunk.end}]')\n",
    "                        print(f'start: {doc[chunk.start]} => plus count: {doc[chunk.start + count]} => end: {doc[chunk.end - 1]}')\n",
    "                        print(f'ent_iob: {doc[chunk.start].ent_iob} => plus count: {doc[chunk.start + count].ent_iob} => end: {doc[chunk.end - 1].ent_iob}')\n",
    "                        print(chunk.start, ': ', chunk, '=> becomes =>', doc[chunk.start + count : chunk.end])\n",
    "                    retokenizer.merge(doc[chunk.start + count : chunk.end], attrs = attrs)\n",
    "                \n",
    "        # Find matches in doc\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        # If none are found then return None\n",
    "        if not matches:\n",
    "            return pairs\n",
    "\n",
    "        for match_id, start, end in matches:\n",
    "            predicate = nlp.vocab.strings[match_id]\n",
    "            \n",
    "            if predicate in self.last: # if the predicate is in the list where the hypernym is last\n",
    "                hypernym = doc[end - 1]\n",
    "                hyponym = doc[start]\n",
    "            else:\n",
    "                hypernym = doc[start] # if the predicate is in the list where the hypernym is first\n",
    "                hyponym = doc[end - 1]\n",
    "\n",
    "                # create dictionary object with the format:\n",
    "                # {\"predicate\" : predicate term derived from pattern name,\n",
    "                #  \"pairs\" : [(hypernym, hyponym)] + [hyponym conjuncts (tokens linked by and | or)],\n",
    "                #  \"sent\" : string of sentence in which the pair appears}\n",
    "            \n",
    "            pairs.append(dict({\"predicate\" : predicate, \n",
    "                               \"pairs\" : [(hypernym, hyponym)] + [(hypernym, token) for token in hyponym.conjuncts if token != hypernym],\n",
    "                               \"sent\" : (hyponym.sent.text).strip()}))\n",
    "            \n",
    "        return pairs\n",
    "    \n",
    "h = hearst_patterns(extended = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test of Hearst Pattern Detection Object\n",
    "\n",
    "First sentence contains a 'first' relationship' where hypernym preceeds hyponym.\n",
    "\n",
    "Second sentence contains both a 'first' and 'last' relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########  0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'merges'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-418994dcc562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mshow_hyps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-167-418994dcc562>\u001b[0m in \u001b[0;36mshow_hyps\u001b[1;34m(lst)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'########### '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mhypernyms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_hyponyms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mfind_hyponyms\u001b[1;34m(self, text)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'merges'"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    #\"We are hunting for terrorist groups, particularly the Taliban and al Qaeda\",\n",
    "    \"We are using docs, spans, tokens and some other spacy features, such as merge_entities, merge_noun_chunks and especially retokenizer\"\n",
    "]\n",
    "\n",
    "def show_hyps(lst):\n",
    "    \n",
    "    if type(lst) == list:\n",
    "        for i, text in enumerate(lst):\n",
    "            print('########### ', i)\n",
    "            hypernyms = h.find_hyponyms(text)\n",
    "\n",
    "            if hypernyms:\n",
    "                for hypernym in hypernyms:\n",
    "                    print(hypernym[\"sent\"])\n",
    "                    print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "                    print()\n",
    "                    \n",
    "    elif type(lst) == str:\n",
    "         \n",
    "        hypernyms = h.find_hyponyms(lst)\n",
    "\n",
    "        if hypernyms:\n",
    "            for hypernym in hypernyms:\n",
    "                print(hypernym[\"sent\"])\n",
    "                print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "                print()\n",
    "                \n",
    "        print('-----')\n",
    "\n",
    "show_hyps(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | We\n",
      "4 | the IRA\n",
      "6 | ISIS\n",
      "8 | al Qaeda\n",
      "10 | some other terrorist groups\n",
      "14 | the Taliban\n",
      "16 | Web Scientists\n",
      "18 | particularly Southampton University\n",
      "[We, are, hunting, for, the IRA, ,, ISIS, ,, al Qaeda, and, some other terrorist groups, ,, such, as, the Taliban, ,, Web Scientists, and, particularly Southampton University]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for component in nlp2.pipe_names:\n",
    "    if component not in ['tagger', \"parser\", \"ner\"]:\n",
    "        nlp2.remove_pipe(component)\n",
    "\n",
    "merge_ents = nlp2.create_pipe(\"merge_entities\")\n",
    "nlp2.add_pipe(merge_ents)\n",
    "\n",
    "merge_nps = nlp2.create_pipe(\"merge_noun_chunks\")\n",
    "nlp2.add_pipe(merge_nps)\n",
    "\n",
    "\n",
    "text = docs[1]\n",
    "new_doc = nlp2(text)\n",
    "for chunk in new_doc.noun_chunks:\n",
    "    #if str(chunk).find(\"terrorist\") != -1:\n",
    "    print(chunk.start, '|', chunk)\n",
    "    \n",
    "print([span for span in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test With a Larger Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########  0\n",
      "Forty-four percent => becomes => Forty-four percent\n",
      "patients => becomes => patients\n",
      "uveitis => becomes => uveitis\n",
      "one or more identifiable signs => becomes => or more identifiable signs\n",
      "symptoms => becomes => symptoms\n",
      "red eye => becomes => red eye\n",
      "ocular pain => becomes => ocular pain\n",
      "visual acuity => becomes => visual acuity\n",
      "photophobia => becomes => photophobia\n",
      "order => becomes => order\n",
      "decreasing frequency => becomes => decreasing frequency\n",
      "Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\n",
      "such_as => [(symptoms, red eye), (symptoms, ocular pain), (symptoms, visual acuity), (symptoms, photophobia)]\n",
      "\n",
      "###########  1\n",
      "Other close friends => becomes => close friends\n",
      "Canada => becomes => Canada\n",
      "Australia => becomes => Australia\n",
      "Germany => becomes => Germany\n",
      "France => becomes => France\n",
      "forces => becomes => forces\n",
      "the operation => becomes => the operation\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "include => [(close friends, Canada), (close friends, Australia), (close friends, Germany), (close friends, France)]\n",
      "\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "as => [(the operation, forces)]\n",
      "\n",
      "###########  2\n",
      "The evidence => becomes => The evidence\n",
      "we => becomes => we\n",
      "all points => becomes => all points\n",
      "a collection => becomes => collection\n",
      "loosely affiliated terrorist organizations => becomes => loosely affiliated terrorist organizations\n",
      "al Qaeda => becomes => al Qaeda\n",
      "The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\n",
      "know_as => [(loosely affiliated terrorist organizations, al Qaeda)]\n",
      "\n",
      "###########  3\n",
      "Terrorist groups => becomes => Terrorist groups\n",
      "al Qaeda => becomes => al Qaeda\n",
      "the aid => becomes => the aid\n",
      "indifference => becomes => indifference\n",
      "governments => becomes => governments\n",
      "Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\n",
      "like => [(Terrorist groups, al Qaeda)]\n",
      "\n",
      "###########  4\n",
      "This new law => becomes => This new law\n",
      "I => becomes => I\n",
      "surveillance => becomes => surveillance\n",
      "all communications => becomes => all communications\n",
      "terrorists => becomes => terrorists\n",
      "e-mails => becomes => e-mails\n",
      "the Internet => becomes => the Internet\n",
      "cell phones => becomes => cell phones\n",
      "This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\n",
      "include => [(terrorists, e-mails), (terrorists, the Internet), (terrorists, cell phones)]\n",
      "\n",
      "###########  5\n",
      "this day => becomes => this day\n",
      "any nation => becomes => any nation\n",
      "terrorism => becomes => terrorism\n",
      "the United States => becomes => the United States\n",
      "a hostile regime => becomes => hostile regime\n",
      "###########  6\n",
      "We => becomes => We\n",
      "the Taliban => becomes => the Taliban\n",
      "al Qaeda => becomes => al Qaeda\n",
      "other terrorist groups => becomes => terrorist groups\n",
      "We are looking out for the Taliban, al Qaeda and other terrorist groups\n",
      "and-or_other => [(terrorist groups, al Qaeda), (terrorist groups, the Taliban)]\n",
      "\n",
      "###########  7\n",
      "We => becomes => We\n",
      "al Qaeda => becomes => al Qaeda\n",
      "other terrorist groups => becomes => terrorist groups\n",
      "especially the Taliban => becomes => the Taliban\n",
      "the muppets => becomes => the muppets\n",
      "We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\n",
      "and-or_other => [(terrorist groups, al Qaeda)]\n",
      "\n",
      "We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\n",
      "especially => [(terrorist groups, the Taliban), (terrorist groups, the muppets)]\n",
      "\n",
      "Wall time: 263 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a list of docs\n",
    "docs = [\n",
    "    \"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\",\n",
    "    \"Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\",\n",
    "    \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\",\n",
    "    \"Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\",\n",
    "    \"This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\",\n",
    "    \"From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\",\n",
    "    \"We are looking out for the Taliban, al Qaeda and other terrorist groups\",\n",
    "    \"We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\"\n",
    "]\n",
    "show_hyps(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with a Full Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E102] Can't merge non-disjoint spans. 'desk' is already part of tokens to merge. If you want to find the longest non-overlapping spans, you can use the util.filter_spans helper:\nhttps://spacy.io/api/top-level#util.filter_spans",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-fc1878896aa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow_hyps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bush\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-efb50bb225bd>\u001b[0m in \u001b[0;36mshow_hyps\u001b[1;34m(lst)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'########### '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mhypernyms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_hyponyms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mfind_hyponyms\u001b[1;34m(self, text)\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize.Retokenizer.merge\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E102] Can't merge non-disjoint spans. 'desk' is already part of tokens to merge. If you want to find the longest non-overlapping spans, you can use the util.filter_spans helper:\nhttps://spacy.io/api/top-level#util.filter_spans"
     ]
    }
   ],
   "source": [
    "show_hyps(orators[\"bush\"].texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
