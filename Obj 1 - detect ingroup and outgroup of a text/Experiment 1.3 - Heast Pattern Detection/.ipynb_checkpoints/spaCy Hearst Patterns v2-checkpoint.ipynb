{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Hearst Patterns\n",
    "---\n",
    "\n",
    "In this experiment we test the utility of Hearst Patterns for detecting the ingroup and outgroup of a text.\n",
    "\n",
    "For this experiment spaCy matcher is used with code adapted from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/hearstPatterns.py\n",
    "\n",
    "Hypernym relations are semantic relationships between two concepts: C1 is a hypernym of C2 means that C1 categorizes C2 (e.g. “instrument” is a hypernym of “Piano”). For this research, the phrase, \"America has enemies, such as Al Qaeda and the Taliban\" would return the following '[('Al Qaeda', 'enemy'), ('the Taliban', 'enemy')]'. In this example, the categorising term 'enemy' is a hypernym of both 'Al Qaeda' and the 'Taliban'; conversely 'al Qaeda' and 'the Tabliban' are hyponyms of 'enemy'. Using this technique, hypernym terms could be classified as ingroup or outgroup and named entities identified as hyponym terms could be identified as either group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the spaCy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "for component in nlp.pipe_names:\n",
    "    if component not in ['tagger', \"parser\", \"ner\"]:\n",
    "        self.nlp.remove_pipe(component)\n",
    "\n",
    "merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(merge_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\n",
      "C:\\Users\\Steve\\anaconda3\\python37.zip\n",
      "C:\\Users\\Steve\\anaconda3\\DLLs\n",
      "C:\\Users\\Steve\\anaconda3\\lib\n",
      "C:\\Users\\Steve\\anaconda3\n",
      "\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\win32\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\win32\\lib\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\Pythonwin\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\Steve\\.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "#sys.path.insert(0, r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\")\n",
    "\n",
    "#del(sys.path)[0]\n",
    "\n",
    "for p in sys.path:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset of Political Speeches from George Bush, Osama bin Laden and Martin Luther King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object bush called George Bush has 14 speeches\n",
      "object king called Martin Luther King has 5 speeches\n",
      "object laden called Osama bin Laden has 7 speeches\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "from cndlib import entities\n",
    "\n",
    "dirpath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\speeches\"\n",
    "\n",
    "orators = entities.Dataset(dirpath)\n",
    "\n",
    "for orator in orators:\n",
    "    print(f'object {orator.ref} called {orator.name} has {len(orator)} speeches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hearst Pattern Detection Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :   - Ive Been to the Mountaintop.txt\n",
      "the Lord\n",
      "for fucks sake:  a kind\n",
      "for fucks sake:  a kind\n",
      "for fucks sake:  a kind\n",
      "for fucks sake:  a kind\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mfind_hyponyms\u001b[1;34m(self, text)\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize.Retokenizer.__exit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize._merge\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.vector.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Hearst patterns take the form of (NP <predicate> (NP (and | or)?)+)\n",
    "\n",
    "class hearst_patterns(object):\n",
    "    \n",
    "    \"\"\" Hearst Patterns is a class object used to detects hypernym relations to hyponyms in a text\n",
    "    \n",
    "    input: raw text\n",
    "    returns: list of dict object with each entry all the hypernym-hyponym pairs of a text\n",
    "    entry format: [\"predicate\" : [(hyponym, hypernym), (hyponym, hypernym), ..]]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import spacy\n",
    "    \n",
    "    def __init__(self, extended=False):\n",
    "        \n",
    "#     Included in each entry is the original regex pattern now adapted as a spaCy matcher pattern.\n",
    "#     Many of these patterns are in the same format, next iteration of code should include an\n",
    "#     automatic pattern generator for patterns.\n",
    "            \n",
    "#     These patterns need checking and cleaning up for testing.\n",
    "            \n",
    "#     Format for the dict entry of each pattern\n",
    "#     {\n",
    "#      \"label\" : predicate, \n",
    "#      \"pattern\" : spaCy pattern, \n",
    "#      \"posn\" : first/last depending on whether the hypernym appears before its hyponym\n",
    "#     }\n",
    "      \n",
    "        # make the patterns easier to read\n",
    "        # as lexical understanding develops, consider adding attributes to dstinguish between hypernyms and hyponyms\n",
    "        hypernym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}} \n",
    "        hyponym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
    "        punct = {\"IS_PUNCT\": True, \"OP\": \"?\"}\n",
    "\n",
    "        self.patterns = [\n",
    "\n",
    "        {\"label\" : \"such_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\": \"such\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"know_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?know as (NP_\\\\w+ ?(, )?(and |or )?)+)', # added for this experiment\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\": \"know\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"such_NOUN_as\", \"pattern\" : [\n",
    "#                 '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             {\"LEMMA\": \"such\"}, hypernym, punct, {\"LEMMA\": \"as\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"include\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\" : \"include\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"especially\", \"pattern\" : [ ## problem - especially is merged as a modifier in to a noun phrase\n",
    "#                 '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\" : \"especially\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"and-or_other\", \"pattern\" : [ ## problem - other is merged as a modifier in to a noun phrase\n",
    "#                 '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "#                 'last'\n",
    "             hyponym, punct, {\"DEP\": \"cc\"}, {\"LEMMA\" : \"other\"}, hypernym\n",
    "        ], \"posn\" : \"last\"},\n",
    "\n",
    "        ]\n",
    "\n",
    "        if extended:\n",
    "            self.patterns.extend([\n",
    "\n",
    "            {\"label\" : \"which_may_include\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "#                     '?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"may\"}, {\"LEMMA\" : \"include\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"which_be_similar_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"similar\"}, {\"LEMMA\" : \"to\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"example_of_this_be\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"this\"}, {\"LEMMA\" : \"be\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \",type\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"type\"}, punct, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"mainly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"mainly\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"mostly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"mostly\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"notably\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"notably\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"particularly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"particularly\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"principally\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)', - fuses in a noun phrase\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"principally\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"in_particular\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"in\"}, {\"LEMMA\" : \"particular\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"except\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"except\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"other_than\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"other\"}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"eg\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"e.g.\", \"eg\"]}}, hyponym \n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "#                 {\"label\" : \"eg-ie\", \"pattern\" : [ \n",
    "# #                     '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+' - need to understand this pattern better\n",
    "# #                     '(\\\\. )?\\\\))',\n",
    "# #                     'first'\n",
    "#                     hypernym, punct, {\"LEMMA\" : {IN : [\"e.g.\", \"i.e.\", \"eg\", \"ie\"]}}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "#                 ]},\n",
    "\n",
    "            {\"label\" : \"ie\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"i.e.\", \"ie\"]}}, hyponym \n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"for_example\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?for example (, )?'\n",
    "#                     '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"example\"}, punct, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"example_of_be\", \"pattern\" : [\n",
    "#                     'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym, punct, {\"LEMMA\" : \"be\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"like\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"like\"}, hyponym,\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            # repeat of such_as pattern in primary patterns???\n",
    "#                     'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "\n",
    "                {\"label\" : \"whether\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"whether\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"compare_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"compare\"}, {\"LEMMA\" : \"to\"}, hyponym \n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"among_-PRON-\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"among\"}, {\"LEMMA\" : \"-PRON-\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"for_instance\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "#                     'for instance)',\n",
    "#                     'first'\n",
    "                hypernym, punct, hyponym, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"instance\"}\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"and-or_any_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"DEP\": \"cc\"}, {\"LEMMA\" : \"any\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"some_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"some\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"be_a\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"a\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"like_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"like\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "             {\"label\" : \"one_of_the\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"the\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"one_of_these\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "#                     'last'\n",
    "            hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"these\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"one_of_those\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "#                     'last'\n",
    "            hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"those\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"be_example_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)', added optional \"an\" to spaCy pattern for singular vs. plural\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"an\", \"OP\" : \"?\"}, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"which_be_call\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"call\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "#               \n",
    "            {\"label\" : \"which_be_name\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"name\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"a_kind_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"a\", \"OP\" : \"?\"}, {\"LEMMA\" : \"kind\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)', - combined with above\n",
    "#                     'last'\n",
    "\n",
    "            {\"label\" : \"form_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"a\", \"OP\" : \"?\"}, {\"LEMMA\" : \"form\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"which_look_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"look\"}, {\"LEMMA\" : \"like\"}, hyponym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"which_sound_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"sound\"}, {\"LEMMA\" : \"like\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"type\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"type\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"compare_with\", \"pattern\" : [\n",
    "#                     '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                {\"LEMMA\" : \"compare\"}, hyponym, punct, {\"LEMMA\" : \"with\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"as\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"as\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"sort_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"sort\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "        ]),        \n",
    "\n",
    "    ## initiate matcher\n",
    "        from spacy.matcher import Matcher\n",
    "        self.matcher = Matcher(nlp.vocab, validate = True)\n",
    "\n",
    "        self.predicates = []\n",
    "        self.first = []\n",
    "        self.last = []\n",
    "\n",
    "        # add patterns to matcher\n",
    "        for pattern in self.patterns:\n",
    "            self.matcher.add(pattern[\"label\"], None, pattern[\"pattern\"])\n",
    "\n",
    "            # gather list of predicate terms for the noun_chunk deconfliction\n",
    "            self.predicates.append(pattern[\"label\"].split('_'))\n",
    "\n",
    "            # gather list of predicates where the hypernym appears first\n",
    "            if pattern[\"posn\"] == \"first\":\n",
    "                self.first.append(pattern[\"label\"])\n",
    "\n",
    "            # gather list of predicates where the hypernym appears last\n",
    "            if pattern[\"posn\"] == \"last\":\n",
    "                self.last.append(pattern[\"label\"])\n",
    "\n",
    "    def isPredicateMatch(self, chunk, predicates):\n",
    "        \n",
    "            \"\"\"\n",
    "            Function to remove predicate phrases from noun_chunks\n",
    "            \n",
    "            input: the chunk to be checked, list of predicate phrases\n",
    "            returns: the chnunk with predicate phrases removed.\n",
    "            \n",
    "            \"\"\"\n",
    "\n",
    "            def match(count, chunk, predicates):\n",
    "\n",
    "                while predicates != [] and count < len(predicates[0]) and chunk[count].lemma_ == predicates[0][count]:\n",
    "                    count += 1 \n",
    "\n",
    "                return count\n",
    "\n",
    "            def isMatch(chunk, predicates):\n",
    "                counter = match(0, chunk, predicates)\n",
    "\n",
    "                if predicates == [] or counter == len(predicates[0]):\n",
    "                    if len(chunk[counter:]) == 0:\n",
    "                        print(\"for fucks sake: \", chunk)\n",
    "                    return chunk[counter:]\n",
    "                else:\n",
    "                    return isMatch(chunk, predicates[1:])\n",
    "\n",
    "            return isMatch(chunk, predicates)\n",
    "    \n",
    "    \n",
    "    def find_hyponyms(self, text):\n",
    "        \n",
    "        \"\"\"\n",
    "        this is the main function of the class object\n",
    "        \n",
    "        follows logic of:\n",
    "        1. checks whether text has been parsed\n",
    "        2. pre-processing for noun_chunks\n",
    "        3. generate matches\n",
    "        4. create list of dict object containing match results\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(text) is spacy.tokens.doc.Doc:\n",
    "            doc = text\n",
    "        else:\n",
    "            doc = nlp(text) # initiate doc \n",
    "            \n",
    "        \n",
    "        ## Pre-processing\n",
    "        # there are some predicate terms, such as \"particularly\", \"especially\" and \"some other\" which are\n",
    "        # merged with the noun phrase. Such terms are part of the pattern and become part of the\n",
    "        # merged noun-chunk, consequently, they are not detected in by the matcher.\n",
    "        # This pre-processing, therefore, walks through the noun_chunks of a doc object to remove those\n",
    "        # predicate terms from each noun_chunk and merges the result.\n",
    "        \n",
    "        length = len(list(doc.noun_chunks))\n",
    "        print(list(doc.noun_chunks)[-1])\n",
    "        with doc.retokenize() as retokenizer:\n",
    "\n",
    "            for i, chunk in enumerate(doc.noun_chunks):\n",
    "\n",
    "                attrs = {\"tag\": chunk.root.tag, \"dep\": chunk.root.dep}\n",
    "                \n",
    "                new_chunk = self.isPredicateMatch(chunk, self.predicates)\n",
    "                \n",
    "                #if i < 1188:\n",
    "#                     print(i, ': ', new_chunk, '==', new_chunk.sent.text.strip())\n",
    "#                     print(chunk.start, '|', chunk.end, '|', len(doc))\n",
    "#                     print(i, '|', length)\n",
    "\n",
    "                retokenizer.merge(new_chunk, attrs = attrs)\n",
    "\n",
    "                \n",
    "                \n",
    "        ## Main Body\n",
    "        #Find matches in doc\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        pairs = [] # set up dictionary containing pairs\n",
    "        \n",
    "        # If none are found then return None\n",
    "        if not matches:\n",
    "            return pairs\n",
    "\n",
    "        for match_id, start, end in matches:\n",
    "            predicate = nlp.vocab.strings[match_id]\n",
    "            \n",
    "            if predicate in self.last: # if the predicate is in the list where the hypernym is last\n",
    "                hypernym = doc[end - 1]\n",
    "                hyponym = doc[start]\n",
    "            else:\n",
    "                hypernym = doc[start] # if the predicate is in the list where the hypernym is first\n",
    "                hyponym = doc[end - 1]\n",
    "\n",
    "            # create a list of dictionary objects with the format:\n",
    "            # {\n",
    "            # \"predicate\" : \" predicate term based from pattern name,\n",
    "            # \"pairs\" : [(hypernym, hyponym)] + [hyponym conjuncts (tokens linked by and | or)]\n",
    "            # \"sent\" : sentence in which the pairs originate\n",
    "            # }\n",
    "            \n",
    "            pairs.append(dict({\"predicate\" : predicate, \n",
    "                               \"pairs\" : [(hypernym, hyponym)] + [(hypernym, token) for token in hyponym.conjuncts if token != hypernym],\n",
    "                               \"sent\" : (hyponym.sent.text).strip()}))\n",
    "\n",
    "        return pairs\n",
    "    \n",
    "h = hearst_patterns(extended = True)\n",
    "\n",
    "for i, text in enumerate(orators[\"king\"]):\n",
    "    print(i, ': ', text.title)\n",
    "\n",
    "    hypernyms = h.find_hyponyms(text.text)\n",
    "\n",
    "    if hypernyms:\n",
    "        for hypernym in hypernyms:\n",
    "            print(hypernym[\"sent\"])\n",
    "            print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test of Hearst Pattern Detection Object\n",
    "\n",
    "First sentence contains a 'first' relationship' where hypernym preceeds hyponym.\n",
    "\n",
    "Second sentence contains both a 'first' and 'last' relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"We are hunting for terrorist groups, particularly the Taliban and al Qaeda\",\n",
    "    \"We are hunting for the IRA, ISIS, al Qaeda and some other terrorist groups, especially the Taliban, Web Scientists and particularly Southampton University\"\n",
    "]\n",
    "\n",
    "def show_hyps(o):\n",
    "    \n",
    "   \n",
    "    for i, text in enumerate(o):\n",
    "        print(i, \"#####\")\n",
    "        hypernyms = h.find_hyponyms(text)\n",
    "\n",
    "        if hypernyms:\n",
    "            for hypernym in hypernyms:\n",
    "                print(hypernym[\"sent\"])\n",
    "                print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "                print()\n",
    "                \n",
    "        \n",
    "    \n",
    "        print('-----')\n",
    "\n",
    "show_hyps(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test With a Larger Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########  0\n",
      "Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\n",
      "such_as => [(symptoms, red eye), (symptoms, ocular pain), (symptoms, visual acuity), (symptoms, photophobia)]\n",
      "\n",
      "-----\n",
      "###########  1\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "include => [(Other close friends, Canada), (Other close friends, Australia), (Other close friends, Germany), (Other close friends, France)]\n",
      "\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "as => [(the operation, forces)]\n",
      "\n",
      "-----\n",
      "###########  2\n",
      "The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\n",
      "know_as => [(loosely affiliated terrorist organizations, al Qaeda)]\n",
      "\n",
      "-----\n",
      "###########  3\n",
      "Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\n",
      "like => [(Terrorist groups, al Qaeda)]\n",
      "\n",
      "-----\n",
      "###########  4\n",
      "This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\n",
      "include => [(terrorists, e-mails), (terrorists, the Internet), (terrorists, cell phones)]\n",
      "\n",
      "-----\n",
      "###########  5\n",
      "From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\n",
      "as => [(a hostile regime, the United States)]\n",
      "\n",
      "-----\n",
      "###########  6\n",
      "-----\n",
      "###########  7\n",
      "We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\n",
      "especially => [(other terrorist groups, the Taliban), (other terrorist groups, the muppets)]\n",
      "\n",
      "-----\n",
      "Wall time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a list of docs\n",
    "docs = [\n",
    "    \"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\",\n",
    "    \"Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\",\n",
    "    \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\",\n",
    "    \"Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\",\n",
    "    \"This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\",\n",
    "    \"From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\",\n",
    "    \"We are looking out for the Taliban, al Qaeda and other terrorist groups\",\n",
    "    \"We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\"\n",
    "]\n",
    "show_hyps(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with a Full Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :   - Ive Been to the Mountaintop.txt\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6fe134547eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mhypernyms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_hyponyms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mfind_hyponyms\u001b[1;34m(self, text)\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize.Retokenizer.__exit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize._merge\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.vector.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(orators[\"king\"]):\n",
    "    print(i, ': ', text.title)\n",
    "    \n",
    "    hypernyms = h.find_hyponyms(text.text)\n",
    "\n",
    "    if hypernyms:\n",
    "        for hypernym in hypernyms:\n",
    "            print(hypernym[\"sent\"])\n",
    "            print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
