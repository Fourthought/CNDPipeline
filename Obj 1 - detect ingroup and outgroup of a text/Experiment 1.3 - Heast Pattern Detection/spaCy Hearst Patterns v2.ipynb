{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Hearst Patterns\n",
    "---\n",
    "\n",
    "In this experiment we test the utility of Hearst Patterns for detecting the ingroup and outgroup of a text.\n",
    "\n",
    "For this experiment spaCy matcher is used with code adapted from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/hearstPatterns.py\n",
    "\n",
    "Hypernym relations are semantic relationships between two concepts: C1 is a hypernym of C2 means that C1 categorizes C2 (e.g. “instrument” is a hypernym of “Piano”). For this research, the phrase, \"America has enemies, such as Al Qaeda and the Taliban\" would return the following '[('Al Qaeda', 'enemy'), ('the Taliban', 'enemy')]'. In this example, the categorising term 'enemy' is a hypernym of both 'Al Qaeda' and the 'Taliban'; conversely 'al Qaeda' and 'the Tabliban' are hyponyms of 'enemy'. Using this technique, hypernym terms could be classified as ingroup or outgroup and named entities identified as hyponym terms could be identified as either group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the spaCy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "for component in nlp.pipe_names:\n",
    "    if component not in ['tagger', \"parser\", \"ner\"]:\n",
    "        self.nlp.remove_pipe(component)\n",
    "\n",
    "merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(merge_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\n",
      "C:\\Users\\Steve\\anaconda3\\python37.zip\n",
      "C:\\Users\\Steve\\anaconda3\\DLLs\n",
      "C:\\Users\\Steve\\anaconda3\\lib\n",
      "C:\\Users\\Steve\\anaconda3\n",
      "\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\win32\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\win32\\lib\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\Pythonwin\n",
      "C:\\Users\\Steve\\anaconda3\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\Steve\\.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "#sys.path.insert(0, r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\")\n",
    "\n",
    "#del(sys.path)[0]\n",
    "\n",
    "for p in sys.path:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset of Political Speeches from George Bush, Osama bin Laden and Martin Luther King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object bush called George Bush has 14 speeches\n",
      "object king called Martin Luther King has 5 speeches\n",
      "object laden called Osama bin Laden has 7 speeches\n"
     ]
    }
   ],
   "source": [
    "from cndlib import entities\n",
    "\n",
    "dirpath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\speeches\"\n",
    "\n",
    "orators = entities.Dataset(dirpath)\n",
    "\n",
    "for orator in orators:\n",
    "    print(f'object {orator.ref} called {orator.name} has {len(orator)} speeches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hearst Pattern Detection Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  -OBL Declaration v2.txt\n",
      "I bear witness that there is no God except Allah-no associates with Him-\n",
      "except => [(no God, Allah-no associates)]\n",
      "\n",
      "It should not be hidden from you that the people of Islam had suffered from aggression, iniquity and injustice imposed on them by the Zionist-Crusaders alliance and their collaborators; to the extent that the Muslims blood became the cheapest and their wealth as loot in the hands of the enemies.\n",
      "as => [(loot, their wealth), (loot, cheapest)]\n",
      "\n",
      "And after a long absence, imposed on the scholars (Ulama) and callers (Da'ees) of Islam by the iniquitous crusaders movement under the leadership of the USA; who fears that they, the scholars and callers of Islam, will instigate the Ummah of Islam against its' enemies as their ancestor scholars-may Allah be pleased with them- like Ibn Taymiyyah and Al'iz Ibn Abdes-Salaam did.\n",
      "as => [(their ancestor, its' enemies)]\n",
      "\n",
      "And after a long absence, imposed on the scholars (Ulama) and callers (Da'ees) of Islam by the iniquitous crusaders movement under the leadership of the USA; who fears that they, the scholars and callers of Islam, will instigate the Ummah of Islam against its' enemies as their ancestor scholars-may Allah be pleased with them- like Ibn Taymiyyah and Al'iz Ibn Abdes-Salaam did.\n",
      "like => [(them-, Ibn Taymiyyah), (them-, Al'iz Ibn Abdes-Salaam)]\n",
      "\n",
      "the prominent Sheikh Salman Al-Oud'a and Sheikh Safar Al-Hawali and their brothers\n",
      "among_-PRON- => [(the two Holy Places-, the prominent Sheikh Salman Al-Oud'a), (the two Holy Places-, Sheikh Safar Al-Hawali), (the two Holy Places-, their brothers)]\n",
      "\n",
      "The crusader forces became the main cause of our disastrous condition, particularly in the economical aspect of it due to the unjustified heavy spending on these forces.\n",
      "as => [(a result, these forces)]\n",
      "\n",
      "; the media carried out the plan of the enemy of idolising cult of certain personalities and spreading scandals among the believers to repel the people away from their religion, as Allah, the Exalted said: {surely- as for-\n",
      "as => [(Allah, their religion)]\n",
      "\n",
      "; the media carried out the plan of the enemy of idolising cult of certain personalities and spreading scandals among the believers to repel the people away from their religion, as Allah, the Exalted said: {surely- as for-\n",
      "as => [(for-, surely-)]\n",
      "\n",
      "They refused to be played against each others and to be used by the regime as a tool to carry out the policy of the American-Israeli alliance through their agent in our country: the Saudi regime.\n",
      "as => [(a tool, the regime)]\n",
      "\n",
      "No other priority, except Belief, could be considered before it; the people of knowledge, Ibn Taymiyyah, stated: \"to fight in defence of religion and Belief is a collective duty; there is no other duty after Belief than fighting the enemy who is corrupting the life and the religion.\n",
      "except => [(No other priority, Belief)]\n",
      "\n",
      "Man fabricated laws were put forward permitting what has been forbidden by Allah such as usury (Riba) and other matters.\n",
      "such_as => [(Allah, usury), (Allah, other matters)]\n",
      "\n",
      ", what is it then to the person who make himself a partner and equal to Allah, legalising (usury and other sins)\n",
      "and-or_other => [(sins, usury), (sins, legalising)]\n",
      "\n",
      "1-consumption of the Muslims human resources as most casualties and fatalities will be among the Muslims people.\n",
      "as => [(most casualties, the Muslims human resources)]\n",
      "\n",
      "They consider the Seerah of their forefathers as a source and an example for re-establishing the greatness of this Ummah and to raise the word of Allah again.\n",
      "as => [(a source, their forefathers)]\n",
      "\n",
      "To those little group of men within the army, police and security forces, who have been tricked and pressured by the regime to attack the Muslims and spill their blood, we would like to remind them of the narration: (I promise war against those who take my friends as their enemy) narrated by Al--Bukhari.\n",
      "as => [(their enemy, my friends)]\n",
      "\n",
      "you moved tens of thousands of international force, including twenty eight thousands American solders into Somalia.\n",
      "include => [(international force, twenty eight thousands American solders)]\n",
      "\n",
      "(Saheeh Al-Jame' As-Sagheer).\n",
      "as => [(-Sagheer, (Saheeh Al-Jame')]\n",
      "\n",
      "Its appropriate \"remedy,\" however, is in the hands of the youths of Islam, as the poet said:\n",
      " \n",
      "I am willing to sacrifice self and wealth for knights who never disappointed me.\n",
      "as => [(the poet, Islam)]\n",
      "\n",
      "Our youths knew that the humiliation suffered by the Muslims as a result of the occupation of their sanctities can not be kicked and removed except by explosions and Jihad.\n",
      "as => [(a result, the Muslims)]\n",
      "\n",
      "Our youths knew that the humiliation suffered by the Muslims as a result of the occupation of their sanctities can not be kicked and removed except by explosions and Jihad.\n",
      "as => [(the poet, Jihad), (the poet, explosions)]\n",
      "\n",
      "Our Lord, do not lay on us a burden as Thou didst lay on those before us; Our Lord, do not impose upon us that which we have no strength to bear; and pardon us and grant us protection and have mercy on us,\n",
      "as => [(Thou, a burden)]\n",
      "\n",
      "1 :  -OBL Declaration v3.txt\n",
      "I bear witness that there is no God except Allah-no associates with Him-\n",
      "except => [(no God, Allah-no associates)]\n",
      "\n",
      "It should not be hidden from you that the people of Islam had suffered from aggression, iniquity and injustice imposed on them by the Zionist-Crusaders alliance and their collaborators; to the extent that the Muslims blood became the cheapest and their wealth as loot in the hands of the enemies.\n",
      "as => [(loot, their wealth), (loot, cheapest)]\n",
      "\n",
      "And after a long absence, imposed on the scholars (Ulama) and callers (Da'ees) of Islam by the iniquitous crusaders movement under the leadership of the USA; who fears that they, the scholars and callers of Islam, will instigate the Ummah of Islam against its' enemies as their ancestor scholars-may Allah be pleased with them- like Ibn Taymiyyah and Al'iz Ibn Abdes-Salaam did.\n",
      "as => [(their ancestor, its' enemies)]\n",
      "\n",
      "And after a long absence, imposed on the scholars (Ulama) and callers (Da'ees) of Islam by the iniquitous crusaders movement under the leadership of the USA; who fears that they, the scholars and callers of Islam, will instigate the Ummah of Islam against its' enemies as their ancestor scholars-may Allah be pleased with them- like Ibn Taymiyyah and Al'iz Ibn Abdes-Salaam did.\n",
      "like => [(them-, Ibn Taymiyyah), (them-, Al'iz Ibn Abdes-Salaam)]\n",
      "\n",
      "the prominent Sheikh Salman Al-Oud'a and Sheikh Safar Al-Hawali and their brothers\n",
      "among_-PRON- => [(the two Holy Places-, the prominent Sheikh Salman Al-Oud'a), (the two Holy Places-, Sheikh Safar Al-Hawali), (the two Holy Places-, their brothers)]\n",
      "\n",
      "The crusader forces became the main cause of our disastrous condition, particularly in the economical aspect of it due to the unjustified heavy spending on these forces.\n",
      "as => [(a result, these forces)]\n",
      "\n",
      "; the media carried out the plan of the enemy of idolising cult of certain personalities and spreading scandals among the believers to repel the people away from their religion, as Allah, the Exalted said: {surely- as for-\n",
      "as => [(Allah, their religion)]\n",
      "\n",
      "; the media carried out the plan of the enemy of idolising cult of certain personalities and spreading scandals among the believers to repel the people away from their religion, as Allah, the Exalted said: {surely- as for-\n",
      "as => [(for-, surely-)]\n",
      "\n",
      "They refused to be played against each others and to be used by the regime as a tool to carry out the policy of the American-Israeli alliance through their agent in our country: the Saudi regime.\n",
      "as => [(a tool, the regime)]\n",
      "\n",
      "No other priority, except Belief, could be considered before it; the people of knowledge, Ibn Taymiyyah, stated: \"to fight in defence of religion and Belief is a collective duty; there is no other duty after Belief than fighting the enemy who is corrupting the life and the religion.\n",
      "except => [(No other priority, Belief)]\n",
      "\n",
      "Man fabricated laws were put forward permitting what has been forbidden by Allah such as usury (Riba) and other matters.\n",
      "such_as => [(Allah, usury), (Allah, other matters)]\n",
      "\n",
      ", what is it then to the person who make himself a partner and equal to Allah, legalising (usury and other sins)\n",
      "and-or_other => [(sins, usury), (sins, legalising)]\n",
      "\n",
      "1-consumption of the Muslims human resources as most casualties and fatalities will be among the Muslims people.\n",
      "as => [(most casualties, the Muslims human resources)]\n",
      "\n",
      "They consider the Seerah of their forefathers as a source and an example for re-establishing the greatness of this Ummah and to raise the word of Allah again.\n",
      "as => [(a source, their forefathers)]\n",
      "\n",
      "2 :  -OBL Declaration.txt\n",
      "I bear witness that there is no God except Allah, no associates has He, and I bear witness that Muhammad is His slave and messenger.\n",
      "except => [(no God, Allah)]\n",
      "\n",
      "And after a long absence, imposed on the scholars (Ulama) and callers (Da'ees) of Islam, by the iniquitous Crusaders movement under the leadership of the USA, who fears that they, the scholars and callers of Islam, will instigate the Ummah of Islam against its enemies as their ancestors, may Allah be pleased with them, like Ibn Taymiyyah and Al'iz Ibn Abd es-Salaam, did.\n",
      "as => [(their ancestors, its enemies)]\n",
      "\n",
      "They refused to be played against each others and to be used by the regime as a tool to carry out the policy of the American/Israeli alliance through their agent in our country:\n",
      "as => [(a tool, the regime)]\n",
      "\n",
      "No other priority, except Belief, could be considered before it; the people of knowledge, as Ibn Taymiyyah said, stated: \"To fight in defence of religion and Belief is a collective duty; there is no other duty after Belief than fighting the enemy who is corrupting the life and the religion.\n",
      "except => [(No other priority, Belief)]\n",
      "\n",
      "No other priority, except Belief, could be considered before it; the people of knowledge, as Ibn Taymiyyah said, stated: \"To fight in defence of religion and Belief is a collective duty; there is no other duty after Belief than fighting the enemy who is corrupting the life and the religion.\n",
      "as => [(Ibn Taymiyyah, knowledge)]\n",
      "\n",
      "Humanly fabricated laws have been put forward permitting what has been forbidden by Allah such as usury (Riba) and other matters.\n",
      "such_as => [(Allah, usury), (Allah, other matters)]\n",
      "\n",
      ", what is it then to the person who make himself a partner and equal to Allah, legalising (usury and other sins)\n",
      "and-or_other => [(sins, usury), (sins, legalising)]\n",
      "\n",
      "Consumption of the Muslims' human resources, as most casualties and fatalities will be among the Muslims people.\n",
      "as => [(most casualties, the Muslims' human resources)]\n",
      "\n",
      "They consider the Seerah of their forefathers as a source and an example for re-establishing the greatness of this Ummah and to raise the word of Allah again.\n",
      "as => [(a source, their forefathers)]\n",
      "\n",
      "The regime wants to deceive the Muslim people in the same manner as the Palestinian Mujahideen were deceived, causing the loss of Al-Aqsa Mosque.\n",
      "as => [(the Palestinian Mujahideen, the same manner)]\n",
      "\n",
      "To those little group of men within the army, police and security forces, who have been tricked and pressurised by the regime to attack the Muslims and spill their blood, we would like to remind them of the narration: (I promise war against those who take my friends as their enemy) narrated by Al-Bukhari; and his saying (Allah's Blessings and Salutations be on him):\n",
      "as => [(their enemy, my friends)]\n",
      "\n",
      "where, after vigorous propaganda about the power of the USA and its post-cold war leadership of the new world order, you moved tens of thousands of international forces, including twenty-eight thousand American solders, into Somalia.\n",
      "include => [(international forces, twenty-eight thousand American solders)]\n",
      "\n",
      "(Saheeh Al-Jame' As-Sagheer).\n",
      "as => [(-Sagheer, (Saheeh Al-Jame)]\n",
      "\n",
      "(Saheeh Al-Jame' As-Sagheer)\n",
      "as => [(-Sagheer, (Saheeh Al-Jame)]\n",
      "\n",
      "Its appropriate remedy ,however, is in the hands of the youth of Islam, as the poet said:\n",
      "as => [(the poet, Islam)]\n",
      "\n",
      "Our youth know that the humiliation suffered by the Muslims as a result of the occupation of their sanctuaries cannot be obliterated and removed except by explosions and Jihad.\n",
      "as => [(a result, the Muslims)]\n",
      "\n",
      "Our youth know that the humiliation suffered by the Muslims as a result of the occupation of their sanctuaries cannot be obliterated and removed except by explosions and Jihad.\n",
      "as => [(the poet, Jihad), (the poet, explosions)]\n",
      "\n",
      "Our Lord, do not lay on us a burden as Thou didst lay on those before us; Our Lord, do not impose upon us that which we have no strength to bear; and pardon us and grant us protection and have mercy on us,\n",
      "as => [(Thou, a burden)]\n",
      "\n",
      "3 :  -OBL Full Warning.txt\n",
      "4 :  -OBL.txt\n",
      "5 :  -OBL Letter to America.txt\n",
      "Here we wanted to outline the truth - as an explanation and warning - hoping for Allah's reward, seeking success and support from Him.\n",
      "as => [(an explanation, the truth)]\n",
      "\n",
      "Muslims believe in all of the Prophets, including Abraham, Moses, Jesus and Muhammad, peace and blessings of Allah be upon them all.\n",
      "include => [(the Prophets, Abraham), (the Prophets, Moses), (the Prophets, peace), (the Prophets, Jesus), (the Prophets, blessings), (the Prophets, Muhammad)]\n",
      "\n",
      "Allah has challenged anyone to bring a book like the Quran or even ten verses like it.\n",
      "like => [(a book, the Quran), (a book, even ten verses)]\n",
      "\n",
      "Yet you build your economy and investments on Usury.\n",
      "as => [(a result, Usury)]\n",
      "\n",
      "You are a nation that exploits women like consumer products or advertising tools calling upon customers to purchase them.\n",
      "like => [(women, consumer products), (women, advertising tools)]\n",
      "\n",
      "And because of all this, you have been described in history as a nation that spreads diseases that were unknown to man in the past.\n",
      "as => [(a nation, history)]\n",
      "\n",
      "Go ahead and boast to the nations of man, that you brought them AIDS as a Satanic American Invention.\n",
      "as => [(a Satanic American Invention, AIDS)]\n",
      "\n",
      "This is our message to the Americans, as an answer to theirs.\n",
      "as => [(an answer, the Americans)]\n",
      "\n",
      "6 :  -Al Jazeera Speech.txt\n",
      "No one except a dumb thief plays with the security of others and then makes himself believe he will be secure.\n",
      "except => [(No one, a dumb thief)]\n",
      "\n",
      "This means the oppressing and embargoing to death of millions as Bush Sr did in Iraq in the greatest mass slaughter of children mankind has ever known, and it means the throwing of millions of pounds of bombs and explosives at millions of children - also in Iraq - as Bush Jr did, in order to remove an old agent and replace him with a new puppet to assist in the pilfering of Iraq's oil and other outrages.\n",
      "as => [(Bush Sr, millions)]\n",
      "\n",
      "This means the oppressing and embargoing to death of millions as Bush Sr did in Iraq in the greatest mass slaughter of children mankind has ever known, and it means the throwing of millions of pounds of bombs and explosives at millions of children - also in Iraq - as Bush Jr did, in order to remove an old agent and replace him with a new puppet to assist in the pilfering of Iraq's oil and other outrages.\n",
      "as => [(Bush Jr, Iraq)]\n",
      "\n",
      "In addition, Bush sanctioned the installing of sons as state governors, and didn't forget to import expertise in election fraud from the region's presidents to Florida to be made use of in moments of difficulty.\n",
      "as => [(state governors, sons)]\n",
      "\n",
      "All that we have to do is to send two mujahidin to the furthest point east to raise a piece of cloth on which is written al-Qaida, in order to make the generals race there to cause America to suffer human, economic, and political losses without their achieving for it anything of note other than some benefits for their private companies.\n",
      "other_than => [(note, some benefits)]\n",
      "\n",
      "It is true that this shows that al-Qaida has gained, but on the other hand, it shows that the Bush administration has also gained, something of which anyone who looks at the size of the contracts acquired by the shady Bush administration-linked mega-corporations, like Halliburton and its kind, will be convinced.\n",
      "like => [(the shady Bush administration-linked mega-corporations, Halliburton), (the shady Bush administration-linked mega-corporations, its kind)]\n",
      "\n",
      "He fits the saying \"like the naughty she-goat who used her hoof to dig up a knife from under the earth\".\n",
      "like => [(the saying, the naughty she-goat)]\n",
      "\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Hearst patterns take the form of (NP <predicate> (NP (and | or)?)+)\n",
    "\n",
    "class hearst_patterns(object):\n",
    "    \n",
    "    \"\"\" Hearst Patterns is a class object used to detects hypernym relations to hyponyms in a text\n",
    "    \n",
    "    input: raw text\n",
    "    returns: list of dict object with each entry all the hypernym-hyponym pairs of a text\n",
    "    entry format: [\"predicate\" : [(hyponym, hypernym), (hyponym, hypernym), ..]]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import spacy\n",
    "    \n",
    "    def __init__(self, extended=False):\n",
    "        \n",
    "#     Included in each entry is the original regex pattern now adapted as a spaCy matcher pattern.\n",
    "#     Many of these patterns are in the same format, next iteration of code should include an\n",
    "#     automatic pattern generator for patterns.\n",
    "            \n",
    "#     These patterns need checking and cleaning up for testing.\n",
    "            \n",
    "#     Format for the dict entry of each pattern\n",
    "#     {\n",
    "#      \"label\" : predicate, \n",
    "#      \"pattern\" : spaCy pattern, \n",
    "#      \"posn\" : first/last depending on whether the hypernym appears before its hyponym\n",
    "#     }\n",
    "      \n",
    "        # make the patterns easier to read\n",
    "        # as lexical understanding develops, consider adding attributes to dstinguish between hypernyms and hyponyms\n",
    "        hypernym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}} \n",
    "        hyponym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
    "        punct = {\"IS_PUNCT\": True, \"OP\": \"?\"}\n",
    "\n",
    "        self.patterns = [\n",
    "\n",
    "        {\"label\" : \"such_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\": \"such\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"know_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?know as (NP_\\\\w+ ?(, )?(and |or )?)+)', # added for this experiment\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\": \"know\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"such_NOUN_as\", \"pattern\" : [\n",
    "#                 '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             {\"LEMMA\": \"such\"}, hypernym, punct, {\"LEMMA\": \"as\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"include\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\" : \"include\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"especially\", \"pattern\" : [ ## problem - especially is merged as a modifier in to a noun phrase\n",
    "#                 '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\" : \"especially\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"and-or_other\", \"pattern\" : [ ## problem - other is merged as a modifier in to a noun phrase\n",
    "#                 '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "#                 'last'\n",
    "             hyponym, punct, {\"DEP\": \"cc\"}, {\"LEMMA\" : \"other\"}, hypernym\n",
    "        ], \"posn\" : \"last\"},\n",
    "\n",
    "        ]\n",
    "\n",
    "        if extended:\n",
    "            self.patterns.extend([\n",
    "\n",
    "            {\"label\" : \"which_may_include\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "#                     '?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"may\"}, {\"LEMMA\" : \"include\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"which_be_similar_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"similar\"}, {\"LEMMA\" : \"to\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"example_of_this_be\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"this\"}, {\"LEMMA\" : \"be\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \",type\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"type\"}, punct, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"mainly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"mainly\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"mostly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"mostly\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"notably\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"notably\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"particularly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"particularly\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"principally\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)', - fuses in a noun phrase\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"principally\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"in_particular\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"in\"}, {\"LEMMA\" : \"particular\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"except\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"except\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"other_than\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"other\"}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"eg\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"e.g.\", \"eg\"]}}, hyponym \n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "#                 {\"label\" : \"eg-ie\", \"pattern\" : [ \n",
    "# #                     '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+' - need to understand this pattern better\n",
    "# #                     '(\\\\. )?\\\\))',\n",
    "# #                     'first'\n",
    "#                     hypernym, punct, {\"LEMMA\" : {IN : [\"e.g.\", \"i.e.\", \"eg\", \"ie\"]}}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "#                 ]},\n",
    "\n",
    "            {\"label\" : \"ie\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"i.e.\", \"ie\"]}}, hyponym \n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"for_example\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?for example (, )?'\n",
    "#                     '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"example\"}, punct, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"example_of_be\", \"pattern\" : [\n",
    "#                     'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym, punct, {\"LEMMA\" : \"be\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"like\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"like\"}, hyponym,\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            # repeat of such_as pattern in primary patterns???\n",
    "#                     'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "\n",
    "                {\"label\" : \"whether\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"whether\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"compare_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"compare\"}, {\"LEMMA\" : \"to\"}, hyponym \n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"among_-PRON-\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"among\"}, {\"LEMMA\" : \"-PRON-\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"for_instance\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "#                     'for instance)',\n",
    "#                     'first'\n",
    "                hypernym, punct, hyponym, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"instance\"}\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"and-or_any_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"DEP\": \"cc\"}, {\"LEMMA\" : \"any\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"some_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"some\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"be_a\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"a\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"like_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"like\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "             {\"label\" : \"one_of_the\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"the\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"one_of_these\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "#                     'last'\n",
    "            hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"these\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"one_of_those\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "#                     'last'\n",
    "            hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"those\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"be_example_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)', added optional \"an\" to spaCy pattern for singular vs. plural\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"an\", \"OP\" : \"?\"}, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"which_be_call\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"call\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "#               \n",
    "            {\"label\" : \"which_be_name\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"name\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"a_kind_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"a\"}, {\"LEMMA\" : \"kind\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)', - combined with above\n",
    "#                     'last'\n",
    "\n",
    "            {\"label\" : \"form_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"a\", \"OP\" : \"?\"}, {\"LEMMA\" : \"form\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"which_look_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"look\"}, {\"LEMMA\" : \"like\"}, hyponym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"which_sound_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"sound\"}, {\"LEMMA\" : \"like\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"type\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"type\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"compare_with\", \"pattern\" : [\n",
    "#                     '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                {\"LEMMA\" : \"compare\"}, hyponym, punct, {\"LEMMA\" : \"with\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"as\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"as\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"sort_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"sort\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "        ]),        \n",
    "\n",
    "    ## initiate matcher\n",
    "        from spacy.matcher import Matcher\n",
    "        self.matcher = Matcher(nlp.vocab, validate = True)\n",
    "\n",
    "        self.predicates = []\n",
    "        self.first = []\n",
    "        self.last = []\n",
    "\n",
    "        # add patterns to matcher\n",
    "        for pattern in self.patterns:\n",
    "            self.matcher.add(pattern[\"label\"], None, pattern[\"pattern\"])\n",
    "\n",
    "            # gather list of predicate terms for the noun_chunk deconfliction\n",
    "            self.predicates.append(pattern[\"label\"].split('_'))\n",
    "\n",
    "            # gather list of predicates where the hypernym appears first\n",
    "            if pattern[\"posn\"] == \"first\":\n",
    "                self.first.append(pattern[\"label\"])\n",
    "\n",
    "            # gather list of predicates where the hypernym appears last\n",
    "            if pattern[\"posn\"] == \"last\":\n",
    "                self.last.append(pattern[\"label\"])\n",
    "\n",
    "    def isPredicateMatch(self, chunk, predicates):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to remove predicate phrases from noun_chunks\n",
    "\n",
    "        input: the chunk to be checked, list of predicate phrases\n",
    "        returns: the chnunk with predicate phrases removed.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def match(empty, count, chunk, predicates):#\n",
    "            # empty: check whether predicates list is empty\n",
    "            # chunk[count].lemma_ != predicates[0][-count]: checks convergence between chunk and predicate string, removes empty spans\n",
    "            # chunk[count].lemma_ == predicates[0][count]: check whether chunk term is equal to the predicates term\n",
    "            \n",
    "            while not empty and chunk[count].lemma_ != predicates[0][-count] and chunk[count].lemma_ == predicates[0][count]:\n",
    "                count += 1\n",
    "\n",
    "            return empty, count\n",
    "    \n",
    "        def isMatch(chunk, predicates):\n",
    "\n",
    "            empty, counter = match(predicates == [], 0, chunk, predicates)\n",
    "            if empty or counter == len(predicates[0]):\n",
    "                return chunk[counter:]\n",
    "            else:\n",
    "                return isMatch(chunk, predicates[1:])\n",
    "\n",
    "        return isMatch(chunk, predicates)\n",
    "    \n",
    "    \n",
    "    def find_hyponyms(self, text):\n",
    "        \n",
    "        \"\"\"\n",
    "        this is the main function of the class object\n",
    "        \n",
    "        follows logic of:\n",
    "        1. checks whether text has been parsed\n",
    "        2. pre-processing for noun_chunks\n",
    "        3. generate matches\n",
    "        4. create list of dict object containing match results\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(text) is spacy.tokens.doc.Doc:\n",
    "            doc = text\n",
    "        else:\n",
    "            doc = nlp(text) # initiate doc \n",
    "            \n",
    "        \n",
    "        ## Pre-processing\n",
    "        # there are some predicate terms, such as \"particularly\", \"especially\" and \"some other\" which are\n",
    "        # merged with the noun phrase. Such terms are part of the pattern and become part of the\n",
    "        # merged noun-chunk, consequently, they are not detected in by the matcher.\n",
    "        # This pre-processing, therefore, walks through the noun_chunks of a doc object to remove those\n",
    "        # predicate terms from each noun_chunk and merges the result.\n",
    "        \n",
    "        with doc.retokenize() as retokenizer:\n",
    "\n",
    "            for chunk in doc.noun_chunks:\n",
    "\n",
    "                attrs = {\"tag\": chunk.root.tag, \"dep\": chunk.root.dep}\n",
    "\n",
    "                retokenizer.merge(self.isPredicateMatch(chunk, self.predicates), attrs = attrs)\n",
    "\n",
    "                \n",
    "                \n",
    "        ## Main Body\n",
    "        #Find matches in doc\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        pairs = [] # set up dictionary containing pairs\n",
    "        \n",
    "        # If none are found then return None\n",
    "        if not matches:\n",
    "            return pairs\n",
    "\n",
    "        for match_id, start, end in matches:\n",
    "            predicate = nlp.vocab.strings[match_id]\n",
    "            \n",
    "            if predicate in self.last: # if the predicate is in the list where the hypernym is last\n",
    "                hypernym = doc[end - 1]\n",
    "                hyponym = doc[start]\n",
    "            else:\n",
    "                hypernym = doc[start] # if the predicate is in the list where the hypernym is first\n",
    "                hyponym = doc[end - 1]\n",
    "\n",
    "            # create a list of dictionary objects with the format:\n",
    "            # {\n",
    "            # \"predicate\" : \" predicate term based from pattern name,\n",
    "            # \"pairs\" : [(hypernym, hyponym)] + [hyponym conjuncts (tokens linked by and | or)]\n",
    "            # \"sent\" : sentence in which the pairs originate\n",
    "            # }\n",
    "            \n",
    "            pairs.append(dict({\"predicate\" : predicate, \n",
    "                               \"pairs\" : [(hypernym, hyponym)] + [(hypernym, token) for token in hyponym.conjuncts if token != hypernym],\n",
    "                               \"sent\" : (hyponym.sent.text).strip()}))\n",
    "\n",
    "        return pairs\n",
    "    \n",
    "h = hearst_patterns(extended = True)\n",
    "\n",
    "for i, text in enumerate(orators[\"laden\"]):\n",
    "    print(i, ': ', text.title)\n",
    "\n",
    "    hypernyms = h.find_hyponyms(text.text)\n",
    "\n",
    "    if hypernyms:\n",
    "        for hypernym in hypernyms:\n",
    "            print(hypernym[\"sent\"])\n",
    "            print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test of Hearst Pattern Detection Object\n",
    "\n",
    "First sentence contains a 'first' relationship' where hypernym preceeds hyponym.\n",
    "\n",
    "Second sentence contains both a 'first' and 'last' relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"We are hunting for terrorist groups, particularly the Taliban and al Qaeda\",\n",
    "    \"We are hunting for the IRA, ISIS, al Qaeda and some other terrorist groups, especially the Taliban, Web Scientists and particularly Southampton University\"\n",
    "]\n",
    "\n",
    "def show_hyps(o):\n",
    "    \n",
    "   \n",
    "    for i, text in enumerate(o):\n",
    "        print(i, \"#####\")\n",
    "        hypernyms = h.find_hyponyms(text)\n",
    "\n",
    "        if hypernyms:\n",
    "            for hypernym in hypernyms:\n",
    "                print(hypernym[\"sent\"])\n",
    "                print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "                print()\n",
    "                \n",
    "        \n",
    "    \n",
    "        print('-----')\n",
    "\n",
    "show_hyps(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test With a Larger Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########  0\n",
      "Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\n",
      "such_as => [(symptoms, red eye), (symptoms, ocular pain), (symptoms, visual acuity), (symptoms, photophobia)]\n",
      "\n",
      "-----\n",
      "###########  1\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "include => [(Other close friends, Canada), (Other close friends, Australia), (Other close friends, Germany), (Other close friends, France)]\n",
      "\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "as => [(the operation, forces)]\n",
      "\n",
      "-----\n",
      "###########  2\n",
      "The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\n",
      "know_as => [(loosely affiliated terrorist organizations, al Qaeda)]\n",
      "\n",
      "-----\n",
      "###########  3\n",
      "Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\n",
      "like => [(Terrorist groups, al Qaeda)]\n",
      "\n",
      "-----\n",
      "###########  4\n",
      "This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\n",
      "include => [(terrorists, e-mails), (terrorists, the Internet), (terrorists, cell phones)]\n",
      "\n",
      "-----\n",
      "###########  5\n",
      "From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\n",
      "as => [(a hostile regime, the United States)]\n",
      "\n",
      "-----\n",
      "###########  6\n",
      "-----\n",
      "###########  7\n",
      "We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\n",
      "especially => [(other terrorist groups, the Taliban), (other terrorist groups, the muppets)]\n",
      "\n",
      "-----\n",
      "Wall time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a list of docs\n",
    "docs = [\n",
    "    \"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\",\n",
    "    \"Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\",\n",
    "    \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\",\n",
    "    \"Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\",\n",
    "    \"This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\",\n",
    "    \"From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\",\n",
    "    \"We are looking out for the Taliban, al Qaeda and other terrorist groups\",\n",
    "    \"We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\"\n",
    "]\n",
    "show_hyps(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with a Full Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :   - Ive Been to the Mountaintop.txt\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6fe134547eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mhypernyms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_hyponyms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mfind_hyponyms\u001b[1;34m(self, text)\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize.Retokenizer.__exit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize._merge\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.vector.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(orators[\"king\"]):\n",
    "    print(i, ': ', text.title)\n",
    "    \n",
    "    hypernyms = h.find_hyponyms(text.text)\n",
    "\n",
    "    if hypernyms:\n",
    "        for hypernym in hypernyms:\n",
    "            print(hypernym[\"sent\"])\n",
    "            print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
