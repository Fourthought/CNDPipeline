{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Hearst Patterns\n",
    "---\n",
    "\n",
    "In this experiment we test the utility of Hearst Patterns for detecting the ingroup and outgroup of a text.\n",
    "\n",
    "For this experiment spaCy matcher is used with code adapted from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/hearstPatterns.py\n",
    "\n",
    "Hypernym relations are semantic relationships between two concepts: C1 is a hypernym of C2 means that C1 categorizes C2 (e.g. “instrument” is a hypernym of “Piano”). For this research, the phrase, \"America has enemies, such as Al Qaeda and the Taliban\" would return the following '[('Al Qaeda', 'enemy'), ('the Taliban', 'enemy')]'. In this example, the categorising term 'enemy' is a hypernym of both 'Al Qaeda' and the 'Taliban'; conversely 'al Qaeda' and 'the Tabliban' are hyponyms of 'enemy'. Using this technique, hypernym terms could be classified as ingroup or outgroup and named entities identified as hyponym terms could be identified as either group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the spaCy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "for component in nlp.pipe_names:\n",
    "    if component not in ['tagger', \"parser\", \"ner\"]:\n",
    "        nlp.remove_pipe(component)\n",
    "\n",
    "merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(merge_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset of Political Speeches from George Bush, Osama bin Laden and Martin Luther King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object bush called George Bush has 14 speeches\n",
      "object king called Martin Luther King has 5 speeches\n",
      "object laden called Osama bin Laden has 7 speeches\n"
     ]
    }
   ],
   "source": [
    "import cndobjects\n",
    "\n",
    "dirpath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\speeches\"\n",
    "\n",
    "orators = cndobjects.Dataset(dirpath)\n",
    "\n",
    "for orator in orators:\n",
    "    print(f'object {orator.ref} called {orator.name} has {len(orator)} speeches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hearst Pattern Detection Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold\n",
      "12\n",
      "[('an exceptional man', 'passenger', 'like'), ('al Qaeda', 'loosely affiliated terrorist organization', 'know_as'), ('woman', 'civilian', 'include'), ('child', 'civilian', 'include'), ('the Egyptian Islamic Jihad', 'different countrie', 'include'), ('the Islamic Movement of Uzbekistan', 'different countrie', 'include'), ('Afghanistan', 'place', 'like'), ('american citizen', 'all foreign national', 'include'), ('Egypt', 'many muslim countrie', 'such_as'), ('Saudi Arabia', 'many muslim countrie', 'such_as'), ('Jordan', 'many muslim countrie', 'such_as'), ('the will', 'every value', 'except')]\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Hearst patterns take the form of (NP <predicate> (NP (and | or)?)+)\n",
    "\n",
    "class hearst_patterns(object):\n",
    "    \n",
    "    \"\"\" Hearst Patterns is a class object used to detects hypernym relations to hyponyms in a text\n",
    "    \n",
    "    input: raw text\n",
    "    returns: list of dict object with each entry all the hypernym-hyponym pairs of a text\n",
    "    entry format: [\"predicate\" : [(hyponym, hypernym), (hyponym, hypernym), ..]]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import spacy\n",
    "    \n",
    "    def __init__(self, nlp, extended=False, predicatematch = \"basic\"):\n",
    "        \n",
    "       \n",
    "#     Included in each entry is the original regex pattern now adapted as a spaCy matcher pattern.\n",
    "#     Many of these patterns are in the same format, next iteration of code should include an\n",
    "#     automatic pattern generator for patterns.\n",
    "            \n",
    "#     These patterns need checking and cleaning up for testing.\n",
    "            \n",
    "#     Format for the dict entry of each pattern\n",
    "#     {\n",
    "#      \"label\" : predicate, \n",
    "#      \"pattern\" : spaCy pattern, \n",
    "#      \"posn\" : first/last depending on whether the hypernym appears before its hyponym\n",
    "#     }\n",
    "      \n",
    "        # make the patterns easier to read\n",
    "        # as lexical understanding develops, consider adding attributes to dstinguish between hypernyms and hyponyms\n",
    "        self.nlp = nlp\n",
    "        \n",
    "        options = [\"bronze\", \"silver\", \"gold\"]\n",
    "        if predicatematch not in options:\n",
    "            entry = \"\"\n",
    "            while entry not in [\"1\", \"2\", \"3\"]: \n",
    "                entry = input(f\"1. {options[0]}, 2. {options[1]}, 3. {options[2]}\")\n",
    "            self.predicatematch = options[int(entry) -1]\n",
    "        else:\n",
    "            self.predicatematch = predicatematch\n",
    "        \n",
    "        hypernym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}} \n",
    "        hyponym = {\"POS\" : {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
    "        punct = {\"IS_PUNCT\": True, \"OP\": \"?\"}\n",
    "\n",
    "        self.patterns = [\n",
    "\n",
    "        {\"label\" : \"such_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\": \"such\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"know_as\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?know as (NP_\\\\w+ ?(, )?(and |or )?)+)', # added for this experiment\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\": \"know\"}, {\"LEMMA\": \"as\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"such\", \"pattern\" : [\n",
    "#                 '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             {\"LEMMA\": \"such\"}, hypernym, punct, {\"LEMMA\": \"as\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"include\", \"pattern\" : [\n",
    "#                 '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\" : \"include\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"especially\", \"pattern\" : [ ## problem - especially is merged as a modifier in to a noun phrase\n",
    "#                 '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                 'first'\n",
    "             hypernym, punct, {\"LEMMA\" : \"especially\"}, hyponym\n",
    "        ], \"posn\" : \"first\"},\n",
    "\n",
    "        {\"label\" : \"other\", \"pattern\" : [\n",
    "#             problem: the noun_chunk, 'others' clashes with this rule to create a zero length chunk when predicate removed\n",
    "#                 '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "#                 'last'\n",
    "             hyponym, punct, {\"LEMMA\" : {\"IN\" : [\"and\", \"or\"]}}, {\"LEMMA\" : \"other\"}, hypernym\n",
    "#             There were bruises, lacerations, or other injuries were not prevalent.\"\n",
    "        ], \"posn\" : \"last\"},\n",
    "\n",
    "        ]\n",
    "\n",
    "        if extended:\n",
    "            self.patterns.extend([\n",
    "\n",
    "            {\"label\" : \"which_may_include\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "#                     '?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"may\"}, {\"LEMMA\" : \"include\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"which_be_similar_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"similar\"}, {\"LEMMA\" : \"to\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"example_of_this_be\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"this\"}, {\"LEMMA\" : \"be\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \",type\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"type\"}, punct, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"mainly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"mainly\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"mostly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"mostly\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"notably\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"notably\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"particularly\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"particularly\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"principally\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)', - fuses in a noun phrase\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"principally\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"in_particular\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"in\"}, {\"LEMMA\" : \"particular\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"except\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"except\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"other_than\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"other\"}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"eg\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"e.g.\", \"eg\"]}}, hyponym \n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "#                 {\"label\" : \"eg-ie\", \"pattern\" : [ \n",
    "# #                     '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+' - need to understand this pattern better\n",
    "# #                     '(\\\\. )?\\\\))',\n",
    "# #                     'first'\n",
    "#                     hypernym, punct, {\"LEMMA\" : {IN : [\"e.g.\", \"i.e.\", \"eg\", \"ie\"]}}, {\"LEMMA\" : \"than\"}, hyponym\n",
    "#                 ]},\n",
    "\n",
    "            {\"label\" : \"ie\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : {\"IN\" : [\"i.e.\", \"ie\"]}}, hyponym \n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"for_example\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?for example (, )?'\n",
    "#                     '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"example\"}, punct, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"example_of_be\", \"pattern\" : [\n",
    "#                     'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym, punct, {\"LEMMA\" : \"be\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"like\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"like\"}, hyponym,\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            # repeat of such_as pattern in primary patterns???\n",
    "#                     'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "\n",
    "                {\"label\" : \"whether\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"whether\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"compare_to\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"compare\"}, {\"LEMMA\" : \"to\"}, hyponym \n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"among_-PRON-\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "#                     '(, )?(and |or )?)+)',\n",
    "#                     'first'\n",
    "                hypernym, punct, {\"LEMMA\" : \"among\"}, {\"LEMMA\" : \"-PRON-\"}, hyponym\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"for_instance\", \"pattern\" : [\n",
    "#                     '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "#                     'for instance)',\n",
    "#                     'first'\n",
    "                hypernym, punct, hyponym, {\"LEMMA\" : \"for\"}, {\"LEMMA\" : \"instance\"}\n",
    "            ], \"posn\" : \"first\"},\n",
    "\n",
    "            {\"label\" : \"and-or_any_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"DEP\": \"cc\"}, {\"LEMMA\" : \"any\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"some_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"some\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"be_a\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"a\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"like_other\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"like\"}, {\"LEMMA\" : \"other\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "             {\"label\" : \"one_of_the\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"the\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"one_of_these\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "#                     'last'\n",
    "            hyponym, punct, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"these\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"one_of_those\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "#                     'last'\n",
    "            hyponym, punct, {\"DEP\": \"cc\", \"OP\" : \"?\"}, {\"LEMMA\" : \"one\"}, {\"LEMMA\" : \"of\"}, {\"LEMMA\" : \"those\"}, hypernym,\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"be_example_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)', added optional \"an\" to spaCy pattern for singular vs. plural\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"an\", \"OP\" : \"?\"}, {\"LEMMA\" : \"example\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"which_be_call\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"call\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "#               \n",
    "            {\"label\" : \"which_be_name\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"be\"}, {\"LEMMA\" : \"name\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"a_kind_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"a\"}, {\"LEMMA\" : \"kind\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)', - combined with above\n",
    "#                     'last'\n",
    "\n",
    "            {\"label\" : \"form_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"a\", \"OP\" : \"?\"}, {\"LEMMA\" : \"form\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"which_look_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"look\"}, {\"LEMMA\" : \"like\"}, hyponym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"which_sound_like\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"which\"}, {\"LEMMA\" : \"sound\"}, {\"LEMMA\" : \"like\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"type\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"type\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"compare_with\", \"pattern\" : [\n",
    "#                     '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                {\"LEMMA\" : \"compare\"}, hyponym, punct, {\"LEMMA\" : \"with\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "#             {\"label\" : \"as\", \"pattern\" : [\n",
    "# #                     '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "# #                     'last'\n",
    "#                 hyponym, punct, {\"LEMMA\" : \"as\"}, hypernym\n",
    "#             ], \"posn\" : \"last\"},\n",
    "\n",
    "            {\"label\" : \"sort_of\", \"pattern\" : [\n",
    "#                     '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "#                     'last'\n",
    "                hyponym, punct, {\"LEMMA\" : \"sort\"}, {\"LEMMA\" : \"of\"}, hypernym\n",
    "            ], \"posn\" : \"last\"},\n",
    "\n",
    "        ]),        \n",
    "\n",
    "        ## initiate matcher\n",
    "        from spacy.matcher import Matcher\n",
    "        self.matcher = Matcher(self.nlp.vocab, validate = True)\n",
    "        \n",
    "        # added \"some\" to original list\n",
    "        self.predicate_list = [\n",
    "            'able', 'available', 'brief', 'certain',\n",
    "            'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "            'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "            'its', 'last', 'latter', 'least', 'less', 'likely', 'little',\n",
    "            'many', 'ml', 'more', 'most', 'much', 'my', 'necessary',\n",
    "            'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',\n",
    "            'particular', 'past', 'possible', 'present', 'proud', 'recent',\n",
    "            'same', 'several', 'significant', 'similar', 'some', 'such', 'sup', 'sure'\n",
    "        ]\n",
    "\n",
    "        self.predicates = []\n",
    "        self.first = []\n",
    "        self.last = []\n",
    "\n",
    "        # add patterns to matcher\n",
    "        for pattern in self.patterns:\n",
    "            self.matcher.add(pattern[\"label\"], None, pattern[\"pattern\"])\n",
    "\n",
    "            # gather list of predicate terms for the noun_chunk deconfliction\n",
    "            self.predicates.append(pattern[\"label\"].split('_'))\n",
    "\n",
    "            # gather list of predicates where the hypernym appears first\n",
    "            if pattern[\"posn\"] == \"first\":\n",
    "                self.first.append(pattern[\"label\"])\n",
    "\n",
    "            # gather list of predicates where the hypernym appears last\n",
    "            if pattern[\"posn\"] == \"last\":\n",
    "                self.last.append(pattern[\"label\"])\n",
    "                \n",
    "    def isPredicateMatch_bronze(self, chunk, predicates):\n",
    "        \n",
    "        \"\"\"\n",
    "        Bronze option to remove predicate phrases from noun_chunks using a predefined list of modifiers\n",
    "\n",
    "        input: the chunk to be checked, list of predicate phrases\n",
    "        returns: the chnunk with predicate phrases removed.\n",
    "\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        while chunk[counter].lemma_ in predicates:\n",
    "                counter += 1\n",
    "                \n",
    "        #remove empty spans, eg the noun_chunk 'others' becomes a zero length span\n",
    "        if len(chunk[count:]) == 0:\n",
    "            count = 0\n",
    "                \n",
    "        return chunk[counter:]\n",
    "    \n",
    "    def isPredicateMatch_silver(self, chunk, predicates):\n",
    "        \n",
    "        \"\"\"\n",
    "        Silver option to remove predicate phrases from noun_chunks using dependency parse labels\n",
    "\n",
    "        input: the chunk to be checked, list of predicate phrases\n",
    "        returns: the chnunk with predicate phrases removed.\n",
    "\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        while chunk[counter].pos_ in [\"DET\", \"ADJ\", \"ADV\"]:\n",
    "            counter += 1\n",
    "                \n",
    "        #remove empty spans, eg the noun_chunk 'others' becomes a zero length span\n",
    "        if len(chunk[counter:]) == 0:\n",
    "            count = 0\n",
    "                \n",
    "        return chunk[counter:]\n",
    "\n",
    "    def isPredicateMatch_gold(self, chunk, predicates):\n",
    "        \n",
    "        \"\"\"\n",
    "        Gold option to remove predicate phrases from noun_chunks using pattern labels.\n",
    "\n",
    "        input: the chunk to be checked, list of predicate phrases\n",
    "        returns: the chnunk with predicate phrases removed.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def match(empty, count, chunk, predicates):#\n",
    "            # empty: check whether predicates list is empty\n",
    "            # count < len(predicates[0]): checks whether the count has reached the final token of the predicate\n",
    "            # chunk[count].lemma_ == predicates[0][count]: check whether chunk token is equal to the predicate token\n",
    "\n",
    "            \n",
    "            while not empty and count < len(predicates[0]) and chunk[count].lemma_ == predicates[0][count]:\n",
    "                count += 1\n",
    "                \n",
    "            #remove empty spans, eg the noun_chunk 'others' becomes a zero length span\n",
    "            if len(chunk[count:]) == 0:\n",
    "                count = 0\n",
    "\n",
    "            return empty, count\n",
    "    \n",
    "        def isMatch(chunk, predicates):\n",
    "\n",
    "            empty, counter = match(predicates == [], 0, chunk, predicates)\n",
    "            if empty or counter == len(predicates[0]):\n",
    "                #print(chunk, \"becomes: \", chunk[counter:])\n",
    "                return chunk[counter:]\n",
    "            else:\n",
    "                return isMatch(chunk, predicates[1:])\n",
    "\n",
    "        return isMatch(chunk, predicates)\n",
    "    \n",
    "    \n",
    "    def find_hyponyms(self, text):\n",
    "        \n",
    "        \"\"\"\n",
    "        this is the main function of the class object\n",
    "        \n",
    "        follows logic of:\n",
    "        1. checks whether text has been parsed\n",
    "        2. pre-processing for noun_chunks\n",
    "        3. generate matches\n",
    "        4. create list of dict object containing match results\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(text) is spacy.tokens.doc.Doc:\n",
    "            doc = text\n",
    "        else:\n",
    "            doc = self.nlp(text) # initiate doc \n",
    "            \n",
    "        \n",
    "        ## Pre-processing\n",
    "        # there are some predicate terms, such as \"particularly\", \"especially\" and \"some other\" which are\n",
    "        # merged with the noun phrase. Such terms are part of the pattern and become part of the\n",
    "        # merged noun-chunk, consequently, they are not detected in by the matcher.\n",
    "        # This pre-processing, therefore, walks through the noun_chunks of a doc object to remove those\n",
    "        # predicate terms from each noun_chunk and merges the result.\n",
    "        \n",
    "        with doc.retokenize() as retokenizer:\n",
    "\n",
    "            for chunk in doc.noun_chunks:\n",
    "\n",
    "                attrs = {\"tag\": chunk.root.tag, \"dep\": chunk.root.dep}\n",
    "\n",
    "                if self.predicatematch == \"bronze\":\n",
    "                    retokenizer.merge(self.isPredicateMatch_bronze(chunk, self.predicate_list), attrs = attrs)\n",
    "                elif self.predicatematch == \"silver\":\n",
    "                    retokenizer.merge(self.isPredicateMatch_silver(chunk, self.predicate_list), attrs = attrs)\n",
    "                elif self.predicatematch == \"gold\":\n",
    "                    retokenizer.merge(self.isPredicateMatch_gold(chunk, self.predicates), attrs = attrs)\n",
    "    \n",
    "        ## Main Body\n",
    "        #Find matches in doc\n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        pairs = [] # set up dictionary containing pairs\n",
    "        \n",
    "        # If none are found then return None\n",
    "        if not matches:\n",
    "            return pairs\n",
    "\n",
    "        for match_id, start, end in matches:\n",
    "            predicate = self.nlp.vocab.strings[match_id]\n",
    "            \n",
    "            # if the predicate is in the list where the hypernym is last, else hypernym is first\n",
    "            if predicate in self.last: \n",
    "                hypernym = doc[end - 1]\n",
    "                hyponym = doc[start]\n",
    "            else:\n",
    "                # an inelegent way to deal with the \"such_NOUN_as pattern\" since the first token is not the hypernym\n",
    "                if doc[start].lemma_ == \"such\":\n",
    "                    start += 1\n",
    "                hypernym = doc[start]\n",
    "                hyponym = doc[end - 1]\n",
    "\n",
    "            # create a list of dictionary objects with the format:\n",
    "            # {\n",
    "            # \"predicate\" : \" predicate term based from pattern name,\n",
    "            # \"pairs\" : [(hypernym, hyponym)] + [hyponym conjuncts (tokens linked by and | or)]\n",
    "            # \"sent\" : sentence in which the pairs originate\n",
    "            # }\n",
    "            \n",
    "#             pairs.append(dict({\"predicate\" : predicate, \n",
    "#                                \"pairs\" : [(hypernym, hyponym)] + [(hypernym, token) for token in hyponym.conjuncts if token != hypernym],\n",
    "#                                \"sent\" : (hyponym.sent.text).strip()}))\n",
    "\n",
    "            pairs.append((hyponym.lemma_, hypernym.lemma_, predicate))  \n",
    "            for token in hyponym.conjuncts:   \n",
    "                if token != hypernym and token != None:\n",
    "                    pairs.append((token.lemma_, hypernym.lemma_, predicate))\n",
    "\n",
    "        return pairs\n",
    "    \n",
    "h = hearst_patterns(nlp, extended = True, predicatematch = \"gold\")\n",
    "print(h.predicatematch)\n",
    "result = h.find_hyponyms(orators[\"bush\"][3].text)\n",
    "print(len(result))\n",
    "print(result)\n",
    "#print(h.predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Herrick', 'author', 'such'), ('Goldsmith', 'author', 'such'), ('Shakespeare', 'author', 'such')]\n",
      "----------\n",
      "[('laceration', 'injury', 'other'), ('bruise', 'injury', 'other')]\n",
      "----------\n",
      "[('Canada', 'common law countrie', 'include'), ('Australia', 'common law countrie', 'include'), ('England', 'common law countrie', 'include')]\n",
      "----------\n",
      "[('France', 'many countrie', 'especially'), ('England', 'many countrie', 'especially'), ('Spain', 'many countrie', 'especially')]\n",
      "----------\n",
      "[('postharvest losses reduction', 'benefit', 'such'), ('food increase', 'benefit', 'such'), ('soil fertility improvement', 'benefit', 'such')]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "docs = [\"There are works by such authors as Herrick, Goldsmith, and Shakespeare.\",\n",
    "        \"There were bruises, lacerations, or other injuries were not prevalent.\",\n",
    "        \"common law countries, including Canada, Australia, and England enjoy toast.\",\n",
    "        \"Many countries, especially France, England and Spain also enjoy toast.\",\n",
    "        \"There are such benefits as postharvest losses reduction, food increase and soil fertility improvement.\"\n",
    "       ]\n",
    "\n",
    "for doc in docs:\n",
    "    print(h.find_hyponyms(doc))\n",
    "    print('----------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common law country\n"
     ]
    }
   ],
   "source": [
    "dox = nlp(\"common law countries\")\n",
    "for t in dox.noun_chunks:\n",
    "    print(t.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test of Hearst Pattern Detection Object\n",
    "\n",
    "First sentence contains a 'first' relationship' where hypernym preceeds hyponym.\n",
    "\n",
    "Second sentence contains both a 'first' and 'last' relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 #####\n",
      "[]\n",
      "-----\n",
      "1 #####\n",
      "[]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"We are hunting for terrorist groups, particularly the Taliban and al Qaeda\",\n",
    "    \"We are hunting for the IRA, ISIS, al Qaeda and some other terrorist groups, especially the Taliban, Web Scientists and particularly Southampton University\"\n",
    "]\n",
    "\n",
    "def show_hyps(o):\n",
    "    \n",
    "   \n",
    "    for i, text in enumerate(o):\n",
    "        print(i, \"#####\")\n",
    "        print(h.find_hyponyms(text))\n",
    "    \n",
    "        print('-----')\n",
    "\n",
    "show_hyps(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test With a Larger Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('red eye', 'symptom', 'such_as'), ('ocular pain', 'symptom', 'such_as'), ('visual acuity', 'symptom', 'such_as'), ('photophobia', 'symptom', 'such_as')]\n",
      "[('Canada', 'close friend', 'include'), ('Australia', 'close friend', 'include'), ('Germany', 'close friend', 'include'), ('France', 'close friend', 'include')]\n",
      "[('al Qaeda', 'loosely affiliated terrorist organization', 'know_as')]\n",
      "[('al Qaeda', 'terrorist group', 'like')]\n",
      "[('e-mail', 'terrorist', 'include'), ('the internet', 'terrorist', 'include'), ('cell phone', 'terrorist', 'include')]\n",
      "[]\n",
      "[('al Qaeda', 'terrorist group', 'other'), ('the Taliban', 'terrorist group', 'other')]\n",
      "[('al Qaeda', 'terrorist group', 'other'), ('the Taliban', 'terrorist group', 'especially'), ('the muppet', 'terrorist group', 'especially')]\n",
      "Wall time: 165 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a list of docs\n",
    "docs = [\n",
    "    \"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\",\n",
    "    \"Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\",\n",
    "    \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\",\n",
    "    \"Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\",\n",
    "    \"This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\",\n",
    "    \"From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\",\n",
    "    \"We are looking out for the Taliban, al Qaeda and other terrorist groups\",\n",
    "    \"We are looking out for al Qaeda and other terrorist groups, especially the Taliban and the muppets\"\n",
    "]\n",
    "\n",
    "for doc in docs:\n",
    "    print(h.find_hyponyms(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with a Full Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :   - Ive Been to the Mountaintop.txt\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6fe134547eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mhypernyms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_hyponyms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mfind_hyponyms\u001b[1;34m(self, text)\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize.Retokenizer.__exit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize._merge\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.vector.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(orators[\"king\"]):\n",
    "    print(i, ': ', text.title)\n",
    "    \n",
    "    hypernyms = h.find_hyponyms(text.text)\n",
    "\n",
    "    if hypernyms:\n",
    "        for hypernym in hypernyms:\n",
    "            print(hypernym[\"sent\"])\n",
    "            print(hypernym[\"predicate\"], '=>', hypernym[\"pairs\"])\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
