{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of doc:  111934\n",
      "completed at: Apr 29 2020 18:25:29\n",
      "Wall time: 62.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "FileList = ['20010114-Remarks at the National Day of Prayer & Remembrance Service.txt',\n",
    "            '20010115-First Radio Address following 911.txt',\n",
    "            '20010117-Address at Islamic Center of Washington, D.C..txt',\n",
    "           '20010120-Address to Joint Session of Congress Following 911 Attacks.txt',\n",
    "           '20010911-Address to the Nation.txt',\n",
    "           '20011007-Operation Enduring Freedom in Afghanistan Address to the Nation.txt',\n",
    "           '20011011-911 Pentagon Remembrance Address.txt',\n",
    "           '20011011-Prime Time News Conference on War on Terror.txt',\n",
    "           '20011026-Address on Signing the USA Patriot Act of 2001.txt',\n",
    "           '20011110-First Address to the United Nations General Assembly.txt',\n",
    "           '20011211-Address to Citadel Cadets.txt',\n",
    "           '20011211-The World Will Always Remember 911.txt',\n",
    "           '20020129-First (Official) Presidential State of the Union Address.txt',\n",
    "           ]\n",
    "bushraw = ''\n",
    "binladenraw = ''\n",
    "\n",
    "filepath = 'C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Speeches/'\n",
    "\n",
    "binladenpath = os.path.join(filepath, 'Osama bin Laden/')\n",
    "bushpath = os.path.join(filepath, 'George Bush/')\n",
    "\n",
    "for f in FileList:\n",
    "    with open(bushpath + f, 'r') as text:\n",
    "        bushraw = bushraw + text.read()\n",
    "        \n",
    "with open(bushpath + \"bush_complete.txt\", \"w\") as file:\n",
    "    file.write(bushraw)\n",
    "\n",
    "FileList = ['19960823-OBL Declaration.txt',\n",
    "            '20011007-OBL Full Warning.txt',\n",
    "            '20011109-OBL.txt',\n",
    "            '20021124-OBL Letter to America.txt',\n",
    "            '20041101-Al Jazeera Speech.txt'\n",
    "           ]\n",
    "\n",
    "for f in FileList:\n",
    "    with open(binladenpath + f, 'r') as text:\n",
    "        binladenraw = binladenraw + text.read()\n",
    "        \n",
    "with open(binladenpath + \"binladen_complete.txt\", \"w\") as file:\n",
    "    file.write(binladenraw)\n",
    "        \n",
    "# with open(os.path.join(filepath, \"fulltext.txt\"), 'w') as text:\n",
    "#         text.write(raw)\n",
    "\n",
    "print('length of doc: ', len(bushraw))\n",
    "print(f'completed at: {datetime.datetime.now().strftime(\"%b %d %Y %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 > We are here in the middle hour of our grief.\n",
      "1 > So many have suffered so great a loss, and today we express our nation's sorrow.\n",
      "2 > We come before God to pray for the missing and the dead, and for those who loved them.\n",
      "3 > On Tuesday, our country was attacked with deliberate and massive cruelty.\n",
      "4 > We have seen the images of fire and ashes and bent steel.\n",
      "5 > Now come the names, the list of casualties we are only beginning to read:\n",
      "6 > They are the names of men and women who began their day at a desk or in an airport, busy with life.\n",
      "7 > They are the names of people who faced death and in their last moments called home to say, be brave and I love you.\n",
      "8 > They are the names of passengers who defied their murderers and prevented the murder of others on the ground.\n",
      "9 > They are the names of men and women who wore the uniform of the United States and died at their posts.\n",
      "10 > They are the names of rescuers -- the ones whom death found running up the stairs and into the fires to help others.\n",
      "-----\n",
      "0 > A Message From Usama Bin Muhammad Bin Ladin unto His Muslim Brethren All Over The World\n",
      "1 > Generally\n",
      "2 > And In The Arabian Peninsula Specifically Praise be to Allah, we seek His help and ask for his pardon.\n",
      "3 > We take refuge in Allah from the evil of ourselves and our bad deeds.\n",
      "4 > Whoever been guided by Allah will not be misled, and who ever has been misled, he will never be guided.\n",
      "5 > I bear witness that there is no God except Allah, no associates has He, and I bear witness that Muhammad is His slave and messenger.\n",
      "6 > \"\n",
      "7 > O you who believe!\n",
      "8 > Be careful of your duty to Allah with the proper care which is due to Him, and do not die except as Muslim.\n",
      "9 > \"\n",
      "10 > (Imraan; 3:102).\n",
      "number of Bush sentences:  1210\n",
      "number of Bin Laden sentences:  1210\n",
      "completed at: Apr 22 2020 16:49:00\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with nlp.disable_pipes('tagger', 'ner'):\n",
    "    bush_doc = nlp(bushraw)\n",
    "    binladen_doc = nlp(binladenraw)\n",
    "\n",
    "# dictionary object for the sentences from each file\n",
    "bush_sentences = dict()\n",
    "binladen_sentences = dict()\n",
    "\n",
    "# iterate over sentences from each orator, remove any return symbols and add to dictionary object\n",
    "# note, sentences are identified by their index in a document rather than the word\n",
    "for sentence in bush_doc.sents:\n",
    "        bush_sentences[len(bush_sentences)] = sentence.text.strip()\n",
    "        \n",
    "for sentence in binladen_doc.sents:\n",
    "        binladen_sentences[len(binladen_sentences)] = sentence.text.strip()\n",
    "        \n",
    "for i, s in enumerate(bush_sentences.values()):\n",
    "    print(i, '>', str(s))\n",
    "    if i == 10:\n",
    "        break\n",
    "        \n",
    "print(\"-----\")\n",
    "        \n",
    "for i, s in enumerate(binladen_sentences.values()):\n",
    "    print(i, '>', str(s))\n",
    "    if i == 10:\n",
    "        break\n",
    "        \n",
    "print('number of Bush sentences: ', len(bush_sentences))\n",
    "print('number of Bin Laden sentences: ', len(bush_sentences))\n",
    "print(f'completed at: {datetime.datetime.now().strftime(\"%b %d %Y %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy import displacy\n",
    "\n",
    "ingroup = dict()\n",
    "outgroup = dict()\n",
    "index = dict()\n",
    "\n",
    "# open previous file and progress index\n",
    "\n",
    "filepath = 'C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Experiment 7 - Dependency Matcher/'\n",
    "\n",
    "ingroupfile = \"binladen_ingroup_sents.json\"\n",
    "outgroupfile = \"binladen_outgroup_sents.json\"\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(filepath, ingroupfile), 'r') as fp:\n",
    "        ingroup = json.load(fp)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(filepath, ingroupfile), 'r') as fp:\n",
    "        outgroup = json.load(fp)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(filepath, \"index.json\"), 'r') as fp:\n",
    "        index = json.load(fp)\n",
    "except:\n",
    "    index[\"index\"] = 0\n",
    "\n",
    "\n",
    "print(index)\n",
    "\n",
    "#iterate over each sentence dictionary for classification of ingroup or outgroup\n",
    "while i < len(sentences):\n",
    "    # get the current progress through the dictionary object\n",
    "    i = index[\"index\"]\n",
    "    \n",
    "    # record progress  through dictionary object\n",
    "    with open(os.path.join(filepath, \"index.json\"), \"wb\") as f:\n",
    "            f.write(json.dumps(index).encode(\"utf-8\"))\n",
    "    \n",
    "    # print current sentence\n",
    "    print('-----')\n",
    "    print('index: ', i)\n",
    "    displacy.render(nlp(sentences[i]), style = 'ent')\n",
    "    print(i, '/', len(sentences), '=>', sentences[i])\n",
    "    entry = input('ingroup(i) / outgroup(o) / back(b)').lower()\n",
    "    \n",
    "    # ask if sentence is refering to an ingroup or outgroup\n",
    "    if entry in ['i', 'o']:        \n",
    "        if entry == 'i': # add sentence to ingroup dictionary if user selects ingroup\n",
    "            print(len(ingroup), ' => ingroup add: ', sentences[i])\n",
    "            ingroup[len(ingroup)] = sentences[i]\n",
    "            \n",
    "            # write dictionary to file\n",
    "            with open(os.path.join(filepath, ingroupfile), \"wb\") as f:\n",
    "                f.write(json.dumps(ingroup).encode(\"utf-8\"))\n",
    "            \n",
    "        else: # else add sentence to outgroup dictionary\n",
    "            print(len(outgroup), ' => outgroup add: ', sentences[i])\n",
    "            outgroup[len(outgroup)] = sentences[i]\n",
    "            \n",
    "            # write dictionary to file\n",
    "            with open(os.path.join(filepath, outgroupfile), \"wb\") as f:\n",
    "                f.write(json.dumps(outgroup).encode(\"utf-8\"))\n",
    "                \n",
    "        # increase index by 1\n",
    "        index[\"index\"] += 1\n",
    "    \n",
    "    \n",
    "    # if user enters 'b' then go back by 1 in the dictionary and delete\n",
    "    elif entry == 'b': \n",
    "        if i != 0:\n",
    "            \n",
    "            # test whether the previous sentence was ingroup or outgroup and delete from respective dictionary\n",
    "            \n",
    "            if len(ingroup) - 1 >= 0 and sentences[i-1] == ingroup[len(ingroup) - 1]:\n",
    "                print('deleting from ingroup: ', sentences[i-1])\n",
    "                del(ingroup[len(ingroup) - 1])\n",
    "\n",
    "                with open(os.path.join(filepath, ingroupfile), \"wb\") as f:\n",
    "                    f.write(json.dumps(ingroup).encode(\"utf-8\"))\n",
    "\n",
    "            elif len(outgroup) - 1 >= 0 and sentences[i-1] == outgroup[len(outgroup) - 1]:\n",
    "                print('deleting from outgroup: ', sentences[i-1])\n",
    "                del(outgroup[len(outgroup) - 1])\n",
    "\n",
    "                with open(os.path.join(filepath, outgroupfile), \"wb\") as f:\n",
    "                    f.write(json.dumps(outgroup).encode(\"utf-8\"))\n",
    "\n",
    "            index[\"index\"] -= 1\n",
    "        \n",
    "        else:\n",
    "            print('iterating backwards by one sentence')\n",
    "            pass\n",
    "        \n",
    "    # quit    \n",
    "    elif entry == 'q':\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        index[\"index\"] += 1\n",
    "\n",
    "print(f'completed at {str(datetime.datetime.now())}') #1220"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
