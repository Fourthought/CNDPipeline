{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Hearst Patterns\n",
    "---\n",
    "In this experiment we test the utility of Hearst Patterns for detecting the ingroup and outgroup of a text.\n",
    "\n",
    "For this experiment regex is used with code taken from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/hearstPatterns.py\n",
    "\n",
    "Hypernym relations are semantic relationships between two concepts: C1 is a hypernym of C2 means that C1 categorizes C2 (e.g. “instrument” is a hypernym of “Piano”). For this research, the phrase, \"America has enemies, such as Al Qaeda and the Taliban\" would return the following '[('Al Qaeda', 'enemy'), ('the Taliban', 'enemy')]'. In this example, the categorising term 'enemy' is a hypernym of both 'Al Qaeda' and the 'Taliban'; conversely 'al Qaeda' and 'the Tabliban' are hyponyms of 'enemy'. Using this technique, hypernym terms could be classified as ingroup or outgroup and named entities identified as hyponym terms could be identified as either group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment has not produced any results from the bin Laden text, but has produced some promising results from the Bush text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_the_evidence -PRON- have gather NP_all_point to NP_a_collection of NP_loosely_affiliate_terrorist_organization know as NP_al_Qaeda .\n",
      "[('al Qaeda', 'loosely affiliate terrorist organization')]\n",
      "NP_terrorist_group like NP_al_Qaeda depend upon NP_the_aid or NP_indifference of NP_government .\n",
      "[('al Qaeda', 'terrorist group')]\n",
      "other NP_close_friend , include NP_Canada , NP_Australia , NP_Germany and NP_France , have pledge NP_force as NP_the_operation unfold .\n",
      "[('Canada', 'close friend'), ('Australia', 'close friend'), ('Germany', 'close friend'), ('France', 'close friend'), ('force', 'the operation')]\n"
     ]
    }
   ],
   "source": [
    "h = HearstPatterns(extended=True, merge = False)\n",
    "\n",
    "true_positives = [\n",
    "    \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\",\n",
    "    \"Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\",\n",
    "    \"Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\",\n",
    "]\n",
    "             \n",
    "for sentence in true_positives:\n",
    "    print(h.find_hyponyms(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are some false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "NP_terrorist , include NP_e__mail , NP_the_internet , and NP_cell_phone \n",
      "[('e  mail', 'terrorist'), ('the internet', 'terrorist'), ('cell phone', 'terrorist')]\n",
      "-----\n",
      "NP_the_United_States as NP_a_hostile_regime\n",
      "[('the United States', 'a hostile regime')]\n"
     ]
    }
   ],
   "source": [
    "false_positives = [\n",
    "    \"This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\",\n",
    "    \"From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\"\n",
    "]\n",
    "\n",
    "for sentence in false_positives:\n",
    "    print(h.find_hyponyms(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object bush called George Bush has 14 speeches\n",
      "object king called Martin Luther King has 5 speeches\n",
      "object laden called Osama bin Laden has 7 speeches\n"
     ]
    }
   ],
   "source": [
    "import entities\n",
    "\n",
    "dirpath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\speeches\"\n",
    "\n",
    "orators = entities.Dataset(dirpath)\n",
    "\n",
    "for orator in orators:\n",
    "    print(f'object {orator.ref} called {orator.name} has {len(orator)} speeches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "the following code is taken from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/test/test_hearstPatterns.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from spacy.pipeline import merge_noun_chunks\n",
    "from spacy.pipeline import merge_entities\n",
    "\n",
    "\n",
    "class HearstPatterns(object):\n",
    "\n",
    "    def __init__(self, extended=False, merge = False):\n",
    "\n",
    "        self.__adj_stopwords = [\n",
    "            'able', 'available', 'brief', 'certain',\n",
    "            'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "            'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "            'its', 'last', 'latter', 'least', 'less', 'likely', 'little',\n",
    "            'many', 'ml', 'more', 'most', 'much', 'my', 'necessary',\n",
    "            'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',\n",
    "            'particular', 'past', 'possible', 'present', 'proud', 'recent',\n",
    "            'same', 'several', 'significant', 'similar', 'such', 'sup', 'sure'\n",
    "        ]\n",
    "\n",
    "        # now define the Hearst patterns\n",
    "        # format is <hearst-pattern>, <general-term>\n",
    "        # so, what this means is that if you apply the first pattern,\n",
    "        # the first Noun Phrase (NP)\n",
    "        # is the general one, and the rest are specific NPs\n",
    "        self.__hearst_patterns = [\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?know as (NP_\\\\w+ ?(, )?(and |or )?)+)', # added for this experiment\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "                'last'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if extended:\n",
    "            self.__hearst_patterns.extend([\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?for example (, )?'\n",
    "                    '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+'\n",
    "                    '(\\\\. )?\\\\))',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "                    'for instance)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "                    '?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                )\n",
    "            ])\n",
    "\n",
    "        self.__spacy_nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        if merge:\n",
    "            self.__spacy_nlp.add_pipe(nlp.create_pipe(\"merge_entities\"), after = \"ner\")\n",
    "            #self.__spacy_nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"), last = True)\n",
    "            \n",
    "\n",
    "    def chunk(self, rawtext):\n",
    "        doc = self.__spacy_nlp(rawtext)\n",
    "        chunks = []\n",
    "        for sentence in doc.sents:\n",
    "            \n",
    "            sentence_text = sentence.lemma_   ## lemmatise sentence\n",
    "            \n",
    "            ## iterate through the sentence noun chunks\n",
    "            for chunk in sentence.noun_chunks: \n",
    "                \n",
    "                # if the chunk is 'for example' or 'example of' move to next chunk\n",
    "                if chunk.lemma_.lower() == \"example\":   \n",
    "                    start = chunk.start\n",
    "                    pre_token = sentence[start - 1].lemma_.lower()\n",
    "                    post_token = sentence[start + 1].lemma_.lower()\n",
    "                    if start > 0 and\\\n",
    "                            (pre_token == \"for\" or post_token == \"of\"):\n",
    "                        continue\n",
    "                \n",
    "                # if the chunk is 'type' move to the next chunk\n",
    "                if chunk.lemma_.lower() == \"type\": \n",
    "                    continue\n",
    "                \n",
    "                chunk_arr = []\n",
    "                replace_arr = []\n",
    "                # print(\"chunk:\", chunk)\n",
    "                \n",
    "                ## iterate through the tokens in the chunk\n",
    "                for token in chunk:\n",
    "                    \n",
    "                    ## if the token is a stopword or 'i.e.' or 'e.g.' move to next token\n",
    "                    if token.lemma_ in self.__adj_stopwords + [\"i.e.\", \"e.g.\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    # append token lemma to the chunk array\n",
    "                    chunk_arr.append(token.lemma_)\n",
    "                    \n",
    "                    # Remove punctuation and stopword adjectives\n",
    "                    # (generally quantifiers of plurals)\n",
    "                    if token.lemma_.isalnum():\n",
    "                        replace_arr.append(token.lemma_)\n",
    "                    else:\n",
    "                        replace_arr.append(''.join(\n",
    "                            char for char in token.lemma_ if char.isalnum()\n",
    "                        ))\n",
    "                if len(chunk_arr) == 0:\n",
    "                    chunk_arr.append(chunk[-1].lemma_)\n",
    "                chunk_lemma = ' '.join(chunk_arr)\n",
    "                # print(chunk_lemma)\n",
    "                replacement_value = 'NP_' + '_'.join(replace_arr)\n",
    "                if chunk_lemma:\n",
    "                    sentence_text = re.sub(r'\\b%s\\b' % re.escape(chunk_lemma),\n",
    "                                           r'%s' % replacement_value,\n",
    "                                           sentence_text)\n",
    "\n",
    "            chunks.append(sentence_text)    \n",
    "        return chunks\n",
    "\n",
    "    \"\"\"\n",
    "        This is the main entry point for this code.\n",
    "        It takes as input the rawtext to process and returns a list\n",
    "        of tuples (specific-term, general-term)\n",
    "        where each tuple represents a hypernym pair.\n",
    "    \"\"\"\n",
    "    def find_hyponyms(self, rawtext):\n",
    "\n",
    "        hyponyms = []\n",
    "        np_tagged_sentences = self.chunk(rawtext)\n",
    "\n",
    "        for sentence in np_tagged_sentences:\n",
    "            # two or more NPs next to each other should be merged\n",
    "            # into a single NP, it's a chunk error\n",
    "\n",
    "            for (hearst_pattern, parser) in self.__hearst_patterns:\n",
    "                matches = re.search(hearst_pattern, sentence)\n",
    "                if matches:\n",
    "                    match_str = matches.group(0)\n",
    "\n",
    "                    nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "                    \n",
    "                    if parser == \"first\":\n",
    "                        general = nps[0]\n",
    "                        specifics = nps[1:]\n",
    "                    else:\n",
    "                        general = nps[-1]\n",
    "                        specifics = nps[:-1]\n",
    "\n",
    "                    for i in range(len(specifics)):\n",
    "                        pair = (\n",
    "                            self.clean_hyponym_term(specifics[i]),\n",
    "                            self.clean_hyponym_term(general)\n",
    "                        )\n",
    "                        # reduce duplicates\n",
    "                        if pair not in hyponyms:\n",
    "                            hyponyms.append(pair)\n",
    "\n",
    "        return hyponyms\n",
    "\n",
    "    def clean_hyponym_term(self, term):\n",
    "        # good point to do the stemming or lemmatization\n",
    "        return term.replace(\"NP_\", \"\").replace(\"_\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('an exceptional man', 'passenger'), ('al Qaeda', 'loosely affiliate terrorist organization'), ('woman', 'civilian'), ('child', 'civilian'), ('the Egyptian Islamic Jihad', 'country'), ('the Islamic Movement', 'country'), ('Afghanistan', 'place'), ('american citizen', 'all foreign national'), ('Egypt', 'muslim country'), ('Saudi Arabia', 'muslim country'), ('Jordan', 'muslim country'), ('the will', 'every value'), ('the United States', 'a hostile regime'), ('terrorism', 'a threat')]\n"
     ]
    }
   ],
   "source": [
    "h = HearstPatterns(extended=True, merge = False)\n",
    "\n",
    "print(h.find_hyponyms(orators[\"bush\"][3].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_hyponym_finder (__main__.TestHearstPatterns)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-e0484314abfb>\", line 40, in test_hyponym_finder\n",
      "    self.assertEqual(tuple(map(str.lower, hyps6[0])), (\"postharvest loss reduction\", \"benefit\"))\n",
      "AssertionError: Tuples differ: ('postharv loss reduction', 'benefit') != ('postharvest loss reduction', 'benefit')\n",
      "\n",
      "First differing element 0:\n",
      "'postharv loss reduction'\n",
      "'postharvest loss reduction'\n",
      "\n",
      "- ('postharv loss reduction', 'benefit')\n",
      "+ ('postharvest loss reduction', 'benefit')\n",
      "?           +++\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 2.726s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestHearstPatterns(unittest.TestCase):\n",
    "\n",
    "    def test_hyponym_finder(self):\n",
    "        h = HearstPatterns(extended=True)\n",
    "\n",
    "        # H1\n",
    "        hyps1 = h.find_hyponyms(\"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\")\n",
    "\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[0])), (\"red eye\", \"symptom\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[1])), (\"ocular pain\", \"symptom\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[2])), (\"visual acuity\", \"symptom\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[3])), (\"photophobia\", \"symptom\"))\n",
    "\n",
    "        # H2\n",
    "        hyps2 = h.find_hyponyms(\"There are works by such authors as Herrick, Goldsmith, and Shakespeare.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps2[0])), (\"herrick\", \"author\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps2[1])), (\"goldsmith\", \"author\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps2[2])), (\"shakespeare\", \"author\"))\n",
    "\n",
    "        # H3\n",
    "        hyps3 = h.find_hyponyms(\"There were bruises, lacerations, or other injuries were not prevalent.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps3[0])), (\"bruise\", \"injury\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps3[1])), (\"laceration\", \"injury\"))\n",
    "\n",
    "        # H4\n",
    "        hyps4 = h.find_hyponyms(\"common law countries, including Canada, Australia, and England enjoy toast.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps4[0])), (\"canada\", \"common law country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps4[1])), (\"australia\", \"common law country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps4[2])), (\"england\", \"common law country\"))\n",
    "\n",
    "        # H5\n",
    "        hyps5 = h.find_hyponyms(\"Many countries, especially France, England and Spain also enjoy toast.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps5[0])), (\"france\", \"country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps5[1])), (\"england\", \"country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps5[2])), (\"spain\", \"country\"))\n",
    "\n",
    "        # H2\n",
    "        hyps6 = h.find_hyponyms(\"There are such benefits as postharvest losses reduction, food increase and soil fertility improvement.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps6[0])), (\"postharvest loss reduction\", \"benefit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps6[1])), (\"food increase\", \"benefit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps6[2])), (\"soil fertility improvement\", \"benefit\"))\n",
    "\n",
    "        # H'1\n",
    "        hyps7 = h.find_hyponyms(\"Fruits, i.e. , apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        hyps7 = h.find_hyponyms(\"Fruits, e.g. apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        # H'2\n",
    "\n",
    "        hyps10 = h.find_hyponyms(\"Fruits (e.g. apples, bananas, oranges and peaches.)\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        hyps10 = h.find_hyponyms(\"Fruits (i.e. apples, bananas, oranges and peaches.)\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        # H'3\n",
    "        hyps8 = h.find_hyponyms(\"Fruits, for example apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        # H'4\n",
    "        hyps9 = h.find_hyponyms(\"Fruits, which may include apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_hyponym_finder (__main__.TestHearstPatterns)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-88c88a93b418>\", line 14, in test_hyponym_finder\n",
      "    self.assertEqual(hyps2[0], (\"herrick\", \"author\"))\n",
      "AssertionError: Tuples differ: ('Herrick', 'author') != ('herrick', 'author')\n",
      "\n",
      "First differing element 0:\n",
      "'Herrick'\n",
      "'herrick'\n",
      "\n",
      "- ('Herrick', 'author')\n",
      "?   ^\n",
      "\n",
      "+ ('herrick', 'author')\n",
      "?   ^\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.183s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestHearstPatterns(unittest.TestCase):\n",
    "\n",
    "    def test_hyponym_finder(self):\n",
    "        h = HearstPatterns()\n",
    "        hyps1 =  h.find_hyponyms(\"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\")\n",
    "        self.assertEqual(hyps1[0], (\"red eye\", \"symptom\"))\n",
    "        self.assertEqual(hyps1[1], (\"ocular pain\", \"symptom\"))\n",
    "        self.assertEqual(hyps1[2], (\"visual acuity\", \"symptom\"))\n",
    "        self.assertEqual(hyps1[3], (\"photophobia\", \"symptom\"))\n",
    "\n",
    "        hyps2 = h.find_hyponyms(\"There are works by such authors as Herrick, Goldsmith, and Shakespeare.\")\n",
    "        self.assertEqual(hyps2[0], (\"herrick\", \"author\"))\n",
    "        self.assertEqual(hyps2[1], (\"goldsmith\", \"author\"))\n",
    "        self.assertEqual(hyps2[2], (\"shakespeare\", \"author\"))\n",
    "\n",
    "        hyps3 = h.find_hyponyms(\"There were bruises, lacerations, or other injuries were not prevalent.\")\n",
    "        self.assertEqual(hyps3[0], (\"bruise\", \"injury\"))\n",
    "        self.assertEqual(hyps3[1], (\"laceration\", \"injury\"))\n",
    "\n",
    "        hyps4 =  h.find_hyponyms(\"common law countries, including Canada, Australia, and England enjoy toast.\")\n",
    "        self.assertEqual(hyps4[0], (\"canada\", \"common law country\"))\n",
    "        self.assertEqual(hyps4[1], (\"australia\", \"common law country\"))\n",
    "        self.assertEqual(hyps4[2], (\"england\", \"common law country\"))\n",
    "\n",
    "        hyps5 = h.find_hyponyms(\"Many countries, especially France, England and Spain also enjoy toast.\")\n",
    "        self.assertEqual(hyps5[0], (\"france\", \"country\"))\n",
    "        self.assertEqual(hyps5[1], (\"england\", \"country\"))\n",
    "        self.assertEqual(hyps5[2], (\"spain\", \"country\"))\n",
    "\n",
    "        hyps6 = h.find_hyponyms(\"There are such benefits as postharvest losses reduction, food increase and soil fertility improvement.\")\n",
    "        self.assertEqual(hyps6[0], (\"postharvest loss reduction\", \"benefit\"))\n",
    "        self.assertEqual(hyps6[1], (\"food increase\", \"benefit\"))\n",
    "        self.assertEqual(hyps6[2], (\"soil fertility improvement\", \"benefit\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
