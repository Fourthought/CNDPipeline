{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning\n",
      "creating string\n",
      "checking for palindrome:  22\n",
      "manaplanafcanalpanam\n",
      "result:  True\n",
      "checking for palindrome:  20\n",
      "anaplanafcanalpana\n",
      "result:  True\n",
      "checking for palindrome:  18\n",
      "naplanafcanalpan\n",
      "result:  True\n",
      "checking for palindrome:  16\n",
      "aplanafcanalpa\n",
      "result:  True\n",
      "checking for palindrome:  14\n",
      "planafcanalp\n",
      "result:  True\n",
      "checking for palindrome:  12\n",
      "lanafcanal\n",
      "result:  True\n",
      "checking for palindrome:  10\n",
      "anafcana\n",
      "result:  True\n",
      "checking for palindrome:  8\n",
      "nafcan\n",
      "result:  True\n",
      "checking for palindrome:  6\n",
      "afca\n",
      "result:  True\n",
      "checking for palindrome:  4\n",
      "fc\n",
      "result:  True\n",
      "checking for palindrome:  2\n",
      "\n",
      "result:  False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def isPalindrome(s):\n",
    "    print('beginning')\n",
    "    \n",
    "    def toChars(s):\n",
    "        print('creating string')\n",
    "        s = s.lower()\n",
    "        ans = ''\n",
    "        for c in s:\n",
    "            if c.islower(): # 'abcdefghijklmnopqrstuvwxyz':\n",
    "                ans = ans + c\n",
    "        return ans\n",
    "    \n",
    "    def isPal(s):\n",
    "        print('checking for palindrome: ', len(s))\n",
    "        if len(s) <=1:\n",
    "            return True\n",
    "        else:\n",
    "            print(s[1:-1])\n",
    "            print(\"result: \", s[0] == s[-1])\n",
    "            return s[0] == s[-1] and isPal(s[1:-1])\n",
    "        \n",
    "    return isPal(toChars(s))\n",
    "\n",
    "print(isPalindrome('A man, a plan, a fcanal, Panama'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Steve/OneDrive - University of Southampton/CNDPipeline/speeches/Martin Luther King/19630403 - Ive Been to the Mountaintop.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-58e25547c1fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/Steve/OneDrive - University of Southampton/CNDPipeline/speeches/Martin Luther King/19630403 - Ive Been to the Mountaintop.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Steve/OneDrive - University of Southampton/CNDPipeline/speeches/Martin Luther King/19630403 - Ive Been to the Mountaintop.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dirpath = r\"C:/Users/Steve/OneDrive - University of Southampton/CNDPipeline/speeches/Martin Luther King/\"\n",
    "filename = r\"19630403 - Ive Been to the Mountaintop.txt\"\n",
    "\n",
    "filepath = os.path.join(dirpath, filename)\n",
    "filepath = dirpath + filename\n",
    "filepath = \"C:/Users/Steve/OneDrive - University of Southampton/CNDPipeline/speeches/Martin Luther King/19630403 - Ive Been to the Mountaintop.txt\"\n",
    "with open(filepath, 'r') as t:\n",
    "            text = t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "predicates = [['such', 'as'], ['know', 'as'], ['such', 'NOUN', 'as'], ['include'], ['especially'], ['other'], ['which', 'may', 'include'], ['which', 'be', 'similar', 'to'], ['example', 'of', 'this', 'be'], [',type'], ['mainly'], ['mostly'], ['notably'], ['particularly'], ['principally'], ['in', 'particular'], ['except'], ['other', 'than'], ['eg'], ['ie'], ['for', 'example'], ['example', 'of', 'be'], ['like'], ['whether'], ['compare', 'to'], ['among', '-PRON-'], ['for', 'instance'], ['and-or', 'any', 'other'], ['some', 'other'], ['be', 'a'], ['like', 'other'], ['one', 'of', 'the'], ['one', 'of', 'these'], ['one', 'of', 'those'], ['be', 'example', 'of'], ['which', 'be', 'call'], ['which', 'be', 'name'], ['a', 'kind', 'of'], ['form', 'of'], ['which', 'look', 'like'], ['which', 'sound', 'like'], ['type'], ['compare', 'with'], ['as'], ['sort', 'of']]\n",
    "print(len(predicates))\n",
    "\n",
    "def check(predicates):\n",
    "    if len(predicates) == 0:\n",
    "        return len(predicates), \"finshed\"\n",
    "    \n",
    "    print(len(predicates), predicates[0])\n",
    "    \n",
    "    return check(predicates[1:])\n",
    "\n",
    "#check(predicates)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[We, are, using, docs, ,, spans, ,, tokens, and, other, spacy, features, ,, such, as, merge, entities, ,, merge, noun, chunks, and, especially, the, retokenizer]\n",
      "We becomes We\n",
      "docs becomes docs\n",
      "spans becomes spans\n",
      "tokens becomes tokens\n",
      "other spacy features becomes spacy features\n",
      "merge entities becomes merge entities\n",
      "noun chunks becomes noun chunks\n",
      "especially the retokenizer becomes the retokenizer\n",
      "[We, are, using, docs, ,, spans, ,, tokens, and, other, spacy features, ,, such, as, merge entities, ,, merge, noun chunks, and, especially, the retokenizer]\n"
     ]
    }
   ],
   "source": [
    "def isPredicateMatch(chunk, predicates):\n",
    "\n",
    "    def match(empty, count, chunk, predicates):\n",
    "        #print('length: ', predicates[0])\n",
    "        while not empty and count < len(predicates[0]) and chunk[count].lemma_ == predicates[0][count]:\n",
    "            count += 1\n",
    "            \n",
    "        return empty, count\n",
    "    \n",
    "    def isMatch(chunk, predicates):\n",
    "        \n",
    "        empty, counter = match(predicates == [], 0, chunk, predicates)\n",
    "        if empty or counter == len(predicates[0]):\n",
    "            return chunk[counter:]\n",
    "        else:\n",
    "            return isMatch(chunk, predicates[1:])\n",
    "        \n",
    "    return isMatch(chunk, predicates)\n",
    "\n",
    "text =  \"We are using docs, spans, tokens and other spacy features, such as merge entities, merge noun chunks and especially the retokenizer\"\n",
    "\n",
    "doc = nlp(text)\n",
    "print([token for token in doc])\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "\n",
    "        attrs = {\"tag\": chunk.root.tag, \"dep\": chunk.root.dep}\n",
    "        #retokenizer.merge(chunk, attrs = attrs)\n",
    "        print(chunk, 'becomes', isPredicateMatch(chunk, predicates))\n",
    "\n",
    "        retokenizer.merge(isPredicateMatch(chunk, predicates), attrs = attrs)\n",
    "\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  |  3\n",
      "6  |  10\n",
      "5  |  16\n",
      "4  |  21\n",
      "3  |  25\n",
      "2  |  28\n",
      "1  |  30\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "def Total(x, y):\n",
    "\n",
    "    def foo(x, y):\n",
    "        return 2*x+y\n",
    "\n",
    "    def bar(x, y):\n",
    "        if x == 0:\n",
    "            return 'finished'\n",
    "        else:\n",
    "            print(x, ' | ', y)\n",
    "            return bar((x-1), y+x)\n",
    "    \n",
    "    return bar(foo(x, y), y)\n",
    "\n",
    "print(Total(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1: 4\n",
      " step 1: 3\n",
      " step 1: 2\n",
      "move from P1 to P3\n",
      " step 2: 2\n",
      "move from P1 to P2\n",
      " step 3: 2\n",
      "move from P3 to P2\n",
      " step 2: 3\n",
      "move from P1 to P3\n",
      " step 3: 3\n",
      " step 1: 2\n",
      "move from P2 to P1\n",
      " step 2: 2\n",
      "move from P2 to P3\n",
      " step 3: 2\n",
      "move from P1 to P3\n",
      " step 2: 4\n",
      "move from P1 to P2\n",
      " step 3: 4\n",
      " step 1: 3\n",
      " step 1: 2\n",
      "move from P3 to P2\n",
      " step 2: 2\n",
      "move from P3 to P1\n",
      " step 3: 2\n",
      "move from P2 to P1\n",
      " step 2: 3\n",
      "move from P3 to P2\n",
      " step 3: 3\n",
      " step 1: 2\n",
      "move from P1 to P3\n",
      " step 2: 2\n",
      "move from P1 to P2\n",
      " step 3: 2\n",
      "move from P3 to P2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def printMove(n, fr, to):\n",
    "    print('move from ' + str(fr) + ' to ' + str(to))\n",
    "    \n",
    "def Towers(n, fr, to, spare):\n",
    "    if n == 1:\n",
    "        printMove(n, fr, to)\n",
    "    else:\n",
    "        print(f\" step 1: {n}\")\n",
    "        Towers(n-1, fr, spare, to)\n",
    "        print(f\" step 2: {n}\")\n",
    "        Towers(1, fr, to, spare)\n",
    "        print(f\" step 3: {n}\")\n",
    "        Towers(n-1, spare, to, fr)\n",
    "        \n",
    "print(Towers(4, 'P1', 'P2', 'P3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('entity_dict len', len(entity_dict))\n",
    "max_key = entity_dict.pop(max(entity_dict.items(), key=lambda score: score[1]['score'])[0])\n",
    "print('entity_dict len', len(entity_dict))\n",
    "\n",
    "print(max_key)\n",
    "print(max_key['entity'])\n",
    "print(max_key['group'])\n",
    "print(max_key['score'])\n",
    "\n",
    "print('ent_set len: ', len(ent_set))\n",
    "ent_set.remove(max_key['entity'])\n",
    "print('ent_set len: ', len(ent_set))\n",
    "\n",
    "print('outgroup seeds len: ', len(seed_dict['OUT']))\n",
    "seed_dict[max_key['group']].append(max_key['entity'])\n",
    "print('outgroup seeds len: ', len(seed_dict['OUT']))\n",
    "\n",
    "\n",
    "print (\"\\nItems in sorted order\") \n",
    "for key, value in sorted(entity_dict.items(), \n",
    "                         key = lambda score: score[1]['score'],\n",
    "                        reverse = True): \n",
    "    print(key, \" | \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for ent in new_doc.ents: # entity extraction completed by Spacy pipeline \n",
    "    if ent.label_ in {'ORG', 'GPE', 'NORP'} and ent.text in model: #creates a set of entity types and checks if in model: \n",
    "        ent_set.add(ent.text.replace(' ', '_')) # noun phrases in model are created using _\n",
    "       \n",
    "\n",
    "seed_dict = {'IN' : ['friend', 'Friend'],\n",
    "             'OUT' : ['enemy', 'Enemy']}\n",
    "\n",
    "ent_dict = {}                    \n",
    "for i, entity in enumerate(ent_set):    \n",
    "    ent_pres[i] = {'entity': entity, \n",
    "            'ingp_scores' : [{'word': ingp, 'score' : model.similarity(entity, ingp)} for ingp in seed_dict['IN']],\n",
    "            'outgp_scores' : [{'word': outgp, 'score' : model.similarity(entity, outgp)} for outgp in seed_dict['OUT']]\n",
    "           }\n",
    "        \n",
    "for d in ent_dict.values():\n",
    "    print(d['entity'])\n",
    "    print(pd.DataFrame.from_dict(d['ingp_scores']).T)\n",
    "    print('--')\n",
    "    print(pd.DataFrame(d['outgp_scores']).T)\n",
    "    print('-----')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
